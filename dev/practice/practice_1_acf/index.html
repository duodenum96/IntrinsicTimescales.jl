<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Building the Autocorrelation Function · IntrinsicTimescales.jl</title><meta name="title" content="Building the Autocorrelation Function · IntrinsicTimescales.jl"/><meta property="og:title" content="Building the Autocorrelation Function · IntrinsicTimescales.jl"/><meta property="twitter:title" content="Building the Autocorrelation Function · IntrinsicTimescales.jl"/><meta name="description" content="Documentation for IntrinsicTimescales.jl."/><meta property="og:description" content="Documentation for IntrinsicTimescales.jl."/><meta property="twitter:description" content="Documentation for IntrinsicTimescales.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../home/">IntrinsicTimescales.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../home/">Getting Started</a></li><li><span class="tocitem">Practice</span><ul><li><a class="tocitem" href="../practice_intro/">Practice</a></li><li class="is-active"><a class="tocitem" href>Building the Autocorrelation Function</a></li><li><a class="tocitem" href="../practice_2_acw/">Autocorrelation Windows</a></li><li><a class="tocitem" href="../practice_3_ou/">Ornstein-Uhlenbeck Process as a Generative Model for ACF</a></li><li><a class="tocitem" href="../practice_4_psd/">Dealing with Oscillatory Artifacts using Fourier Transformation</a></li><li><a class="tocitem" href="../practice_5_bayesian/">Bayesian Estimation of Intrinsic Timescales</a></li></ul></li><li><a class="tocitem" href="../../theory/theory/">Theory</a></li><li><a class="tocitem" href="../../tutorial/tutorial_1_acw/">Navigating the Forest of INT Metrics</a></li><li><span class="tocitem">Implementation</span><ul><li><a class="tocitem" href="../../acw/">Model-Free Timescale Estimation</a></li><li><input class="collapse-toggle" id="menuitem-5-2" type="checkbox"/><label class="tocitem" for="menuitem-5-2"><span class="docs-label">Simulation Based Timescale Estimation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../simbasedinference/">Overview</a></li><li><a class="tocitem" href="../../one_timescale/">One Timescale Model</a></li><li><a class="tocitem" href="../../one_timescale_with_missing/">One Timescale Model with Missing Data</a></li><li><a class="tocitem" href="../../one_timescale_and_osc/">One Timescale Model with Oscillations</a></li><li><a class="tocitem" href="../../one_timescale_and_osc_with_missing/">One Timescale Model with Oscillations and Missing Data</a></li><li><a class="tocitem" href="../../fit_parameters/">Model Fitting and Parameters</a></li><li><a class="tocitem" href="../../fit_result/">Results</a></li></ul></li></ul></li><li><a class="tocitem" href="../../">API</a></li><li><a class="tocitem" href="../../citations/">Citations</a></li><li><a class="tocitem" href="../../contributing/">Contributing</a></li><li><a class="tocitem" href="../../developer/">Developer Documentation</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Practice</a></li><li class="is-active"><a href>Building the Autocorrelation Function</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Building the Autocorrelation Function</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/duodenum96/IntrinsicTimescales.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/duodenum96/IntrinsicTimescales.jl/blob/master/docs/src/practice/practice_1_acf.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Building-the-Autocorrelation-Function"><a class="docs-heading-anchor" href="#Building-the-Autocorrelation-Function">Building the Autocorrelation Function</a><a id="Building-the-Autocorrelation-Function-1"></a><a class="docs-heading-anchor-permalink" href="#Building-the-Autocorrelation-Function" title="Permalink"></a></h1><p>Data is noisy. Each time point has a random deviation. It is meaningless to ask anything about a single time point. However, certain statistical properties of random data are not random. For example, if I flip a coin 1000 times it is meaningless to ask whether the 348th flip will be heads or tails but on average, half the time I will get heads and half the time I will get tails. <em>Correlation time</em> is a statistical property of time-series data. It is not random: you can get many different random time series with the same correlation time. Correlation time measures how long does it take for a signal to lose similarity to itself. Why should we care? It is the basis of intrinsic neural timescales (INTs) and since you are here, I am assuming that you care about INTs. I&#39;ll explain more in the <a href="../../theory/theory/">Theory</a> sections. For now, let&#39;s just assume that it matters and learn how to calculate it. </p><p>To quantify the similarity between two things, we can use correlation. The higher the correlation, more similar two things are. The assumption that something loses similarity with itself implies that initially there was a similarity but over time we lost it. To quantify similarity of something with itself at a later time, we can calculate the correlation between that thing and that thing pushed forward in time. It is easier to see this with a figure. </p><p><img src="../assets/practice_acf_1_drawio.svg" alt/></p><p>We took the time series x and shifted it forward in time by an amount <span>$\Delta$</span>t. Then we need to take the correlation between them. To take a correlation between two things, you need to have equal number of data points in each of them. This is due to the definition of correlation, correlation is the average value of multiplication normalized by variance. You need to multiplicate corresponding data points. Take a look at the code example below. Throughout the documentation, there will be many code examples. I encourage you to run them on your computer and play around with them. Even better, take a pen and piece of paper and do the calculation below yourself. There is no better way to train intuition other than grinding your way through a calculation but I digress. Here is the code:</p><pre><code class="language-julia hljs">using Statistics # Import Statistics package for cor function
x1 = [-1, 0, 1] # example data
x2 = [2, -2, 0]
variance_x1 = sqrt(sum(x1 .^ 2) / 3) # Calculate variance of each dataset
variance_x2 = sqrt(sum(x2 .^ 2) / 3)
# Covariance is the average value of multiplication
covariance_x1_x2 = (x1[1]*x2[1] + x1[2]*x2[2] + x1[3]*x2[3]) / 3
# Correlation is normalized covariance
correlation_x1_x2 = covariance_x1_x2 / (variance_x1 * variance_x2)
isapprox(correlation_x1_x2, cor(x1, x2)) # Compare with cor function from Statistics package</code></pre><p>This looks basic, but makes an important point. As you go forward in time, you need to match the time points in your time series and shifted version of it. In the figure above, the only usable part is the part indicated in black vertical lines. This means as we shift further in time, we have less time points at our disposal and our correlation results are less reliable. We will return back to this point later. </p><p>We took a time-series, shifted it by an amount <span>$\Delta$</span>t, calculated the correlation and if the result is not zero, then we can say that the time series still haven&#39;t lost similarity to itself in <span>$\Delta$</span>t amount of time. Take a moment to ponder about this sentence. We are insinuating that there is such a <span>$\Delta$</span>t where the correlation is zero, or close to zero and this is the time it takes for a signal to lose similarity with itself. This is our INT. </p><p>Then a good strategy to calculate INT is simply calculating the correlation at various <span>$\Delta$</span>t values and detecting which <span>$\Delta$</span>t is the time where we lose correlation. Let&#39;s code this. We&#39;ll use the function <a href="../../#IntrinsicTimescales.OrnsteinUhlenbeck.generate_ou_process-Tuple{Union{Real, Vector{&lt;:Real}}, Vararg{Real, 4}}"><code>generate_ou_process</code></a> from the IntrinsicTimescales.jl package. This function simulates time series with a known timescale. I&#39;ll explain more about what it is doing in [Theory] section. For now, just know that this exists and is a good toy to play with. In IntrinsicTimescales.jl package, we have more optimized ways to do the operation I&#39;ll write below. I am doing this below explicitly and in detail so that we know exactly what we are doing when we compute these things. </p><pre><code class="language-julia hljs">using IntrinsicTimescales # import INT package
using Random 
using Plots # to plot the results
Random.seed!(1) # for replicability

timescale = 1.0
sd = 1.0 # sd of data we&#39;ll simulate
dt = 0.001 # Time interval between two time points
duration = 10.0 # 10 seconds of data
num_trials = 1 # Number of trials

data = generate_ou_process(timescale, sd, dt, duration, num_trials)
data = data[:] # Go from a (1, time) matrix to (time) vector</code></pre><p>The resulting <code>data</code> from <code>generate_ou_process</code> is a matrix where rows are different trials and columns are time points. In order to simplify the code below, I do the operation <code>data = data[:]</code> to turn it into a one dimensional vector. </p><p>The next step is doing shifting forward in time and correlating on this data. Look at the code below, take a piece of pen and paper and explicitly write down the indexing operations for different values of <span>$\Delta$</span>t to get a sense of how we are implementing this. Essentially, we are finding the indices corresponding to the data between the black vertical lines shown in the figure above. </p><pre><code class="language-julia hljs">n_timepoints = length(data)
n_lags = 4000 # Calculate the first 4000 lags.
correlation_results = zeros(n_lags) # Initialize empty vector to fill the results
# Start from no shifting (0) and end at number of time points - 1. 
lags = 0:(n_lags-1)
for DeltaT in lags
    # Get the indices for the data in vertical lines
    indices_data = (DeltaT+1):n_timepoints
    indices_shifted_data = 1:(n_timepoints - DeltaT)
    correlation_results[DeltaT+1] = cor(data[indices_data], data[indices_shifted_data])
end
plot(lags, correlation_results, label=&quot;&quot;) 
hline!([0], color=:black, label=&quot;&quot;) # Indicate the zero point of correlations</code></pre><p><img src="../assets/intro_2.svg" alt/></p><p>This is called an <em>autocorrelation function (ACF)</em>. On x axis, we have lags. One lag means we shifted one of the time series by one data point. On y axis, we plot the correlation values. Note that it starts from 1. Because when lag is zero, we did not shift any time series. We are correlating a time series with exactly itself and the correlation between one thing and itself is simply one. As we expected, the ACF decays as we shift lags. We can identify the lag where the correlation reaches zero. This is the first estimate of our timescale. This measure is called <em>ACW-0</em> which stands for <em>autocorrelation window-0</em>. It was first used by <a href="https://www.nature.com/articles/s42003-021-01785-z">Mehrshad Golesorkhi in his 2021 paper</a> and he found that ACW-0 differentiates brain regions better than previously used methods. Let&#39;s calculate the ACW-0 and indicate it in the plot with a vertical red line. </p><pre><code class="language-julia hljs">acw_0 = findfirst(correlation_results .&lt; 0)
plot(correlation_results, xlabel=&quot;Lags&quot;, ylabel=&quot;Correlation&quot;, label=&quot;&quot;)
hline!([0], color=:black, label=&quot;&quot;)
vline!([acw_0], color=:red, label=&quot;ACW-0&quot;)</code></pre><p><img src="../assets/intro_3.svg" alt/></p><p>So our work is done, right? We started with 1) the definition that INT is the time it takes for a time-series to lose its similarity with itself, 2) operationalized similarity with correlation, 3) operationalized similarity with itself as correlation with itself shifted some time lags and 4) identified the INT as the number of time lags required to lose similarity. There is one problem. Remember the problem of number of time points we talked about above. As we go further in lags, we have less and less number of data points to calculate the correlation, the portion inside vertical black lines is getting smaller and smaller. If we do not have enough number of data points to calculate ACW-0, then we will get a noisy estimate. </p><p>Let&#39;s try to see how big of a problem this is. Below, we will simulate the time-series again and again and overlay plots of ACFs. In a different panel, we&#39;ll do a histogram of ACW-0 values. To calculate ACF, we will use the function <a href="../../#IntrinsicTimescales.SummaryStats.comp_ac_fft-Union{Tuple{AbstractArray{T}}, Tuple{T}} where T&lt;:Real"><code>comp_ac_fft</code></a> from  INT.jl package. This function is faster and uses a different technique to calculate ACF which I&#39;ll explain in the [next section]. For now, it should suffice to know that it takes the data and optionally the number of lags we want as input and gives back the ACF. If number of lags is not specified, it goes through all possible lags. To get the ACW-0 from the ACF, we&#39;ll use <a href="../../acw/"><code>acw0</code></a> function which takes lags and ACF as input and gives ACW-0 value. </p><pre><code class="language-julia hljs">acw0_results = [] # Initialize empty vectors to hold the results
acfs = []
n_simulations = 10
for _ in 1:n_simulations
    data = generate_ou_process(timescale, sd, dt, duration, num_trials)[:]
    acf = comp_ac_fft(data; n_lags=n_lags)
    i_acw0 = acw0(lags, acf)
    push!(acw0_results, i_acw0) # Same as .append method in python
    push!(acfs, acf)
end
p1 = plot(lags, acfs, xlabel=&quot;Lags&quot;, ylabel=&quot;Correlation&quot;, 
          label=&quot;&quot;, title=&quot;ACF&quot;, alpha=0.5)
hline!([0], color=:black, label=&quot;&quot;)

p2 = histogram(acw0_results, xlabel=&quot;ACW-0&quot;, ylabel=&quot;Count&quot;,
               label=&quot;&quot;, title=&quot;Distribution of ACW-0&quot;)

# Combine the plots side by side
plot(p1, p2, layout=(1,2))</code></pre><p><img src="../assets/intro_4.svg" alt/></p><p>What&#39;s going on  here? We simulated the same process 10 times and each time we got a different result. All simulations had the same timescale, which we set as 1.0 above. Why did we get different results? Didn&#39;t we start by saying that even the data is random, statistical properties of it are not? That we can flip a coin 1000 times and on average half the time it will be heads and half the time it will be tails? Well, not quite. We said that <em>on average</em>, half the time it will be heads and the other half, it will be tails. Let&#39;s define an experiment as flipping the coin 1000 times. If you do this experiment once and look at the results, perhaps it will be 498 heads and 502 tails. Then do the experiment again, it will maybe give you 505 heads and 495 tails. You do the experiment again and again and keep track of the results. Then if you average over experiments, you&#39;ll see that there are 500 heads and 500 tails in the end. You can do a mini version of this experiment at home with 10 coin flips. The more experiments you do, the better the results will be. </p><p>Here is the central insight: When we calculate ACW-0 from limited data, we are not doing a perfect calculation. We are making an estimation. Based on the data we know, this is the timescale we think. And estimations are noisy. The noisiness of the estimation depends on the properties of data. The more number of data points we have, the better the estimations are. This is why I stressed that as we calculate ACF in later and later lags, our estimations become less and less reliable simply because we have less number of data points at our disposal. To see it clearly, look at the figure in the left panel and observe that at earlier lags, the variance between ACF estimates are low and it progressively increases as you go along later lags. Feel free to change the parameters <code>dt</code>,  <code>timescale</code> and <code>duration</code> to see how they change results. </p><p>This is why it is crucial to not only know your research problem, be it cognitive or basic neuroscience, but also the estimators you use to tackle the problem. How noisy are they? How much they are vulnerable to the number of data points? Are there other things in the data that might bias the results? Just because you are getting a number out of some algorithm does not mean that number has any meaning. It is the responsibility of the researcher, <em>you</em> to make sure your numbers make sense. </p><p>In the <a href="../practice_2_acw/">next section</a>, we will explore various kinds of <em>autocorrelation windows</em>, their motivation and how they address the bias. </p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../practice_intro/">« Practice</a><a class="docs-footer-nextpage" href="../practice_2_acw/">Autocorrelation Windows »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Thursday 19 June 2025 14:11">Thursday 19 June 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
