var documenterSearchIndex = {"docs":
[{"location":"fit_result/#Results","page":"Results","title":"Results","text":"","category":"section"},{"location":"fit_result/","page":"Results","title":"Results","text":"The fit function returns a ADVIResults or ABCResults object. ","category":"page"},{"location":"fit_result/#ABCResults","page":"Results","title":"ABCResults","text":"","category":"section"},{"location":"fit_result/","page":"Results","title":"Results","text":"The ABC algorithm returns an ABCResults type containing the full history and final results of the inference process. The type has the following fields:","category":"page"},{"location":"fit_result/","page":"Results","title":"Results","text":"MAP::Vector: Maximum a posteriori estimates of the parameters.\ntheta_history::Vector{Matrix}: History of parameter values across all PMC iterations. Each matrix contains the accepted parameters for that iteration with columns being parameters and rows being samples.\nepsilon_history::Vector: History of acceptance thresholds (epsilon values) used in each iteration.\nacc_rate_history::Vector: History of acceptance rates achieved in each iteration.\nweights_history::Vector{Vector}: History of importance weights for accepted samples in each iteration.\nfinal_theta::Matrix: Final accepted parameter values from the last iteration.\nfinal_weights::Vector: Final importance weights from the last iteration.","category":"page"},{"location":"fit_result/","page":"Results","title":"Results","text":"You can access these fields directly from the results type:","category":"page"},{"location":"fit_result/","page":"Results","title":"Results","text":"results = fit(model, param_dict)\n\n# Maximum a posteriori estimates\nmap_estimates = results.MAP\n\n# Access final parameter values\nfinal_params = results.final_theta\n\n# Get acceptance rates across iterations\nacc_rates = results.acc_rate_history\n\n# Get MAP estimates\nmap_estimates = results.MAP","category":"page"},{"location":"fit_result/#ADVIResults","page":"Results","title":"ADVIResults","text":"","category":"section"},{"location":"fit_result/","page":"Results","title":"Results","text":"The ADVI algorithm returns an ADVIResults type containing the inference results. The type has the following fields:","category":"page"},{"location":"fit_result/","page":"Results","title":"Results","text":"samples::AbstractArray: Matrix of posterior samples drawn after fitting. Each row represents a sample and each column represents a parameter.\nMAP::AbstractVector: Maximum a posteriori estimates of the parameters.\nvariational_posterior: The fitted variational posterior distribution containing the full inference results. In Turing.jl, this is obtained via q = vi(model, vi_alg).   ","category":"page"},{"location":"fit_result/","page":"Results","title":"Results","text":"You can access these fields directly from the results object:","category":"page"},{"location":"fit_result/","page":"Results","title":"Results","text":"results = fit(model, param_dict)\nposterior_samples = results.samples # Access posterior samples\nmap_estimates = results.MAP # Get MAP estimates\nposterior = results.variational_posterior # Full variational posterior","category":"page"},{"location":"practice/practice_intro/#Practice","page":"Practice","title":"Practice","text":"","category":"section"},{"location":"practice/practice_intro/","page":"Practice","title":"Practice","text":"In the practice section we will build the basic knowledge for timescale estimation. We'll start with  building up the autocorrelation function (ACF) and how it relates to INTs.  Then  we'll move on to more advanced concepts such as linking the ACF to the power spectrum.  If you are familiar with INTs, you can directly  jump to the section [Using IntrinsicTimescales.jl for Timescale Estimation-1: Model-Free Methods] and [2- Simulation-Based Methods]. If you are just starting or you want to brush up on the fundamentals, be my guest. ","category":"page"},{"location":"one_timescale_and_osc_with_missing/#one_timescale_and_osc_with_missing","page":"One Timescale Model with Oscillations and Missing Data","title":"One Timescale and Oscillation with Missing Data (one_timescale_and_osc_with_missing_model)","text":"","category":"section"},{"location":"one_timescale_and_osc_with_missing/","page":"One Timescale Model with Oscillations and Missing Data","title":"One Timescale Model with Oscillations and Missing Data","text":"Uses the same syntax as one_timescale_model and has the same implementation details (i.e. three priors and three results) as one_timescale_and_osc. We refer the users to the respective documentations. The only difference of one_timescale_and_osc_with_missing_model from one_timescale_and_osc is that missing data points is replaced with NaNs in the generative model, as in one_timescale_with_missing_model. ","category":"page"},{"location":"one_timescale_and_osc/#one_timescale_and_osc","page":"One Timescale Model with Oscillations","title":"One Timescale and Oscillation Model (one_timescale_and_osc_model)","text":"","category":"section"},{"location":"one_timescale_and_osc/","page":"One Timescale Model with Oscillations","title":"One Timescale Model with Oscillations","text":"Uses the same syntax as one_timescale_model. We refer the user to the documentation of one_timescale_model for details and point out the differences here. ","category":"page"},{"location":"one_timescale_and_osc/","page":"One Timescale Model with Oscillations","title":"One Timescale Model with Oscillations","text":"The generative model: ","category":"page"},{"location":"one_timescale_and_osc/","page":"One Timescale Model with Oscillations","title":"One Timescale Model with Oscillations","text":"fracdydt = -fracytau + xi(t) \n\n\nx(t) = sqrtay(t) + sqrt1-a sin(2 pi f t + phi)","category":"page"},{"location":"one_timescale_and_osc/","page":"One Timescale Model with Oscillations","title":"One Timescale Model with Oscillations","text":"where f is the frequency, a is the weight of the Ornstein-Uhlenbeck (OU) process and $ \\phi $ is a random number drawn from a normal distribution to reflect a random phase offset for each trial. Note that now we need to fit three parameters: $ \\tau $ for timescale, f for the oscillation frequency and $ a $ for how strong the oscillations are (with a smaller a indicating larger oscillations). Note that a is bounded between 0 and 1. Similarly, the maximum a posteriori estimates (MAP) also has three elements: one for each prior. Due to the three parameters needed, the fitting is more difficult compared to  one_timescale_model. ","category":"page"},{"location":"one_timescale_and_osc/","page":"One Timescale Model with Oscillations","title":"One Timescale Model with Oscillations","text":"If the user wishes to set the priors, they need to specify a prior for each of the parameters. The ordering is 1) the prior for timescale, 2) the prior for frequency second and 3) the prior for the coefficient. An example:","category":"page"},{"location":"one_timescale_and_osc/","page":"One Timescale Model with Oscillations","title":"One Timescale Model with Oscillations","text":"using Distributions, IntrinsicTimescales\npriors = [\n        Normal(0.1, 0.1),    # a prior for a 0.1 second timescale with an uncertainty of 0.1\n        Normal(10.0, 5.0),   # 10 Hz frequency with uncertainty of 5 Hz\n        Uniform(0.0, 1.0)    # Uniform distribution for coefficient\n    ]\nmodel = one_timescale_and_osc_model(data, time, :abc, summary_method=:acf, prior=priors)\nresults = fit(model)\nint = results.MAP[1]  # max a posterori for INT\nfreq = results.MAP[2] # for frequency\ncoef = results.MAP[3] # for coefficient","category":"page"},{"location":"one_timescale_and_osc/","page":"One Timescale Model with Oscillations","title":"One Timescale Model with Oscillations","text":"If the user does not specify a prior or sets prior=\"informed_prior\", IntrinsicTimescales.jl generates priors from data. The prior for the coefficient in this case is Uniform(0.0, 1.0). For summary_method=:acf, the timescale prior is an exponential decay fit to the ACF from data whereas summary_method=:psd fits a Lorentzian function to the PSD from data, obtains the knee frequency and estimates the timescale from it as in one_timescale_model. The prior for the frequency is obtained with first fitting a Lorentzian to the PSD, then subtracting the lorentzian to eliminate aperiodic component as in [FOOOF] and finally obtains the peak frequency with find_oscillation_peak. ","category":"page"},{"location":"one_timescale_and_osc/","page":"One Timescale Model with Oscillations","title":"One Timescale Model with Oscillations","text":"Similarly, the argument combine_distance=true not only calculates the RMSE between PSDs or ACFs, but also combines that distance with the RMSE between timescale and frequency estimates between the model and data. ","category":"page"},{"location":"one_timescale_and_osc/","page":"One Timescale Model with Oscillations","title":"One Timescale Model with Oscillations","text":"The other arguments are the same as one_timescale_model. We refer the reader to that section of the documentation for details. ","category":"page"},{"location":"home/#IntrinsicTimescales.jl-Documentation","page":"Getting Started","title":"IntrinsicTimescales.jl Documentation","text":"","category":"section"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"Welcome to the documentation of IntrinsicTimescales.jl. IntrinsicTimescales.jl is a software package for estimating Intrinsic Neural Timescales (INTs) from time-series data. It uses model-free methods (ACW-50, ACW-0, fitting an exponential decay function etc.) and simulation-based methods (ABC, ADVI) to estimate INTs.","category":"page"},{"location":"home/#Installation","page":"Getting Started","title":"Installation","text":"","category":"section"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"This package is written in Julia. If you do not have Julia installed, you can install it from here. Once you have Julia installed, you can install IntrinsicTimescales.jl by running the following command in the Julia REPL:","category":"page"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"using Pkg\nPkg.add(\"IntrinsicTimescales\")","category":"page"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"Soon, there will also be a Python wrapper called INTpy, which will allow you to use IntrinsicTimescales.jl from Python. ","category":"page"},{"location":"home/#Quickstart","page":"Getting Started","title":"Quickstart","text":"","category":"section"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"IntrinsicTimescales.jl uses two ways to estimate INTs: model-free methods and simulation-based inference. Model-free methods include ACW-50, ACW-0, ACW-e, decay rate of an exponential fit to ACF and knee freqency of a lorentzian fit to PSD. Simulation-based methods are based on Zeraati et al. (2022) paper and do parameter estimation by assuming the data came from an Ornstein-Uhlenbeck process. For estimation, in addition to the aABC method used in Zeraati et al. (2022), we also present ADVI. ","category":"page"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"For model-free methods, simply use ","category":"page"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"using INT\nacwresults = acw(data, fs; acwtypes = [:acw0, :acw50, :acweuler, :tau, :knee]), dims=ndims(data))\n# or even simpler:\nacwresults = acw(data, fs)","category":"page"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"where fs is sampling frequency, optional parameters acwtypes is a vector of  symbols (indicated with :) telling which methods to use and dims is indicating the dimension of time in your array (by default, the last dimension). The resulting acwresults gives the results in the same order of acwtypes. ","category":"page"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"For simulation based methods, pick one of the one_timescale_model, one_timescale_with_missing_model, one_timescale_and_osc_model and one_timescale_and_osc_with_missing_model functions. These models correspond to different generative models depending on whether there is an oscillation or not. For each generative model, there are with or without missing variants which use different ways to calculate ACF and PSD. Once you pick the model, the syntax is ","category":"page"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"model = one_timescale_model(data, time, :abc)\nresult = solve(model)","category":"page"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"or ","category":"page"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"model = one_timescale_model(data, time, :advi)\nresult = solve(model)","category":"page"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"These functions are highly customizable, see the page Simulation Based Timescale Estimation. ","category":"page"},{"location":"home/#Where-to-go-from-here?","page":"Getting Started","title":"Where to go from here?","text":"","category":"section"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"This documentation is divided in four parts. The fourth part API is an exhaustive list of functions and their signatures in the package. It is boring. A better place to start is the third part, Implementation. This part documents model-free and simulation-based methods that are used in the package, with the full function signatures. If you are already familiar with calculating INTs and just want to start using the package, this is the right place. ","category":"page"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"The remaining two parts are to understand the motivation to use various methods for calculating INTs and the motivation to calculate INTs (i.e., practice and theory). The first part is Practice. It is usually easier to understand something after you do it, therefore, I placed the practice section before theory. In Practice, we carefully build our way towards estimating INTs by starting from the autocorrelation function and slowly proceeding to more and more advanced methods. The second part is Theory. This part delves into the history of INT research, what it means in the brain and what it is good for with a particular emphasis on theoretical research, summarizing the cutting edge in this frontier. It is especially useful for researchers working on INT itself. Right now, it is in construction. I will deploy it as soon as it is ready. ","category":"page"},{"location":"home/#Getting-Help-and-Making-Contributions","page":"Getting Started","title":"Getting Help and Making Contributions","text":"","category":"section"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"Questions and contributions are welcome. Use the issues section of our github page to report bugs, make feature requests, ask questions or tackle the issues by making pull requests. ","category":"page"},{"location":"home/#About","page":"Getting Started","title":"About","text":"","category":"section"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"This package is developed by me, Yasir Ã‡atal during my PhD. I got nerdsniped by Zeraati et al., 2021 paper and started writing the package. the rest evolved from the simple motivation of reimplementing abcTau in Julia with various performance optimizations. ","category":"page"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"I am doing my PhD on INTs and our lab is specialized on the topic. As a result, I had many conversations with almost every member of our lab about INTs. I designed this documentation while keeping those conversations in mind. My goal was not only to document the package, but also to build up the knowledge to grasp the concept of INTs especially for new researchers starting their journey and active researchers in the trenches if they wish to brush up their basics. ","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"In construction","category":"page"},{"location":"practice/practice_1_acf/#Building-the-Autocorrelation-Function","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"","category":"section"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"Data is noisy. Each time point has a random deviation. It is meaningless to ask anything about a single time point. However, certain statistical properties of random data are not random. For example, if I flip a coin 1000 times it is meaningless to ask whether the 348th flip will be heads or tails but on average, half the time I will get heads and half the time I will get tails. Correlation time is a statistical property of time-series data. It is not random: you can get many different random time series with the same correlation time. Correlation time measures how long does it take for a signal to lose similarity to itself. Why should we care? It is the basis of intrinsic neural timescales (INTs) and since you are here, I am assuming that you care about INTs. I'll explain more in the [Theory] sections. For now, let's just assume that it matters and learn how to calculate it. ","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"To quantify the similarity between two things, we can use correlation. The higher the correlation, more similar two things are. The assumption that something loses similarity with itself implies that initially there was a similarity but over time we lost it. To quantify similarity of something with itself at a later time, we can calculate the correlation between that thing and that thing pushed forward in time. It is easier to see this with a figure. ","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"(Image: )","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"We took the time series x and shifted it forward in time by an amount Deltat. Then we need to take the correlation between them. To take a correlation between two things, you need to have equal number of data points in each of them. This is due to the definition of correlation, correlation is the average value of multiplication normalized by variance. You need to multiplicate corresponding data points. Take a look at the code example below. Throughout the documentation, there will be many code examples. I encourage you to run them on your computer and play around with them. Even better, take a pen and piece of paper and do the calculation below yourself. There is no better way to train intuition other than grinding your way through a calculation but I digress. Here is the code:","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"using Statistics # Import Statistics package for cor function\nx1 = [-1, 0, 1] # example data\nx2 = [2, -2, 0]\nvariance_x1 = sqrt(sum(x1 .^ 2) / 3) # Calculate variance of each dataset\nvariance_x2 = sqrt(sum(x2 .^ 2) / 3)\n# Covariance is the average value of multiplication\ncovariance_x1_x2 = (x1[1]*x2[1] + x1[2]*x2[2] + x1[3]*x2[3]) / 3\n# Correlation is normalized covariance\ncorrelation_x1_x2 = covariance_x1_x2 / (variance_x1 * variance_x2)\nisapprox(correlation_x1_x2, cor(x1, x2)) # Compare with cor function from Statistics package","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"This looks basic, but makes an important point. As you go forward in time, you need to match the time points in your time series and shifted version of it. In the figure above, the only usable part is the part indicated in black vertical lines. This means as we shift further in time, we have less time points at our disposal and our correlation results are less reliable. We will return back to this point later. ","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"We took a time-series, shifted it by an amount Deltat, calculated the correlation and if the result is not zero, then we can say that the time series still haven't lost similarity to itself in Deltat amount of time. Take a moment to ponder about this sentence. We are insinuating that there is such a Deltat where the correlation is zero, or close to zero and this is the time it takes for a signal to lose similarity with itself. This is our INT. ","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"Then a good strategy to calculate INT is simply calculating the correlation at various Deltat values and detecting which Deltat is the time where we lose correlation. Let's code this. We'll use the function generate_ou_process from the IntrinsicTimescales.jl package. This function simulates time series with a known timescale. I'll explain more about what it is doing in [Theory] section. For now, just know that this exists and is a good toy to play with. In IntrinsicTimescales.jl package, we have more optimized ways to do the operation I'll write below. I am doing this below explicitly and in detail so that we know exactly what we are doing when we compute these things. ","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"using IntrinsicTimescales # import INT package\nusing Random \nusing Plots # to plot the results\nRandom.seed!(1) # for replicability\n\ntimescale = 1.0\nsd = 1.0 # sd of data we'll simulate\ndt = 0.001 # Time interval between two time points\nduration = 10.0 # 10 seconds of data\nnum_trials = 1 # Number of trials\n\ndata = generate_ou_process(timescale, sd, dt, duration, num_trials)\ndata = data[:] # Go from a (1, time) matrix to (time) vector","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"The resulting data from generate_ou_process is a matrix where rows are different trials and columns are time points. In order to simplify the code below, I do the operation data = data[:] to turn it into a one dimensional vector. ","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"The next step is doing shifting forward in time and correlating on this data. Look at the code below, take a piece of pen and paper and explicitly write down the indexing operations for different values of Deltat to get a sense of how we are implementing this. Essentially, we are finding the indices corresponding to the data between the black vertical lines shown in the figure above. ","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"n_timepoints = length(data)\nn_lags = 4000 # Calculate the first 4000 lags.\ncorrelation_results = zeros(n_lags) # Initialize empty vector to fill the results\n# Start from no shifting (0) and end at number of time points - 1. \nlags = 0:(n_lags-1)\nfor DeltaT in lags\n    # Get the indices for the data in vertical lines\n    indices_data = (DeltaT+1):n_timepoints\n    indices_shifted_data = 1:(n_timepoints - DeltaT)\n    correlation_results[DeltaT+1] = cor(data[indices_data], data[indices_shifted_data])\nend\nplot(lags, correlation_results, label=\"\") \nhline!([0], color=:black, label=\"\") # Indicate the zero point of correlations","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"(Image: )","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"This is called an autocorrelation function (ACF). On x axis, we have lags. One lag means we shifted one of the time series by one data point. On y axis, we plot the correlation values. Note that it starts from 1. Because when lag is zero, we did not shift any time series. We are correlating a time series with exactly itself and the correlation between one thing and itself is simply one. As we expected, the ACF decays as we shift lags. We can identify the lag where the correlation reaches zero. This is the first estimate of our timescale. This measure is called ACW-0 which stands for autocorrelation window-0. It was first used by Mehrshad Golesorkhi in his 2021 paper and he found that ACW-0 differentiates brain regions better than previously used methods. Let's calculate the ACW-0 and indicate it in the plot with a vertical red line. ","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"acw_0 = findfirst(correlation_results .< 0)\nplot(correlation_results, xlabel=\"Lags\", ylabel=\"Correlation\", label=\"\")\nhline!([0], color=:black, label=\"\")\nvline!([acw_0], color=:red, label=\"ACW-0\")","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"(Image: )","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"So our work is done, right? We started with 1) the definition that INT is the time it takes for a time-series to lose its similarity with itself, 2) operationalized similarity with correlation, 3) operationalized similarity with itself as correlation with itself shifted some time lags and 4) identified the INT as the number of time lags required to lose similarity. There is one problem. Remember the problem of number of time points we talked about above. As we go further in lags, we have less and less number of data points to calculate the correlation, the portion inside vertical black lines is getting smaller and smaller. If we do not have enough number of data points to calculate ACW-0, then we will get a noisy estimate. ","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"Let's try to see how big of a problem this is. Below, we will simulate the time-series again and again and overlay plots of ACFs. In a different panel, we'll do a histogram of ACW-0 values. To calculate ACF, we will use the function comp_ac_fft from  INT.jl package. This function is faster and uses a different technique to calculate ACF which I'll explain in the [next section]. For now, it should suffice to know that it takes the data and optionally the number of lags we want as input and gives back the ACF. If number of lags is not specified, it goes through all possible lags. To get the ACW-0 from the ACF, we'll use acw0 function which takes lags and ACF as input and gives ACW-0 value. ","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"acw0_results = [] # Initialize empty vectors to hold the results\nacfs = []\nn_simulations = 10\nfor _ in 1:n_simulations\n    data = generate_ou_process(timescale, sd, dt, duration, num_trials)[:]\n    acf = comp_ac_fft(data; n_lags=n_lags)\n    i_acw0 = acw0(lags, acf)\n    push!(acw0_results, i_acw0) # Same as .append method in python\n    push!(acfs, acf)\nend\np1 = plot(lags, acfs, xlabel=\"Lags\", ylabel=\"Correlation\", \n          label=\"\", title=\"ACF\", alpha=0.5)\nhline!([0], color=:black, label=\"\")\n\np2 = histogram(acw0_results, xlabel=\"ACW-0\", ylabel=\"Count\",\n               label=\"\", title=\"Distribution of ACW-0\")\n\n# Combine the plots side by side\nplot(p1, p2, layout=(1,2))","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"(Image: )","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"What's going on  here? We simulated the same process 10 times and each time we got a different result. All simulations had the same timescale, which we set as 1.0 above. Why did we get different results? Didn't we start by saying that even the data is random, statistical properties of it are not? That we can flip a coin 1000 times and on average half the time it will be heads and half the time it will be tails? Well, not quite. We said that on average, half the time it will be heads and the other half, it will be tails. Let's define an experiment as flipping the coin 1000 times. If you do this experiment once and look at the results, perhaps it will be 498 heads and 502 tails. Then do the experiment again, it will maybe give you 505 heads and 495 tails. You do the experiment again and again and keep track of the results. Then if you average over experiments, you'll see that there are 500 heads and 500 tails in the end. You can do a mini version of this experiment at home with 10 coin flips. The more experiments you do, the better the results will be. ","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"Here is the central insight: When we calculate ACW-0 from limited data, we are not doing a perfect calculation. We are making an estimation. Based on the data we know, this is the timescale we think. And estimations are noisy. The noisiness of the estimation depends on the properties of data. The more number of data points we have, the better the estimations are. This is why I stressed that as we calculate ACF in later and later lags, our estimations become less and less reliable simply because we have less number of data points at our disposal. To see it clearly, look at the figure in the left panel and observe that at earlier lags, the variance between ACF estimates are low and it progressively increases as you go along later lags. Feel free to change the parameters dt,  timescale and duration to see how they change results. ","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"This is why it is crucial to not only know your research problem, be it cognitive or basic neuroscience, but also the estimators you use to tackle the problem. How noisy are they? How much they are vulnerable to the number of data points? Are there other things in the data that might bias the results? Just because you are getting a number out of some algorithm does not mean that number has any meaning. It is the responsibility of the researcher, you to make sure your numbers make sense. ","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"In the next section, we will explore various kinds of autocorrelation windows, their motivation and how they address the bias. ","category":"page"},{"location":"simbasedinference/#sim","page":"Overview","title":"Simulation Based Timescale Estimation","text":"","category":"section"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"In simulation based methods, your data is assumed to come from a generative model and INT.jl performs Bayesian parameter estimation (via approximate Bayesian computation, ABC or automatic differentiation variational inference, ADVI) on that model. The goal is to match the autocorrelation function (ACF) or equivalently, power spectral density (PSD) of the generative model and data. The simplest generative model is an Ornstein-Uhlenbeck (OU) process with only one parameter to estimate. In case of oscillations, an oscillation is linearly added to the output of the Ornstein-Uhlenbeck process. If some of your data is missing, indicated by NaN or missing, the data points from the generative model are replaced by NaNs. We note that the variance of noise in the OU process is not fit to data as we scale the output of simulations to match the variance of data in order to reduce the burden or parameter fitting procedure. ","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"All methods assume that your data has one dimension for trials and one dimension for time points. From each trial, IntrinsicTimescales.jl calculates one summary statistic (ACF or PSD) and averages them across trials to get a less noisy estimate. The simulations from the generative model have the same data structure (same number of data points, trials and time resolution) as your data. The goal of the simulation based methods is minimizing the distance between the ACF or PSD of your model and data. Then the parameter corresponding to INT in your model is hopefully the real INT. ","category":"page"},{"location":"simbasedinference/#Model-Types","page":"Overview","title":"Model Types","text":"","category":"section"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"There are four main functions in IntrinsicTimescales.jl to perform simulation based timescale estimation: one_timescale_model, one_timescale_and_osc_model, one_timescale_with_missing_model, one_timescale_and_osc_with_missing_model. For each model, one can choose between :abc or :advi as the inference method and :acf or :psd as the summary method. All models have the same syntax with differences in implementation. The detailed usage is documented in one_timescale_model - other model pages focus on specific differences. ","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"The following table summarizes the four models. ","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"Model Generative Model Summary Method (:acf or :psd) Supported Inference Methods (:abc or :advi)\none_timescale_model Ornstein-Uhlenbeck process comp_ac_fft or comp_psd_adfriendly ABC and ADVI\none_timescale_and_osc_model Sinusoid added on Ornstein-Uhlenbeck process comp_ac_fft or comp_psd_adfriendly ABC and ADVI\none_timescale_with_missing_model Ornstein-Uhlenbeck process with missing data replaced by NaNs comp_ac_time_missing or comp_psd_lombscargle ABC (for both ACF and PSD), ADVI (only ACF)\none_timescale_and_osc_with_missing_model Sinusoid added on Ornstein-Uhlenbeck process with missing data replaced by NaNs comp_ac_time_missing or comp_psd_lombscargle ABC (for both ACF and PSD), ADVI (only ACF)","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"All models are fit with fit function and return ADVIResults or ABCResults type. See the Fitting and Results section for details. ","category":"page"},{"location":"simbasedinference/#Fitting-Methods-ABC","page":"Overview","title":"Fitting Methods - ABC","text":"","category":"section"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"Approximate Bayesian Computation (ABC) is a method to approximate the posterior without solving the likelihood function. The algorithm has two steps: ABC (basic_abc) and population monte carlo (PMC, pmc_abc). In pseudocode, ABC is as follows:","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"summary = summary_statistic(empirical_data)\naccepted_samples = []\nWHILE length(accepted_samples) < min_accepted\n    theta = sample_from_prior()\n    model_data = simulate_data(model, theta)\n    distance = compute_distance(summary, model_data)\n    IF distance < epsilon\n        push!(accepted_samples, theta)\n    END IF\nEND WHILE","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"PMC uses ABC samples as the initial population and iteratively updates. For more details, refer to Zeraati et al, 2021. ","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"To change the parameters of the ABC algorithm, first use the function get_param_dict_abc to get the default parameters. Then modify the parameters and pass them to the function fit. For example, ","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"model = one_timescale_model(data, time, :abc)\nparam_dict = get_param_dict_abc()\nparam_dict[\"convergence_window\"] = 10\nresult = fit(model, param_dict)\nint_map = result.MAP[1] # Maximum a posteriori ","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"The parameters are detailed in Parameters for Approximate Bayesian Computation section.","category":"page"},{"location":"simbasedinference/#Fitting-Methods-ADVI","page":"Overview","title":"Fitting Methods - ADVI","text":"","category":"section"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"Automatic Differentiation Variational Inference (ADVI) approximates the posterior using variational methods. Instead of using MCMC directly, ADVI uses gradient descent to find the optimal parameters that minimize the Kullback-Leibler divergence between the variational posterior and the true posterior. IntrinsicTimescales.jl uses the Turing.jl package to perform ADVI. For more details, refer to Turing documentation or Kucukelbir et al, 2017. ","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"IntrinsicTimescales.jl states the probabilistic problem as the likelihood of each data point in the summary statistic of the data coming from a Gaussian distribution with mean generative model's summary statistic and some uncertainty around it. More clearly:","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"textrmdata summary statistic_i sim N(textrmmodel summary statistic_i sigma)","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"Similar to ABC, in order to change the parameters of the ADVI algorithm, use the function get_param_dict_advi to get the default parameters, modify them, and pass them to the function fit. Example:","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"model = one_timescale_and_osc_model(data, time, :advi)\nparam_dict = get_param_dict_advi()\nparam_dict[\"n_iterations\"] = 20\nfit(model, param_dict)","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"See Parameters for Automatic Differentiation Variational Inference section for details on parameters.","category":"page"},{"location":"simbasedinference/#Notes-on-Summary-Statistics","page":"Overview","title":"Notes on Summary Statistics","text":"","category":"section"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"Each model uses either ACF or PSD as the summary statistic. As can be seen from the table above, with no missing data, comp_ac_fft and comp_psd_adfriendly are used. comp_ac_fft calculates the ACF using the fast fourier transform (FFT). comp_psd_adfriendly is an autodifferentiable implementation of comp_psd; both use Periodogram method with a Hamming window. In case of missing data, ACF is calculated in the time domain with the same techniques used in statsmodels.tsa.stattools.acf with missing=conservative option. For PSD, Lomb-Scargle method (via LombScargle.jl) with the function  comp_psd_lombscargle is used but currently it is not autodifferentiable. If you wish to use PSD with simulation-based inference in the case of missing data, you would need to use ABC. ","category":"page"},{"location":"practice/practice_2_acw/#Autocorrelation-Windows","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"","category":"section"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"We finished the previous section with a discussion about how a determinstic statistic can be influenced by the limitations of our data. In this section, we will generalize the problem and discuss various autocorrelation window (ACW) types. ","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"First, some nomenclature. When we make an analysis on the data, for example, calculate event-related potentials, ACWs and so on, we are aiming for an estimand. In event-related potentials, our estimand is the stereotypical response of the brain to some cognitive task. In ACWs, our estimand is intrinsic neural timescales (INTs). The ACW we get is not INT per se, it is the estimate of INT. To obtain the estimate, we use an estimator. The schema below shows the relationship. ","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"(Image: )","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"Our first note about the noise of estimators was the finiteness of the data. We noted that as we go along further lags, we have less data points at our hand to calculate the correlation values, making the estimate noisier. A first response to the problem is to use a different cutoff. Instead of waiting the autocorrelation function to reach exactly to 0 thus completely losing the similarity, we can cut it off when it reaches 0.5 and say losing half of the similarity. After all, a time-series with a longer timescale should take longer to lose half of it. This method is called ACW-50. It is  older than ACW-0. To my knowledge, used first in Honey et al., 2012. This was a time when the phrase intrinsic neural timescale had not been established. The term at that time was temporal receptive windows (TRW). I will discuss the evolution of the term more in the [Theory] section. For now, we will make simulations from two processes with different timescales and see how well we can distinguish their INTs using ACW-50 versus ACW-0. To quickly get many simulations with the same timescale, I will set numtrials to 1000 in the function [`generateou_process`](@ref). ","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"using IntrinsicTimescales # import IntrinsicTimescales package\nusing Random \nusing Plots # to plot the results\nRandom.seed!(1) # for replicability\n\ntimescale_1 = 1.0\ntimescale_2 = 3.0\nsd = 1.0 # sd of data we'll simulate\ndt = 0.001 # Time interval between two time points\nduration = 10.0 # 10 seconds of data\nnum_trials = 1000 # Number of trials\n\ndata_1 = generate_ou_process(timescale_1, sd, dt, duration, num_trials)\ndata_2 = generate_ou_process(timescale_2, sd, dt, duration, num_trials)\nprintln(size(data_1)) # == 30, 1000: 30 trials and 10000 time points","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"To streamline the ACW calculation, I will use the acw function from INT.jl. This function takes your time series data, sampling rate and ACW types you want to calculate and returns the ACW values in the same shape of the data. Along with ACW results it also returns additional information that will be useful later. To extract ACW values, we will extract the field acw_results from the output of acw. It is best to demonstrate with an example. ","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"fs = 1 / dt # sampling rate\nacwresults_1 = acw(data_1, fs, acwtypes=[:acw50, :acw0]) \nacwresults_2 = acw(data_2, fs, acwtypes=[:acw50, :acw0])\n# Since we used the order [:acw50, :acw0], the first element of results is ACW-50, the second is ACW-0.\nacw50_1 = acwresults_1.acw_results[1]\nacw0_1 = acwresults_1.acw_results[2]\nacw50_2 = acwresults_2.acw_results[1]\nacw0_2 = acwresults_2.acw_results[2]","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"How to quantify the sensitivity of the estimator (to changes in the timescale)? Let's assume an experimental scenario where we are comparing INTs of two conditions or two groups. We are calculating one timescale from each condition. The number of trials (num_trials above) can refer to either number of trials or subjects. Then we'll compare the INTs from two groups. We know for a fact that first condition has a shorter timescale than the second since we set them ourselves in the code above (timescale_1 and _2). We will calculate what percentage of the time we are wrong, that we are getting a longer or equal INT for the first condition and a shorter or equal INT for the second condition. ","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"Take a look at the code below, we will calculate what I described in the previous awful paragraph. Hopefully the code is cleaner than my English. Additionally, we will plot histograms to visualize the overlap between estimates. ","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"using Printf\n\nbad_acw50_timescale = mean(acw50_2 .<= acw50_1) * 100\nbad_acw0_timescale = mean(acw0_2 .<= acw0_1) * 100\n\n# Plot histograms\np1 = histogram(acw50_1, alpha=0.5, label=\"timescale 1 = $(timescale_1)\")\nhistogram!(p1, acw50_2, alpha=0.5, label=\"timescale 2 = $(timescale_2)\")\n# Plot the median since distributions are not normal\nvline!(p1, [median(acw50_1), median(acw50_2)], linewidth=3, color=:black, label=\"\") \ntitle!(p1, \"ACW-50\\n\")\n# Mad string manipulation\nannotate!(p1, 1, 100, \n    (@sprintf(\"Proportion of \\\"wrong\\\" timescale \\nestimates: %.2f%% \\n\", bad_acw50_timescale)), textfont=font(24), :left)\n# ACW-0\np2 = histogram(acw0_1, alpha=0.5, label=\"timescale 1 = $(timescale_1)\")\nhistogram!(p2, acw0_2, alpha=0.5, label=\"timescale 2 = $(timescale_2)\")\n\nvline!(p2, [median(acw0_1), median(acw0_2)], linewidth=3, color=:black, label=\"\")\ntitle!(p2, \"ACW-0\\n\")\nannotate!(p2, 2, 175, \n    (@sprintf(\"Proportion of \\\"wrong\\\" timescale \\nestimates: %.2f%% \\n\", bad_acw0_timescale)), textfont=font(24), :left)\nplot(p1, p2, size=(1600, 800))","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"(Image: )","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"It seems ACW-0 gives messier results. Needless to say, these results depend on the difference between real timescales and the amount of data. Feel free to change these parameters and investigate the results under different scenarios. So ACW-50 seems to be a better estimator at least in the setting we specified above. Is our work done then? First of all, we used a weird way to define \"wrong\". We've reduced the correctness to a binary choice of is something greater or smaller than other. We can be more principled than this and actually quantify how much we are off. We will do this in [the next section]. For now, let's consider another scenario. In the example above, we had dt = 0.001 implying our sampling rate (fs) is 1000 Hz and we have 10 seconds of data. This sounds like an EEG/MEG scenario. Let's try an fMRI scenario where we have a sampling rate of 0.5 Hz (corresponding TR=2 seconds) and 300 seconds of data. We'll set the timescales to 1 seconds and 3 seconds for short timescale and long timescale guys. ","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"timescale_1 = 1.0\ntimescale_2 = 3.0\nsd = 1.0 \ndt = 2.0 # Time interval between two time points\nduration = 300.0 # 5 minutes of data\nnum_trials = 1000 # Number of trials\n\ndata_1 = generate_ou_process(timescale_1, sd, dt, duration, num_trials)\ndata_2 = generate_ou_process(timescale_2, sd, dt, duration, num_trials)\n\nfs = 1 / dt # sampling rate\nacwresults_1 = acw(data_1, fs, acwtypes=[:acw50, :acw0]) \nacwresults_2 = acw(data_2, fs, acwtypes=[:acw50, :acw0])\nacw50_1 = acwresults_1.acw_results[1]\nacw0_1 = acwresults_1.acw_results[2]\nacw50_2 = acwresults_2.acw_results[1]\nacw0_2 = acwresults_2.acw_results[2]\n\nbad_acw50_timescale = mean(acw50_2 .<= acw50_1) * 100\nbad_acw0_timescale = mean(acw0_2 .<= acw0_1) * 100\n\n# Plot histograms\np1 = histogram(acw50_1, alpha=0.5, label=\"timescale 1 = $(timescale_1)\")\nhistogram!(p1, acw50_2, alpha=0.5, label=\"timescale 2 = $(timescale_2)\")\nvline!(p1, [median(acw50_1), median(acw50_2)], linewidth=3, color=:black, label=\"\") \ntitle!(p1, \"ACW-50\\n\")\nannotate!(p1, 1, 100, \n    (@sprintf(\"Proportion of \\\"wrong\\\" timescale \\nestimates: %.2f%% \\n\", bad_acw50_timescale)), textfont=font(24), :left)\n# ACW-0\np2 = histogram(acw0_1, alpha=0.5, label=\"timescale 1 = $(timescale_1)\")\nhistogram!(p2, acw0_2, alpha=0.5, label=\"timescale 2 = $(timescale_2)\")\n\nvline!(p2, [median(acw0_1), median(acw0_2)], linewidth=3, color=:black, label=\"\")\ntitle!(p2, \"ACW-0\\n\")\nannotate!(p2, 2, 175, \n    (@sprintf(\"Proportion of \\\"wrong\\\" timescale \\nestimates: %.2f%% \\n\", bad_acw0_timescale)), textfont=font(24), :left)\nplot(p1, p2, size=(1600, 800))","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"(Image: )","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"Half the time, we got the wrong result with ACW-50! To diagnose the problem, let's plot the autocorrelation functions. This is where the other information stored in the output of acw comes useful. We'll use the function acwplot to plot the ACFs. This function plots the ACFs and returns a plot object which we can modify later. We'll put vertical lines at the lags where we compute autocorrelation. Note that the lags are also stored in the output of acw. To not plot 1000 ACFs for each trial, let's resimulate data with a reasonable number of trials. ","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"num_trials = 20 # Number of trials\ndata_1 = generate_ou_process(timescale_1, sd, dt, duration, num_trials)\ndata_2 = generate_ou_process(timescale_2, sd, dt, duration, num_trials)\nacwresults_1 = acw(data_1, fs, acwtypes=[:acw50, :acw0]) \nacwresults_2 = acw(data_2, fs, acwtypes=[:acw50, :acw0])\np = acwplot(acwresults_1)\nvline!(p, [acwresults_1.lags], linewidth=3, color=:black, label=\"\")","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"(Image: )","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"The autocorrelation is dropping below 0.5 before even 2 seconds pass. And because our time resolution was two seconds, most of the autocorrelation functions drop below 0.5 even before we can calculate ACW-50. ","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"There is one more method in case ACW-50 is not working well. Let's consider the case above: we want to be able to distinguish the processes but we don't have the time resolution to use ACW-50. We can use ACW-0 but the later lags are more noisy. Wouldn't it be great if we had a method that assigns higher weights to earlier lags and lower weight to less reliable later lags? Turns out there is one such method. We can calculate the area under the curve (AUC) of the ACF. Since later lags have less correlation, their contribution to the area under the curve will be less. In IntrinsicTimescales.jl, we can use :auc to calculate the AUC of ACF before ACF touches 0. Under the hood, this method uses Romberg.jl to use Romberg's method. This method is orders of magnitude more accurate than trapezoid (as in np.trapz or MATLAB trapz) and Simpson's methods (as in scipy.integrate.simpson). Let's repeat the above experiment to compare ACW-0 and AUC methods:","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"timescale_1 = 1.0\ntimescale_2 = 3.0\nsd = 1.0 \ndt = 2.0 # Time interval between two time points\nduration = 300.0 # 5 minutes of data\nnum_trials = 1000 # Number of trials\n\ndata_1 = generate_ou_process(timescale_1, sd, dt, duration, num_trials)\ndata_2 = generate_ou_process(timescale_2, sd, dt, duration, num_trials)\n\nfs = 1 / dt # sampling rate\nacwresults_1 = acw(data_1, fs, acwtypes=[:auc, :acw0]) \nacwresults_2 = acw(data_2, fs, acwtypes=[:auc, :acw0])\nauc_1 = acwresults_1.acw_results[1]\nacw0_1 = acwresults_1.acw_results[2]\nauc_2 = acwresults_2.acw_results[1]\nacw0_2 = acwresults_2.acw_results[2]\n\nbad_auc_timescale = mean(auc_2 .<= auc_1) * 100\nbad_acw0_timescale = mean(acw0_2 .<= acw0_1) * 100\n\n# Plot histograms\np1 = histogram(auc_1, alpha=0.5, label=\"timescale 1 = $(timescale_1)\")\nhistogram!(p1, auc_2, alpha=0.5, label=\"timescale 2 = $(timescale_2)\")\nvline!(p1, [median(auc_1), median(auc_2)], linewidth=3, color=:black, label=\"\") \ntitle!(p1, \"AUC\\n\")\nannotate!(p1, 3, 100, \n    (@sprintf(\"Proportion of \\\"wrong\\\" timescale \\nestimates: %.2f%% \\n\", bad_auc_timescale)), textfont=font(24), :left)\n# ACW-0\np2 = histogram(acw0_1, alpha=0.5, label=\"timescale 1 = $(timescale_1)\")\nhistogram!(p2, acw0_2, alpha=0.5, label=\"timescale 2 = $(timescale_2)\")\n\nvline!(p2, [median(acw0_1), median(acw0_2)], linewidth=3, color=:black, label=\"\")\ntitle!(p2, \"ACW-0\\n\")\nannotate!(p2, 20, 300, \n    (@sprintf(\"Proportion of \\\"wrong\\\" timescale \\nestimates: %.2f%% \\n\", bad_acw0_timescale)), textfont=font(24), :left)\nplot(p1, p2, size=(1600, 800))","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"(Image: )","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"We've reduced the \"wrong\" estimates to 1.20%, quite impressive! ","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"It seems different modalities and temporal resolutions call for different ways to calculate ACW. But even with the better estimator, we can still be off 1/5th of the time. Can we do better? Remember the coin flipping experiment from [the previous section]. We said that if we repeat the experiment again and again and take average across experiments, our estimates get better. Let's do this in the context of ACW-50. In the code below, I will first make 1000 simulations, then from each one of them, I'll calculate one autocorrelation function. Then I'll cumulatively average those autocorrelation functions, i.e. I'll average the first two ACFs, the first three, the first four... Then I'll calculate ACW-50 and ACW-0 from each step. Let's see if they are converging. ","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"using Statistics, IntrinsicTimescales, Plots, Random\nRandom.seed!(123)\ntimescale = 3.0\nsd = 1.0 # sd of data we'll simulate\ndt = 0.001 # Time interval between two time points\nfs = 1 / dt\nduration = 10.0 # 10 seconds of data\nnum_trials = 1\nacfs = []\nacw50s = []\nacw0s = []\nn_experiments = 1000\nfor _ in 1:n_experiments\n    data = generate_ou_process(timescale, sd, dt, duration, num_trials)\n    acf = comp_ac_fft(data[:])\n    push!(acfs, acf)\n    current_mean_acf = mean(acfs)\n    lags = (0:(length(current_mean_acf)-1)) * dt\n    current_acw50 = acw50(lags, current_mean_acf)\n    current_acw0 = acw0(lags, current_mean_acf)\n    push!(acw50s, current_acw50)\n    push!(acw0s, current_acw0)\nend\np1 = plot(acw50s, label=\"ACW-50\", xlabel=\"Iterations\", ylabel=\"ACW\")\np2 = plot(acw0s, label=\"ACW-0\", xlabel=\"Iterations\", ylabel=\"ACW\")\nplot(p1, p2, size=(800, 400))","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"(Image: )","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"Note that it takes about 250 trials for the estimates to completely stabilize. This is a huge number. In the case above, we assumed that each trial is 10 seconds. 250 trials x 10 seconds is 41 minutes of data which we usually don't have. Nonetheless, even averaging across a couple trials makes the estimates much closer to the stabilized estimate. This is why the Honey et al. paper I mentioned above calculated one ACF from 20 seconds of data and averaged over ACFs. In IntrinsicTimescales.jl, this is handled by the argument average_over_trials in the acw function. It is your responsibility to put your data in a format where one dimension is trials and one dimension is time. This is usually handled with MNE or FieldTrip helper functions (see for example mne.makefixedlength_epochs, this tutorial from MNE or ft_redefinetrial. If you think taking continuous data and segmenting with different overlap degrees would be useful for you in IntrinsicTimescales.jl, open an issue on github and I can add this as a feature.). Before finishing this section, let's run one final experiment where we now have 20 trials for each subject and 100 subjects. Let's do a t-test between the groups and see if we can capture the difference. To run the code below, you'll need to install the Julia package HypothesisTests. ","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"using HypothesisTests\nRandom.seed!(123)\nn_subjects = 100\nn_trials = 20\nnum_trials = 20\ntimescale_1 = 1.0\ntimescale_2 = 3.0\nsd = 1.0\ndt = 0.001\nfs = 1 / dt\nduration = 10.0\n\nacw50_1 = Float64[] # HypothesisTests doesn't accept Vector{Any} type, requires Vector{<:Real} type\nacw50_2 = Float64[]\nacw0_1 = Float64[]\nacw0_2 = Float64[]\nfor _ in 1:n_subjects\n    data_1 = generate_ou_process(timescale_1, sd, dt, duration, num_trials)\n    data_2 = generate_ou_process(timescale_2, sd, dt, duration, num_trials)\n    acwresults_1 = acw(data_1, fs, acwtypes=[:acw50, :acw0], average_over_trials=true)\n    acwresults_2 = acw(data_2, fs, acwtypes=[:acw50, :acw0], average_over_trials=true)\n    current_acw50_1 = acwresults_1.acw_results[1]\n    current_acw50_2 = acwresults_2.acw_results[1]\n    current_acw0_1 = acwresults_1.acw_results[2]\n    current_acw0_2 = acwresults_2.acw_results[2]\n    push!(acw50_1, current_acw50_1)\n    push!(acw50_2, current_acw50_2)\n    push!(acw0_1, current_acw0_1)\n    push!(acw0_2, current_acw0_2)\nend\n\nbad_acw50_timescale = mean(acw50_2 .<= acw50_1) * 100\nbad_acw0_timescale = mean(acw0_2 .<= acw0_1) * 100\n\n# Plot histograms\np1 = histogram(acw50_1, alpha=0.5, label=\"timescale 1 = $(timescale_1)\")\nhistogram!(p1, acw50_2, alpha=0.5, label=\"timescale 2 = $(timescale_2)\")\n# Plot the median since distributions are not normal\nvline!(p1, [median(acw50_1), median(acw50_2)], linewidth=3, color=:black, label=\"\") \ntitle!(p1, \"ACW-50\\n\")\nannotate!(p1, 0.6, 25, \n    (@sprintf(\"Proportion of \\\"wrong\\\" timescale \\nestimates: %.2f%% \\n\", bad_acw50_timescale)), textfont=font(24), :left)\n# ACW-0\np2 = histogram(acw0_1, alpha=0.5, label=\"timescale 1 = $(timescale_1)\")\nhistogram!(p2, acw0_2, alpha=0.5, label=\"timescale 2 = $(timescale_2)\")\n\nvline!(p2, [median(acw0_1), median(acw0_2)], linewidth=3, color=:black, label=\"\")\ntitle!(p2, \"ACW-0\\n\")\nannotate!(p2, 2, 30, \n    (@sprintf(\"Proportion of \\\"wrong\\\" timescale \\nestimates: %.2f%% \\n\", bad_acw0_timescale)), textfont=font(12), :left)\nplot(p1, p2, size=(1600, 800))\n\nprintln(UnequalVarianceTTest(acw50_1, acw50_2))\nprintln(UnequalVarianceTTest(acw0_1, acw0_2))","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"(Image: )","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"Note that our wrong estimates for ACW-50 reduced to 0! ACW-0 is still slightly noisy but much better now. You can also check out the t-test results, both ACWs returned a significant difference. This approach offers a way to see how many subjects you need to get a significant difference if your hypothesis is right. You can copy-paste the script above to play around with it when designing experiments and figuring out the number of subjects you need for different effect sizes. ","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"This was a long tutorial. Take a deep breath, make a coffee for yourself, go for a walk and come back for the next one. There is still work to do: we need to figure out how to calculate exactly how wrong are we. Under certain assumptions, we can actually do this. But we need some theoretical tools. In the [next section], We'll develop those theoretical tools and they will motivate us to calculate ACW in different ways.  ","category":"page"},{"location":"acw/#acw","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"","category":"section"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"Performed via the function acw in IntrinsicTimescales.jl. The acw function calculates ACF or PSD depending on the acwtypes you specify. If there is no missing data (indicated by NaN or missing), acw calculates ACF as the inverse fourier transform of the power spectrum, using comp_ac_fft internally. Otherwise it calculates ACF as correlations between a time-series and its lag-shifted variants, using comp_ac_time_missing. For PSD, it uses periodogram method (comp_psd) in the case of no missing data and Lomb-Scargle method (comp_psd_lombscargle) in the case of missing data. ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"acwresults = acw(data, fs; acwtypes=[:acw0, :acw50, :acweuler, :auc, :tau, :knee], \n                n_lags=nothing, freqlims=nothing, dims=ndims(data), \n                return_acf=true, return_psd=true, \n                average_over_trials=false, trial_dims=setdiff([1, 2], dims)[1],\n                max_peaks=1, oscillation_peak::Bool=true)","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"Simple usage:","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"results = acw(data, fs)\nacw_results = results.acw_results\nacw_0 = acw_results[1]\nacw_50 = acw_result[2]\n# And so on...","category":"page"},{"location":"acw/#Arguments","page":"Model-Free Timescale Estimation","title":"Arguments","text":"","category":"section"},{"location":"acw/#Mandatory-arguments:","page":"Model-Free Timescale Estimation","title":"Mandatory arguments:","text":"","category":"section"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"data: Your time-series data as a vector or n-dimensional array. ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"If it is n-dimensional, by default, the dimension of time is assumed to be the last dimension. For example, if you have a 2D array where rows are subjects and columns are time points, acw function will correctly assume that the last (2nd) dimension is time. If the dimension of time is any other dimension than the last, you can set it via dims argument. For example: ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"data = randn(100, 200, 5) # 100 trials, 200 time points, 5 channels\nacw(data, fs; dims=2)","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"fs: Sampling rate of your data. A floating point number. ","category":"page"},{"location":"acw/#Optional-arguments:","page":"Model-Free Timescale Estimation","title":"Optional arguments:","text":"","category":"section"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"acwtypes: A symbol or a vector of symbols denoting which ACW types to calculate. ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"In Julia, a symbol is written as :symbol. Example:","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"acw(data, fs; acwtypes=:acw0)\nacw(data, fs; acwtypes=[:acw0, :acw50])","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"Supported ACW types:","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":":acw0: The lag where autocorrelation function crosses 0.","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":":acw50: The lag where autocorrelation function crosses 0.5.","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":":acweuler: The lag where autocorrelation function crosses 1e. Corresponds to the inverse decay rate of an exponential decay function. ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":":tau: Fit an exponential decay function e^fracttau to the autocorrelation function and extract tau, which is the inverse decay rate. ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":":auc: Calculate the area under the autocorrelation function from lag 0 to the lag where autocorrelation function crosses 0. ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":":knee: Fit a lorentzian function fracA1 + (fa)^2 to the power spectrum using an iterative FOOOF-style approach. By Wiener-Khinchine theorem, this is the power spectrum of a time-series with an autocorrelation function of exponential decay form. The parameter a corresponds to the knee frequency. tau and a has the relationship tau = frac12 pi a. The :knee method uses this relationship to estimate tau from the knee frequency. In practice, first, an initial lorentzian fit is performed. Then, any oscillatory peaks are identified and fitted with gaussian functions. These gaussians are subtracted from the original power spectrum to ensure the remaining PSD is closer to a Lorentzian, and a final Lorentzian is fit to this \"cleaned\" spectrum. You can set the maximum number of oscillatory peaks to fit with the max_peaks argument. The argument oscillation_peak is used to specify whether to fit the oscillatory peaks or not. If set to false, just fit a Lorentzian and return the timescale estimated from the knee frequency. ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"n_lags: An integer. Only used when :tau is in acwtypes. The number of lags to be used for fitting an exponential decay function. ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"By default, this is set to 1.1*acw0. The reason for cutting the autocorrelation function is due to increased measurement noise for autocorrelation function for later lags. Intuitively, when we perform correlation of a time-series with a shifted version of itself, we have less and less number of time points to calculate the correlation. This is why if you plot the autocorrelation function, the portion of it after ACW-0 looks more noisy. For more details, see [Practice 1]. ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"freqlims: Only used when :knee is in acwtypes. The frequency limits to fit the lorentzian function. ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"By default, the lowest and highest frequencies that can be estimated from your data. A tuple of two floating point numbers, for example, (freq_low, freq_high) or (1.0, 50.0). ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"dims: The dimension of time in your data. See above, the data argument for the explanation. An integer. \nreturn_acf: Whether or not to return the autocorrelation function (ACF) in the results object. A boolean. \nreturn_psd: Whether or not to return the power spectrum (psd) in the results object. A boolean. \naverage_over_trials: Whether or not to average the ACF or PSD across trials, as in [Honey et al., 2012]. ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"Assuming that your data is stationary, averaging over trials can greatly reduce noise in your ACF/PSD estimations. By default, the dimension of trials is assumed to be the first dimension of your data. For example, if your data is two dimensional with rows as trials and columns as time points, the function will correctly infer the dimension of trials. If this is not the case, set the dimension of trials with the argument trial_dims. Below is an example of a three dimensional data with time as second and trials as third argument:","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"data = randn(10, 1000, 20) # 10 subjects, 1000 time points, 20 trials\nresult = acw(data, fs; dims=2, average_over_trials=true, trial_dims=3)","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"trial_dims: Dimension of trials to average over. See above (average_over_trials) for explanation. An integer.\nmax_peaks: Maximum number of oscillatory peaks to fit when cleaning the PSD for knee frequency estimation. Default is 1. \noscillation_peak: Whether or not to fit the oscillatory peaks when cleaning the PSD for knee frequency estimation. Default is true.","category":"page"},{"location":"acw/#Returns","page":"Model-Free Timescale Estimation","title":"Returns","text":"","category":"section"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"acwresults: An ACWResults object. It has the fields fs, acw_results, acwtypes, n_lags, freqlims, acf, psd, freqs, lags, x_dim. You can access these fields as acwresults.field. The field acw_results contains the ACW results indicated by the input argument acwtypes in the same order you specify. Each element of acw_results is an array of the same size of your data minus the dimension of time, which will be dropped. See below for details. ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"The reason to not return the results directly but return the ACWResults object is 1) give access to ACF and PSDs  where the calculations are performed as well as n_lags and freqlims if the user is using defaults, 2) make plotting easy. You can simply type acwplot(acwresults) to plot ACF and PSDs. ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"Your primary interest should be the field acwresults.acw_results. This is a vector of arrays. Easiest way to explain this is via an example: ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"data = randn(2, 1000, 10) # assume 2 subjects, 1000 time points and 10 trials\nfs = 1.0\nacwresults = acw(data, fs; acwtypes=[:acw0, :tau], dims=2)\nacw_results = acwresults.acw_results ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"acw_results is a two element vector containing the results with the same order of acwtypes as you specify. Since we wrote :acw0 as the first element and :tau as the second element, we can extract the results as ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"acw_0 = acw_results[1]\ntau = acw_results[2]","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"Let's check the dimensionality of these results. Remember that we specified 2 subjects, 1000 time points and 10 trials. The result collapses the dimension of time and gives the result as an 2x10 matrix. ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"size(acw_0) # should be (2, 10)\nsize(tau) # should be (2, 10)","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"Other fields:","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"fs: Sampling rate. Floating point number. \nacwtypes: The ACW types you specified. \nn_lags: The number of lags used to fit exponential decay function to the autocorrelation function. See above in the input arguments for details. An integer.\nfreqlims: Frequency limits to fit a lorentzian to the power spectrum. See above in the input arguments for details. A tuple of floating point numbers. \nacf: Autocorrelation function(s). Has the same size of your data with the time dimension replaced by lag dimension with n_lags elements. \npsd: Power spectrum/spectra. Has the same size of your data with the time dimension replaced by frequency dimension with freqlims as lowest and highest frequencies. \nfreqs: Frequencies corresponding to PSD. \nlags: Lags corresponding to ACF. \nx_dim: The dimension corresponding to lags and frequencies. Used internally in plotting. ","category":"page"},{"location":"acw/#Plotting","page":"Model-Free Timescale Estimation","title":"Plotting","text":"","category":"section"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"The function acwplot can plot power spectra and autocorrelation functions. Currently it supports only two dimensions (for example, subjects x time or trials x time). ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"p = acwplot(acwresults; only_acf=false, only_psd=false, show=true)","category":"page"},{"location":"acw/#Mandatory-Arguments","page":"Model-Free Timescale Estimation","title":"Mandatory Arguments","text":"","category":"section"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"acwresults: ACWResults type obtained by running the function acw. ","category":"page"},{"location":"acw/#Optional-Arguments","page":"Model-Free Timescale Estimation","title":"Optional Arguments","text":"","category":"section"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"only_acf / only_psd: Plot only the ACF or only the PSD. Boolean. \nshow: Whether to show the plot or only return the variable that contains the plot. ","category":"page"},{"location":"acw/#Returns-2","page":"Model-Free Timescale Estimation","title":"Returns","text":"","category":"section"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"p: The plot for further modification using the Plots library. ","category":"page"},{"location":"one_timescale/#one_timescale","page":"One Timescale Model","title":"One Timescale Model (one_timescale_model)","text":"","category":"section"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"The generative model:","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"fracdxdt = -fracxtau + xi(t)","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"with timescale tau. xi(t) is white noise with unit variance. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"Can be used with either ACF or PSD as the summary method. ","category":"page"},{"location":"one_timescale/#ACF-Summary","page":"One Timescale Model","title":"ACF Summary","text":"","category":"section"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"function one_timescale_model(data, time, fit_method; summary_method=:acf,\n                             prior=nothing, n_lags=nothing,\n                             distance_method=nothing,\n                             dims=ndims(data), distance_combined=false,\n                             weights=[0.5, 0.5])","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"Simple usage:","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"model = one_timescale_model(data, time, :abc, summary_method=:acf)\nresults = fit(model)\nint = results.MAP[1] # maximum a posteriori estimate","category":"page"},{"location":"one_timescale/#Mandatory-arguments:","page":"One Timescale Model","title":"Mandatory arguments:","text":"","category":"section"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"data: Your time-series data as a vector or 2-dimensional array. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"If it is n-dimensional, by default, the dimension of time is assumed to be the last dimension. If this is not the case, you can set it via dims argument, similar to acw function. The other dimension should correspond to trials. IntrinsicTimescales.jl calculates one ACF from each trial and averages them to get a less noisy ACF estimate. If the user wants to calculate one INT per trial, they can run a for-loop over trials. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"time: Time points corresponding to the data. \nfit_method: :abc or :advi. Method to use for parameter estimation. ","category":"page"},{"location":"one_timescale/#Optional-arguments:","page":"One Timescale Model","title":"Optional arguments:","text":"","category":"section"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"summary_method: :acf. Method to use for summary statistics. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"If :acf, calculates the autocorrelation function using comp_ac_fft internally. If :psd, calculates the power spectral density using comp_psd. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"prior: Prior distribution for the parameters. \"informed_prior\" or a Distribution object from Distributions.jl. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"If the user does not specify a prior, or specifies \"informed_prior\", IntrinsicTimescales.jl uses a normal distribution with mean determined by fitting an exponential decay to the autocorrelation function using fit_expdecay and standard deviation of 20. Currently we recommend explicitly specifying a prior distribution to improve the accuracy of the inference. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"An example for custom prior distribution:","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"prior = Normal(100.0, 10.0)\nresults = one_timescale_model(data, time, :abc, summary_method=:acf, prior=prior)","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"n_lags: Number of lags to use for the ACF calculation. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"By default, this is set to 1.1*acw0. The reason for cutting the autocorrelation function is due to increased measurement noise for autocorrelation function for later lags. Intuitively, when we perform correlation of a time-series with a shifted version of itself, we have less and less number of time points to calculate the correlation. This is why if you plot the autocorrelation function, the portion of it after ACW-0 looks more noisy. For more details, see [Practice 1]. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"distance_method: :linear or :logarithmic. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"Method to use for distance calculation. :linear is RMSE between the ACF from data and the model ACF whereas :logarithmic is RMSE after log-transforming the ACF. The default is :linear. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"distance_combined: true or false. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"If true, the distance is a weighted sum of RMSE between ACFs and RMSE between exponential decay fits to ACFs. Defaults to false.","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"weights: A vector of two numbers. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"The first number is the weight for RMSE between ACFs and the second number is the weight for RMSE between exponential decay fits to ACFs. The default is [0.5, 0.5]. Used only if distance_combined is true. ","category":"page"},{"location":"one_timescale/#PSD-Summary","page":"One Timescale Model","title":"PSD Summary","text":"","category":"section"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"function one_timescale_model(data, time, fit_method; summary_method=:psd,\n                             prior=nothing, \n                             distance_method=nothing, freqlims=nothing,\n                             dims=ndims(data), distance_combined=false,\n                             weights=[0.5, 0.5])","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"Simple usage:","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"model = one_timescale_model(data, time, :abc, summary_method=:psd)\nresults = fit(model)\nint = results.MAP[1]","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"There are two arguments that are different from the ACF summary:","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"summary_method: :psd. Method to use for summary statistics. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"Calculates the power spectral density using comp_psd. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"freqlims: A tuple of two numbers. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"The first number is the lower frequency limit and the second number is the upper frequency limit used to index the power spectral density. The default is the output from fftfreq function of AbstractFFTs.jl library. ","category":"page"},{"location":"one_timescale/#Implementation-differences-from-ACF-Summary","page":"One Timescale Model","title":"Implementation differences from ACF Summary","text":"","category":"section"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"prior: \"informed_prior\" fits a lorentzian to the PSD and calculates the INT from the knee frequency using tau_from_knee and find_knee_frequency. \ndistance_method: :linear is RMSE between the PSD from data and the model PSD whereas :logarithmic is RMSE after log-transforming the PSD. The default is :linear. \ndistance_combined: Weighted sum between RMSE between PSDs and RMSE between INT estimates from knee frequency obtained from lorentzian fits to PSD. \nweights: A vector of two numbers. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"The first number is the weight for RMSE between PSDs and the second number is the weight for RMSE between INT estimates. ","category":"page"},{"location":"one_timescale/#Returns","page":"One Timescale Model","title":"Returns","text":"","category":"section"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"model: A OneTimescaleModel object. Can be used as an input to fit function to estimate INT and posterior_predictive function to plot posterior predictive check. ","category":"page"},{"location":"one_timescale_with_missing/#one_timescale_with_missing","page":"One Timescale Model with Missing Data","title":"One Timescale with Missing Data (one_timescale_with_missing_model)","text":"","category":"section"},{"location":"one_timescale_with_missing/","page":"One Timescale Model with Missing Data","title":"One Timescale Model with Missing Data","text":"Uses the same syntax as one_timescale_model. We refer the user to the documentation of one_timescale_model for details and point out the differences here. ","category":"page"},{"location":"one_timescale_with_missing/","page":"One Timescale Model with Missing Data","title":"One Timescale Model with Missing Data","text":"The generative model is the same as one_timescale_model: ","category":"page"},{"location":"one_timescale_with_missing/","page":"One Timescale Model with Missing Data","title":"One Timescale Model with Missing Data","text":"fracdxdt = -fracxtau + xi(t)","category":"page"},{"location":"one_timescale_with_missing/","page":"One Timescale Model with Missing Data","title":"One Timescale Model with Missing Data","text":"with timescale tau. xi(t) is white noise with unit variance. The missing data points will be replaced by NaNs as in:","category":"page"},{"location":"one_timescale_with_missing/","page":"One Timescale Model with Missing Data","title":"One Timescale Model with Missing Data","text":"generated_data[isnan.(your_data)] .= NaN","category":"page"},{"location":"one_timescale_with_missing/","page":"One Timescale Model with Missing Data","title":"One Timescale Model with Missing Data","text":"To compute the summary statistic, comp_ac_time_missing for ACF and comp_psd_lombscargle for PSD is used. Note that PSD is not supported for ADVI method since the comp_psd_lombscargle is not autodifferentiable. ","category":"page"},{"location":"one_timescale_with_missing/","page":"One Timescale Model with Missing Data","title":"One Timescale Model with Missing Data","text":"For arguments and examples, see the documentation for one_timescale_model. Just replace one_timescale_model with one_timescale_with_missing_model. ","category":"page"},{"location":"practice/practice_3_ou/#Ornstein-Uhlenbeck-Process-as-a-Generative-Model-for-ACF","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"","category":"section"},{"location":"practice/practice_3_ou/#(Or-Mommy,-Where-Do-the-ACFs-Come-From?)","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"(Or Mommy, Where Do the ACFs Come From?)","text":"","category":"section"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"Despite popular belief, ACFs aren't delivered by storks. So far, we just assumed that they exist and calculated ACW metrics from them. This is where we start building a more comprehensive theory. I will keep the math to a minimum and whenever I explain math, I will supplement it with code so that you can play with to get some intuition. ","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"Let's get right into it. You have some data and from that data you can calculate an autocorrelation function. But what is the most minimal, simplest ground truth that can generate the data that you see? This is a very hard question and the discipline of theoretical neuroscience tries to answer it (Neuronal Dynamics by Gerstner et al. is an excellent starting place for the curious). An easier question is what is the simplest ground truth that can generate the autocorrelation function that you observe in the data? Then we can ask how can we think of the ACF beyond a bunch of numbers. One way to think about ACF is to think of it as an exponential decay function. Here is the math + code:","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"textrmACF(l) = e^-fracltau","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"using Plots\ndt = 0.0001 # time resolution\nlags = (0:2500) * dt # x axis\ntau_short = 0.01 # short timescale\ntau_long = 0.03 # long timescale\nacf_short = exp.(-lags ./ tau_short)\nacf_long = exp.(-lags ./ tau_long)\nplot(lags, acf_short, label=\"Short Timescale ACF\")\nplot!(lags, acf_long, label=\"Long Timescale ACF\")","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"(Image: )","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"Compare this with the ACFs we plotted in the previous tutorials. An exponential decay function is an abstraction of the ACF. In the equation above, we represent the lags with l and timescale with tau. As expected, as I increase tau (tau), the ACF decays slower. You can eyeball the ACW-50. We can do better than eyeballing. Let's do high-school math to get the ACW-50. Remember the definition: ACW-50 is the lag where ACF crosses 0.5:","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"e^-fracltau = 05","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"We need to solve this for l. Remember that the inverse of an exponential is a logarithm. Taking the logarithm of both sides:","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"loge^-fracltau = log05 \n\n-fracltau = log05  \n\nl = -tau log05 \n\nl = -tau log2^-1 \n\nl = tau log2 \n\ntextrmACW-50 = tau log2","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"That's it! We effectively showed that ACW-50 is just the timescale or decay rate of the autocorrelation function up to a constant which is log2. This is good, but the data we have is not just an autocorrelation function. It is the whole time series. The next step is to figure out the generative model for the time-series which gives this autocorrelation function. I will explain the birds and bees of this in the [Theory] section but even pure practicioners need to know a minimum of theory to understand what they practice. The minimum of theory is the mysterious thing that I shied away from explaining properly, the Ornstein-Uhlenbeck (OU) process. The function generate_ou_process that we used again and again without explaining. No more. Here is the OU:","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"fracdx(t)dt = -frac1tau x(t) + xi (t)","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"This equation is called a differential equation. On the left hand side, you have a derivative (the fracddt term). t denotes time here. You have the rate of change of something (x) with respect to time. That is, how does x change when time changes. x is your dynamic variable or your observable: what you observe in data. To see how it changes, look at the right hand side. The first term is -frac1tau x(t). tau is the timescale. This term ensures that your x always moves to 0. To see how, note that tau is always positive. If you give a positive number to x, the - sign will make sure that x decreases with time. If x reduces too much, becomes negative, then the - sign will again ensure that x increases to move it towards zero because if you put a - sign in front of a negative number, it becomes positive (as in -(-3) = +3). The final term, xi (t) ensures that your x doesn't get stuck at zero. This term is called white noise: it is a random number drawn from a Gaussian distribution. How does this all relate to timescales? Let's ask the question: how fast x approaches zero? Well, this is determined by frac1tau in front of it. Higher the tau, slower the approach because frac1textrmbig number is a small number and vice versa. If you feel uncomfortable with the mathematics I present here, I invite you to pull up a pen and paper and plug in different numbers. It'll become clear. There is no better way to build intuition other than grinding your way through intuition and forcing your way through its wooden doors with a battering ram. ","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"Back to generate_ou_process. What this function does is that it solves this equation. What does it mean to solve an equation? There are a number of ways to approach an equation of this type: one can take averages of both sides, calculate moments, use a Fokker-Planck approach, apply perturbation theory via Martin-Siggia-Rose-De Dominicis-Janssen path integral, apply Fourier analysis, use Ito or Stratonovich calculus. generate_ou_process takes a numerical approach: it assigns a random initial number as a starting condition and moves x forward in small steps according to the equation. Under the hood, it uses the amazing [DifferentialEquations.jl] library which is optimized to the bone, this is why it is fast. Since we solve the equation for x(t), the end result is a time-series. Under ideal conditions (that is, sufficiently enough data), if you calculate the autocorrelation function of this time-series, you will get an exponential decay function of the same type above. Let's test this with the tools that we are hopefully familiar with now:","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"using IntrinsicTimescales, Statistics\nsd = 1.0\nduration = length(lags) * dt # match the number of lags\nnum_trials = 100\nn_lags = length(lags)\ndata_short_ts = generate_ou_process(tau_short, sd, dt, duration, num_trials)\ndata_long_ts = generate_ou_process(tau_long, sd, dt, duration, num_trials)\n# average over trials to get a less noisy ACF\nacf_numerical_short = mean(comp_ac_fft(data_short_ts), dims=1)[:]\nacf_numerical_long = mean(comp_ac_fft(data_long_ts), dims=1)[:]\np1 = plot(lags, acf_short, label=\"Analytical ACF\", title=\"Short Timescale\")\nplot!(p1, lags, acf_numerical_short, label=\"Numerical ACF\")\np2 = plot(lags, acf_long, label=\"Analytical ACF\", title=\"Long Timescale\")\nplot!(p2, lags, acf_numerical_long, label=\"Numerical ACF\")\nplot(p1, p2, size=(800,400))","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"(Image: )","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"Note that the numerical ACF estimate decays consistenly faster than the analytical ground truth. The difference between the numerical estimate and analytical one increases as timescale increases. This is a limitation of finite data. As long as your data is finite and has a sampling rate that is not infinitesimally small, you will underestimate the INT. We will address this problem in the [final tutorial of Practice]. ","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"The good thing is even though we are underestimating the INT, the ACF of the long timescale process still decays slower than the short timescale one. ","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"This theoretical knowledge motivates two more ACW types. The first one is the lag where ACF crosses 1e. In IntrinsicTimescales.jl, this is called acweuler (or ACW-e) but I'm not sure if there is a generic name for it in the literature. The math: ","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"e^-fracltau = frac1e \n\nloge^-fracltau = logfrac1e \n\n-fracltau = -1 \n\nl = tau","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"Assuming that your autocorrelation function is a perfect exponential decay, then calculating ACW-e directly gives you the timescale tau. Alternatively, we can fit an exponential decay function to our autocorrelation function and get the decay rate. IntrinsicTimescales.jl uses the Julia package NonlinearSolve.jl for the fitting. In IntrinsicTimescales.jl, this metric is called tau. Note that by default, IntrinsicTimescales.jl cuts the tail of the ACF before fitting. Remember that the ACF estimate gets noisier as we have less and less data for longer lags. If we keep all the ACF, we might fit to the noise as well. By default, the lag where the ACF is cut is 1.1*acw0. You can change this by the parameter n_lags. In the code example below, I show the two methods. ","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"fs = 1 / dt\nacwresults_short = acw(data_short_ts, fs, acwtypes=[:acweuler, :tau], average_over_trials=true)\nacwresults_long = acw(data_long_ts, fs, acwtypes=[:acweuler, :tau], average_over_trials=true)\nacw_e_short, acw_tau_short = acwresults_short.acw_results\nacw_e_long, acw_tau_long = acwresults_long.acw_results\nprintln(\"Short timescale: $(tau_short)\")\n# 0.01\nprintln(\"ACW-e estimate of short timescale: $(acw_e_short)\")\n# 0.0091\nprintln(\"Curve-fitting estimate of short timescale: $(acw_tau_short)\")\n# 0.0088\nprintln(\"Long timescale: $(tau_long)\")\n# 0.03\nprintln(\"ACW-e estimate of long timescale: $(acw_e_long)\")\n# 0.0214\nprintln(\"Curve-fitting estimate of long timescale: $(acw_tau_long)\")\n# 0.02","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"So far, we always assumed that the ACF is a nice exponential decay. This is rarely the case for EEG/MEG data where oscillatory brain activity (alpha oscillations for example) makes a considerable impact on ACF. We will learn how to deal with it in the next section. ","category":"page"},{"location":"practice/practice_4_psd/#Dealing-with-Oscillatory-Artifacts-using-Fourier-Transformation","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"","category":"section"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"So far, we only dealt with autocorrelation functions (ACFs) that are of exponential decay type. This is fine for fMRI data but can be problematic for EEG/MEG data. To demonstrate, I'll use the function generate_ou_with_oscillation. This function adds an oscillation on top of an Ornstein-Uhlenbeck process. It takes three parameters for its generative model: timescale, oscillation frequency and coefficient for OU process (meaning higher the coefficient, lower the oscillatory artifact). The coefficient is bounded between 0 and 1. If you try to give it a coefficient that is greater than 1 or smaller than 0, it will change it to 1 and 0 respectively. It also takes the desired mean and sd for data. This is required for Bayesian estimation of timescales which will be the topic of next tutorial. Consider the following code. I'll simulate two time-series, with and without oscillatory artifacts and calculate ACW-e which I introduced in the previous section. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"using IntrinsicTimescales, Plots, Random, Statistics\nRandom.seed!(666) # for reproducibility\nfs = 1000.0 # 1000 Hz sampling rate\ndt = 1.0 / fs\nduration = 10 # 10 seconds of data\nnum_trials = 10\ndata_mean = 0.0 # desired mean\ndata_sd = 1.0 # desired sd\n\ntimescale = 0.1 # 100 ms\noscillation_freq = 10.0 # 10 Hz alpha oscillation\ncoefficient = 0.95\ntheta = [timescale, oscillation_freq, coefficient] # vector of parameters\n\ndata_osc = generate_ou_with_oscillation(theta, dt, duration, num_trials, data_mean, data_sd)\ndata = generate_ou_process(timescale, data_sd, dt, duration, num_trials)\nacwresults_osc = acw(data_osc, fs; acwtypes=:acweuler)\nacwresults = acw(data, fs; acwtypes=:acweuler)\nprintln(mean(acwresults_osc.acw_results))\n# 0.087\nprintln(mean(acwresults.acw_results))\n# 0.3075\np1 = acwplot(acwresults_osc)\ntitle!(p1, \"ACF with oscillatory component\")\np2 = acwplot(acwresults)\ntitle!(p2, \"ACF\")\nplot(p1, p2)","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"(Image: )","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"The ACW calculated from the simulation with the oscillatory component is terribly off. A good question is why do we see oscillations in the ACF (note the wiggles)? Remember that ACF is calculating the correlation of your data with your data shifted by a certain lag. If you consider a perfect oscillation, its correlation with itself will fluctuate. Whenever the peaks and troughs of oscillation correspond to peaks and troughs of the shifted oscillation (which will happen periodically, when you shift the oscillation just right enough so that it matches with itself), it will nicely correlate with itself. If you shift it half the period of oscillation, so that the peaks of the oscillation will match the troughs of the shifted oscillation, it will have a negative autocorrelation. (Try to draw this on your notebook to get a clearer picture). As a result, the ACF of a perfect oscillation is another oscillation. But oscillation here is also coupled with the OU process. Hence the exponential decay + oscillation type of ACF. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"We need to find a way to decouple the oscillation from the OU process. There is a mathematical technology for this, called the Fourier transform. Essentially it is the correlation of a signal with an oscillation. Let's write down the math. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"tildef(omega) = int_-infty^inftyf(t)e^-iomega t dt=int_-infty^inftyf(t)(cosomega t - i sinomega t) dt","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"The first equality is the definition of the Fourier transform. The second equality comes from Euler's formula. The term cosomega t +i sinomega t is a cosine wave with frequency omega and its complex counterpart i sinomega t. The complex part is there to carry the phase information. Compare this with the formula for covariance of two zero-mean signals x(t) and y(t). Remember that covariance is a correlation that is not normalized between -1 and 1. I will also ignore dividing it by the number of elements in your vector (as in averaging). ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"textrmcov(x y) = sum_tx_ty_t","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"For every time point t, we multiply x and y at that time point, and add all the values we obtained from multiplication. Time is discrete here, as in our data. x_t corresponds to t-th data point of vector x. Let's make time continuous. We will go from x_t to x(t) to note that we can put any t there, not just the elements of a vector. The discrete summation sum will then be replaced by a continuous sum denoted by the integral sign int dt. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"sum_tx_ty_t rightarrow int x(t)y(t) dt","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"The boundaries of the integral are defined by your data. Theoretically, we set them as -infty and infty. To get the final piece, replace y(t) by our complex sinusoid:","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"int x(t)y(t) dt rightarrow int_-infty^inftyx(t)(cosomega t - i sinomega t) dt \n\ntildex(omega) = int_-infty^inftyx(t)(cosomega t - i sinomega t) dt = int_-infty^inftyx(t)e^-i omega t dt","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"Compare this with the definition of Fourier transform above. We denote the Fourier transform of x(t) as tildex(omega). When we sum over the variable t denoting time, that variable disappears. For every time value, we did a multiplication and added the results of those multiplications. There is no time anymore, we picked all of them. We do this for every frequency omega. For frequency omega, we have tildex(omega). This is why when people talk about Fourier transform, they talk about going from time domain to the frequency domain. You remove the indices of time and replace it with indices of frequency. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"Remember our initial starting point: covariance = non-normalized correlation. We are seeing how much our data correlates with a certain frequency. But there is still an annoying part: the complex term with the funny i number in it. Correlation or covariance values need to be real numbers, not imaginary. To resolve this dilemma, we can calculate the magnitude of complex numbers. This would take care of the complex part and give us a real number. In your computer, this is defined as the abs function, corresponding to absolute value. In general:","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"z = a + ib \n\nabs(z) = z=sqrta^2+b^2","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"Finally, we can remove the annoying square root symbol by taking the square of the absolute value. If the square root of something is big, then something got to be big as well. Why bother with square rooting it. These series of handwaving arguments finally lead us to the definition of the power spectrum: the squared magnitude of the fourier transform.","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"textrmPSD(tildex(omega)) = tildex(omega)^2","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"Clearly, the power spectrum is a function of frequency omega. For every value of omega, we have one covariance number. The natural way to represent this is to put the frequencies on the x axis and the covariance values on the y axis. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"Why did we bother with all this. Remember our initial problem. We want to take out the frequency from the rest of the signal so that our timescale estimates are clean. The Fourier transform does exactly that. By moving from time domain to frequency domain, we can pinpoint the oscillations. The power spectrum shows exactly that. IntrinsicTimescales.jl has a bunch of functions to do this: comp_psd for periodogram, comp_psd_adfriendly is an automatic-differentiation friendly version of this (which we'll use in the next section) and comp_psd_lombscargle takes care of the missing data points using Lomb-Scargle periodogram if your data has missing values in it represented by NaNs. In practice, acw function wraps all these and picks the appropriate one for you. Let's plot the two power spectra for our data with and without oscillations. The third plot shows the average across trials. I'll plot on log scale, it is usually easier on the eyes for power spectra. Feel free to remove the scale=:log10 to see them raw. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"psd, freqs = comp_psd(data, fs)\npsd_osc, freqs = comp_psd(data_osc, fs)\np1 = plot(freqs, psd', scale=:log10, label=\"\", title=\"Without oscillation\")\nxlims!(p1, (1.0, 50.0)) # limit to frequencies between 1 and 50 Hz\np2 = plot(freqs, psd_osc', scale=:log10, label=\"\", title=\"With oscillation\")\nxlims!(p2, (1.0, 50.0))\np3 = plot(freqs, mean(psd', dims=2), scale=:log10, \n          linewidth=3, label=\"Without oscillations\", legend_position=:bottomleft)\nplot!(freqs, mean(psd_osc', dims=2), scale=:log10, \n      linewidth=3, label=\"With oscillations\", legend_position=:bottomleft)\nxlims!(p3, (1.0, 50.0))\nvline!(p3, [10.0], color=:black, linewidth=1, label=\"\", linestyle=:dash)\nplot(p1, p2, p3, size=(1200, 400), layout=(1, 3))","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"(Image: )","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"See that huge peak? The correlation with a 10 Hz oscillation and our data is high because our data has a 10 Hz oscillation. That's nice. But what does it have to do with timescales? ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"It is not a coincidence that both autocorrelation and power spectrum are defined in terms of correlations. Correlation with your data with itself and correlation of your data with sinusoids. It would be really nice if there was a way to connect them. Thankfully, there is. Wiener-Khinchin theorem states that the inverse Fourier transform of a power spectrum is the autocorrelation function. Which also means that the Fourier transform of an autocorrelation function is the power spectrum. Showing this is not too difficult but we already did too much math for a documentation page. For the curious reader, I'll link this nice and short proof. Nonetheless, take a look at the source code of the function comp_ac_fft, you'll see this theorem in action. Because Fourier transform on a computer is super fast (hence the name: Fast Fourier Transform, FFT), this function is way faster than comp_ac_time which calculates the ACF by shifting the time-series again and again and calculating correlation. And the really nice thing is the two functions are not approximations of each other. They are exactly the same, up to a floating point error on the order of 10^-16. You can see this in one of the test files for the package. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"How can we utilize this information to get the timescale? Let's assume that our ACF is an exponential decay and calculate its Fourier transform. This is not an easy integral, I won't explain how to do it here (a.k.a. I will use the technique of integration by trust me bro). A nice explanation is given here. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"textrmACF(l) = e^-fracltau \n\ntildetextrmACF(omega) = textrmPSD(omega) = fracAk^2 + omega^2 \n\nk = frac12 pi tau \n\ntau = frac12 pi k","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"fracAa^2 + omega^2 is called a Lorentzian. The A term upstairs is just a normalization factor, not particularly interesting. omega is our usual frequency. We also see that the timescale tau is in there as well. This means that we need only to fit this Lorentzian to our power spectrum and we have access to timescale tau. To my knowledge, this technique was first introduced in Chaudhuri et al., 2017 and further developed in Gao et al., 2020 to cover the cases where the exponent of omega downstairs is not exactly 2. Right now, IntrinsicTimescales.jl uses the simpler fitting that doesn't account for non-2 exponents. This should cover most cases. If you are getting funny results, I recommend the FOOOF package which uses the Gao et al., 2020 technique. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"Some interesting remarks. Consider very low frequencies: omega ll k. When omega is small, then omega^2 will be much smaller compared to k^2 (think of if 2 is smaller than 3, then 2^2=4 is much smaller than 3^2=9). Then we can ignore the \\omega term and our PSD reduces to fracAk^2 which is just a constant. Now consider big frequencies: omega gg k. Since omega is big, omega^2 is now much bigger than k and we can ignore k. Then the power spectrum is fracAomega^2. This is the so-called scale-free power spectrum with a power-law exponent (PLE) of 2. If we take the logarithm, logfracAomega^2 sim -2omega meaning on the log-scale, we should see a straight line with a slope of 2. Right in between, there is a transition from a flat PSD to PLE=2 PSD. This is where k is approximately equal to omega. Alternatively, the frequency between the flat part and the PLE=2 part is the knee frequency and also corresponds to your timescale up to a multiplicative constant. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"In IntrinsicTimescales.jl, you can use :knee in the acwtypes argument of acw to calculate the INT from the knee frequency. In the code below, I'll show this and plot the PSDs on the log scale to visually show the knee frequency. The function knee_from_tau converts the timescale to the knee frequency. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"\nacwresults_osc = acw(data_osc, fs; acwtypes=:knee, average_over_trials=true)\nacwresults = acw(data, fs; acwtypes=:knee, average_over_trials=true)\nprintln(mean(acwresults_osc.acw_results))\n# 0.0795\nprintln(mean(acwresults.acw_results))\n# 0.0757\np1 = acwplot(acwresults_osc)\ntitle!(p1, \"ACF with oscillatory component\")\np2 = acwplot(acwresults)\ntitle!(p2, \"ACF\")\nvline!(p1, [knee_from_tau(acwresults_osc.acw_results)], color=:black, linewidth=1, label=\"Knee frequency\", linestyle=:dash)\nvline!(p2, [knee_from_tau(acwresults.acw_results)], color=:black, linewidth=1, label=\"Knee frequency\", linestyle=:dash)\nplot(p1, p2, size=(800, 500))","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"(Image: )","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"We're almost there. In the final chapter, we will cover Bayesian estimation of INTs and finish the Practice part of the documentation. ","category":"page"},{"location":"practice/practice_4_psd/#Bonus:-Why-does-complex-numbers-in-Fourier-transform-relate-to-phase?","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Bonus: Why does complex numbers in Fourier transform relate to phase?","text":"","category":"section"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"I felt guilty after waving my hands too much when trying to convince you that we really need complex numbers to get the phase information. For the curious, here is a proper explanation. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"Consider the cosine wave g(t)=A cos(ft+phi) with amplitude A, frequency f and phase shift phi. We'll take the Fourier transform of this guy. Let's start with writing it as in exponential form because dealing with trigonometric functions is annoying. Using Euler's formula:","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"textrmEulers Formula e^ix = cosx + i sinx \n\nA cos(ft+phi) = fracA2(e^i(ft+phi)+e^-i(ft+phi)) ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"Now let's take the Fourier transform on the exponential notation. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"tildeg(omega) = int_-infty^inftyg(t)e^-iomega tdt \n\n= fracA2 ( int_-infty^inftye^i(ft+phi)e^-iomega tdt + \nint_-infty^inftye^-i(ft+phi)e^-iomega tdt ) \n\n= fracA2 (e^i phi int_-infty^inftye^t(if-iomega)dt + \ne^-i phi int_-infty^inftye^t(-if-iomega)dt) ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"Integrating complex functions is a messy business but it suffices to know that the integral of a complex exponential is a delta function. See this stackexchange answer for a really nice explanation. If my intuition is right (no guarantees), this is related to the orthogonality of Fourier components. Writing down the solution to the integrals above:","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"tildeg(omega) = fracA2(2 pi delta(omega-f)e^i phi+2 pi delta(omega+f)e^-i phi) \n\n= A pi (delta(omega-f)e^i phi+delta(omega+f)e^-i phi)","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"What this tells us is that in the Fourier domain, there is one peak at the frequency f (from delta(omega-f)) and another peak at the negative frequency -f (from delta(omega+f)). Negative frequencies are not particularly interesting because if your signal is real valued, then the negative and positive parts of the Fourier domain will be symmetrical. The phase information is also encoded in e^i phi and e^-i phi. Now let's try to do the same without using complex exponentials. I used the notation tildeg for Fourier transform. I'll use hatg here to denote that this isn't exactly the Fourier transform but something we cooked up. As a matter of fact, we didn't cook this up, Joseph Fourier used to do his transforms with real functions. We are going back in time and using the old technique. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"hatg(omega) = int_-infty^inftycos(ft+phi) cos(omega t) dt","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"This is a very difficult integral. For me, the definition of very difficult is Mathematica can't solve it. To simplify, let's use Euler's formula:","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"cos(ft+phi) cos(omega t) = left( frac12(e^i(ft+phi)+e^-i(ft+phi)) right)\nleft( frac12 (e^i omega t + e^-i omega t) right) \n\n= frac14 left(e^-i (f t+phi )-i t omega+e^i t omega-i (f t+phi )+e^i (f t+phi )-i t omega+e^i (f t+phi )+i t omega right)","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"And we went back to the complex exponentials. It seems there is no escape from them. Might as well start directly with a complex exponential and save us the trouble. And now I can sleep with peacefully. ","category":"page"},{"location":"fit_parameters/#Model-Fitting-and-Parameters","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"","category":"section"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"All models are fit with fit function and return ADVIResults or ABCResults type. The fit function has the following signature:","category":"page"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"results = fit(model, param_dict=nothing)","category":"page"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"The function determines the inference method based on model attributes which the user provides when initiating the model. When param_dict is not provided, the function uses the default parameters for the inference method, which can be seen with get_param_dict_advi and get_param_dict_abc functions. ","category":"page"},{"location":"fit_parameters/#Parameters-for-Approximate-Bayesian-Computation-(ABC)","page":"Model Fitting and Parameters","title":"Parameters for Approximate Bayesian Computation (ABC)","text":"","category":"section"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"The parameters of ABC can be accessed and modified through the get_param_dict_abc() function. ","category":"page"},{"location":"fit_parameters/#General-ABC-Parameters","page":"Model Fitting and Parameters","title":"General ABC Parameters","text":"","category":"section"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"epsilon_0::Float64 = 1.0: Initial acceptance threshold. If the distance between the observed data and the simulated data is less than epsilon_0, the sample is accepted in the initial step of ABC. Subsequent steps change the epsilon value to adapt better. \nmax_iter::Int = 10000: Maximum number of iterations per basic ABC step\nmin_accepted::Int = 100: The number of accepted samples for basic ABC\nsteps::Int = 30: Number of PMC steps to perform\nsample_only::Bool = false: If true, only perform sampling without adaptation between basic ABC runs","category":"page"},{"location":"fit_parameters/#Epsilon-Selection-Parameters","page":"Model Fitting and Parameters","title":"Epsilon Selection Parameters","text":"","category":"section"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"Different from the Zeraati et al. (2022) method, we adaptively change the epsilon value between basic ABC steps. The epsilon selection procedure adaptively adjusts the acceptance threshold based on the current acceptance rate and distance distribution. The procedure works as follows:","category":"page"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"First, invalid distances (NaN values and distances above distance_max) are filtered out.\nThree quantiles are computed from the valid distances:\nLower quantile (quantile_lower) for conservative threshold\nInitial quantile (quantile_init) for first iteration\nUpper quantile (quantile_upper) for relaxed threshold\nAn adaptive alpha value is computed based on:\nProgress through iterations (iteration/totaliterations) to decay from alphamax to alpha_min\nDifference between current and target acceptance rates:\nIf difference > accratefar: Alpha increases to min(alphamax, basealpha * alphafarmult)\nIf difference < accrateclose: Alpha decreases to max(alphamin, basealpha * alphaclosemult) \nOtherwise: Uses base alpha from iteration progress\nThe new epsilon is then selected:\nFor first iteration: Uses the initial quantile\nFor subsequent iterations:\nIf acceptance rate is too high: Epsilon is set to the maximum of the lower quantile and epsilon * (1-alpha)\nIf acceptance rate is too low: Epsilon is set to the minimum of the upper quantile and epsilon * (1+alpha) \nIf acceptance rate is within buffer of target: Epsilon stays same","category":"page"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"This adaptive procedure helps balance exploration and exploitation during the ABC sampling process by sampling wider for initial steps and narrowing down as the algorithm converges. ","category":"page"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"Parameters controlling epsilon selection:","category":"page"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"target_acc_rate::Float64 = 0.01: Targeted acceptance rate for epsilon adaptation\ndistance_max::Float64 = 10.0: Maximum distance to consider valid\nquantile_lower::Float64 = 25.0: Lower quantile for epsilon adjustment\nquantile_upper::Float64 = 75.0: Upper quantile for epsilon adjustment\nquantile_init::Float64 = 50.0: Initial quantile when no acceptance rate\nacc_rate_buffer::Float64 = 0.1: Buffer around target acceptance rate","category":"page"},{"location":"fit_parameters/#Adaptive-Alpha-Parameters","page":"Model Fitting and Parameters","title":"Adaptive Alpha Parameters","text":"","category":"section"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"alpha_max::Float64 = 0.9: Maximum adaptation rate\nalpha_min::Float64 = 0.1: Minimum adaptation rate\nacc_rate_far::Float64 = 2.0: Threshold for \"far from target\" adjustment\nacc_rate_close::Float64 = 0.2: Threshold for \"close to target\" adjustment\nalpha_far_mult::Float64 = 1.5: Multiplier for alpha when far from target\nalpha_close_mult::Float64 = 0.5: Multiplier for alpha when close to target","category":"page"},{"location":"fit_parameters/#Early-Stopping-Parameters","page":"Model Fitting and Parameters","title":"Early Stopping Parameters","text":"","category":"section"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"convergence_window::Int = 3: Number of iterations to check for convergence\ntheta_rtol::Float64 = 1e-2: Relative tolerance for parameter convergence\ntheta_atol::Float64 = 1e-3: Absolute tolerance for parameter convergence\ntarget_epsilon::Float64 = 5e-3: Stop the PMC if the distance between the observed data and the simulated data is less than target_epsilon.\nminAccRate::Float64 = 0.01: If acceptance rate of basic ABC steps is below minAccRate, the algorithm stops.","category":"page"},{"location":"fit_parameters/#Numerical-Stability-Parameters","page":"Model Fitting and Parameters","title":"Numerical Stability Parameters","text":"","category":"section"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"jitter::Float64 = 1e-6: Small value added to covariance diagonal for numerical stability\ncov_scale::Float64 = 2.0: Scaling factor for covariance matrix to calculate tau_squared which is used to calculate the weights of posterior samples for calculating the new prior. ","category":"page"},{"location":"fit_parameters/#Display-Parameters","page":"Model Fitting and Parameters","title":"Display Parameters","text":"","category":"section"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"show_progress::Bool = true: Whether to show progress bar\nverbose::Bool = true: Whether to print detailed information","category":"page"},{"location":"fit_parameters/#MAP-Estimation-Parameters","page":"Model Fitting and Parameters","title":"MAP Estimation Parameters","text":"","category":"section"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"The find_MAP function estimates the maximum a posteriori (MAP) parameters by performing a grid search over the parameter space. It takes the accepted parameters from the final ABC step and creates a grid of N random positions within the parameter bounds. For each parameter dimension, it estimates the probability density using kernel density estimation (KDE) and evaluates the density at the grid positions. The MAP estimate is then determined by finding the position with maximum probability density for each parameter. This provides a point estimate of the most probable parameter values given the posterior samples.","category":"page"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"N::Int = 10000: Number of samples for maximum a posteriori (MAP) estimation grid search","category":"page"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"To modify these parameters, create a dictionary with your desired values and pass it to the fit function:","category":"page"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"model = one_timescale_model(data, time, :abc)\nparam_dict = get_param_dict_abc()\nparam_dict[:convergence_window] = 10\nparam_dict[:max_iter] = 20000\nresults = fit(model, param_dict)","category":"page"},{"location":"fit_parameters/#Parameters-for-Automatic-Differentiation-Variational-Inference-(ADVI)","page":"Model Fitting and Parameters","title":"Parameters for Automatic Differentiation Variational Inference (ADVI)","text":"","category":"section"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"ADVI is performed via Turing.jl package. See the variational inference tutorial to learn more about Turing's ADVI implementation. The parameters can be accessed and modified through the get_param_dict_advi() function. ","category":"page"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"n_samples::Int = 4000: Number of posterior samples to draw after fitting\nn_iterations::Int = 50: Number of ADVI optimization iterations. Increase this number if your model is not fitting well.\nn_elbo_samples::Int = 20: Number of samples used to estimate the ELBO (Evidence Lower BOund) during optimization. Increase this number if your model is not fitting well.\nautodiff = AutoForwardDiff(): The automatic differentiation backend to use for computing gradients. Currently, only AutoForwardDiff() is supported.","category":"page"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"To modify these parameters, create a dictionary with your desired values and pass it to the fit function:","category":"page"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"model = one_timescale_model(data, time, :advi)\nparam_dict = get_param_dict_advi()\nparam_dict[:n_samples] = 8000\nparam_dict[:n_elbo_samples] = 60\nresults = fit(model, param_dict)","category":"page"},{"location":"","page":"API","title":"API","text":"Pages = [\"API.md\"]","category":"page"},{"location":"#Functions","page":"API","title":"Functions","text":"","category":"section"},{"location":"","page":"API","title":"API","text":"Modules = [IntrinsicTimescales, IntrinsicTimescales.Models, IntrinsicTimescales.ABC, IntrinsicTimescales.ACW, IntrinsicTimescales.TuringBackend, IntrinsicTimescales.SummaryStats, \n    IntrinsicTimescales.Distances, IntrinsicTimescales.Utils, IntrinsicTimescales.OrnsteinUhlenbeck, IntrinsicTimescales.OneTimescale, \n    IntrinsicTimescales.OneTimescaleAndOsc, IntrinsicTimescales.OneTimescaleWithMissing, \n    IntrinsicTimescales.OneTimescaleAndOscWithMissing]\npages = [\"IntrinsicTimescales.jl\", [\"core/model.jl\", \"core/one_timescale.jl\", \"core/one_timescale_and_osc.jl\", \"core/one_timescale_with_missing.jl\", \"core/one_timescale_and_osc_with_missing.jl\"], \"core/abc.jl\", \"core/turing_backend.jl\", \"stats/summary.jl\", \"stats/distances.jl\", \"utils/utils.jl\", \"utils/ou_process.jl\"]\nprivate = false","category":"page"},{"location":"#IntrinsicTimescales.IntrinsicTimescales","page":"API","title":"IntrinsicTimescales.IntrinsicTimescales","text":"IntrinsicTimescales\n\nA Julia package for estimation of timescales from time series data.\n\nFeatures\n\nStandard techniques for INT calculation: ACW-50, ACW-0, FOOOF\nApproximate Bayesian Computation (ABC) for parameter inference\nADVI for variational inference\nMultiple model types:\nSingle timescale\nSingle timescale with oscillations\nModels supporting missing data\nSummary statistics using periodogram, Welch (from DSP.jl) and Lomb-Scargle (from LombScargle.jl):\nAutocorrelation function (ACF)\nPower spectral density (PSD)\n\nSubmodules\n\nModels: Abstract model types and interfaces\nABC: Approximate Bayesian Computation algorithms\nTuringBackend: Turing.jl integration for ADVI\nSummaryStats: ACF and PSD implementations\nDistances: Distance metrics for ABC\nUtils: Utility functions for analysis\nOrnsteinUhlenbeck: OU process generation using DifferentialEquations.jl\nOneTimescale: Single timescale model\nOneTimescaleAndOsc: Single timescale with oscillations\nOneTimescaleWithMissing: Single timescale with missing data\nOneTimescaleAndOscWithMissing: Single timescale and oscillations with missing data\nPlotting: Plotting functions for results\n\n\n\n\n\n","category":"module"},{"location":"#IntrinsicTimescales.Models.AbstractTimescaleModel","page":"API","title":"IntrinsicTimescales.Models.AbstractTimescaleModel","text":"AbstractTimescaleModel\n\nAbstract type representing models for timescale inference. All concrete model implementations should subtype this.\n\n\n\n\n\n","category":"type"},{"location":"#IntrinsicTimescales.Models.BaseModel","page":"API","title":"IntrinsicTimescales.Models.BaseModel","text":"BaseModel <: AbstractTimescaleModel\n\nBase model structure for timescale inference using various methods.\n\nFields\n\ndata: Input time series data\ntime: Time points corresponding to the data\ndata_sum_stats: Pre-computed summary statistics of the data\nfitmethod::Symbol: Fitting method to use. Options: :abc, :advi, :acw\nsummary_method::Symbol: Summary statistic type. Options: :psd (power spectral density) or :acf (autocorrelation)\nlags_freqs::AbstractVector{<:Real}: Lags (for ACF) or frequencies (for PSD) at which to compute summary statistics\nprior: Prior distributions for parameters. Can be Vector{Distribution}, single Distribution, or \"informed_prior\"\nacwtypes::Union{Vector{Symbol}, Symbol}: ACW analysis types (e.g., :ACW50, :ACW0, :ACWe, :tau, :knee)\ndistance_method::Symbol: Distance metric type. Options: :linear or :logarithmic\ndt::Real: Time step between observations\nT::Real: Total time span of the data\nnumTrials::Real: Number of trials/iterations\ndata_mean::Real: Mean of the input data\ndata_sd::Real: Standard deviation of the input data\n\n\n\n\n\n","category":"type"},{"location":"#IntrinsicTimescales.Models.check_acwtypes-Tuple{Any, Any}","page":"API","title":"IntrinsicTimescales.Models.check_acwtypes","text":"check_acwtypes(acwtypes, possible_acwtypes)\n\nValidate the ACW analysis types against allowed options.\n\nArguments\n\nacwtypes: Symbol or Vector of Symbols specifying ACW analysis types\npossible_acwtypes: Vector of allowed ACW analysis types\n\nReturns\n\nValidated vector of ACW types\n\nThrows\n\nErrorException: If invalid ACW types are provided\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Models.check_inputs-Tuple{Any, Any}","page":"API","title":"IntrinsicTimescales.Models.check_inputs","text":"check_inputs(fitmethod, summary_method)\n\nValidate the fitting method and summary statistic choices.\n\nArguments\n\nfitmethod: Symbol specifying the fitting method\nsummary_method: Symbol specifying the summary statistic type\n\nThrows\n\nArgumentError: If invalid options are provided\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Models.check_model_inputs","page":"API","title":"IntrinsicTimescales.Models.check_model_inputs","text":"check_model_inputs(data, time, fit_method, summary_method, prior, acwtypes, distance_method)\n\nValidate inputs for timescale model construction.\n\nArguments\n\ndata: Input time series data\ntime: Time points corresponding to the data\nfit_method: Fitting method (:abc, :advi)\nsummary_method: Summary statistic type (:psd or :acf)\nprior: Prior distribution(s) for parameters\nacwtypes: Types of ACW analysis\ndistance_method: Distance metric type (:linear or :logarithmic)\n\nThrows\n\nArgumentError: If any inputs are invalid or incompatible\n\n\n\n\n\n","category":"function"},{"location":"#IntrinsicTimescales.Models.distance_function","page":"API","title":"IntrinsicTimescales.Models.distance_function","text":"distance_function(model::AbstractTimescaleModel, summary_stats, summary_stats_synth)\n\nCompute distance between two sets of summary statistics.\n\nArguments\n\nmodel: Model instance\nsummary_stats: First set of summary statistics\nsummary_stats_synth: Second set of summary statistics (typically from synthetic data)\n\nReturns\n\nDistance value according to model.distance_method\n\n\n\n\n\n","category":"function"},{"location":"#IntrinsicTimescales.Models.draw_theta","page":"API","title":"IntrinsicTimescales.Models.draw_theta","text":"draw_theta(model::AbstractTimescaleModel)\n\nDraw parameter values from the model's prior distributions.\n\nReturns\n\nArray of proposed model parameters sampled from their respective priors\n\n\n\n\n\n","category":"function"},{"location":"#IntrinsicTimescales.Models.fit","page":"API","title":"IntrinsicTimescales.Models.fit","text":"fit(model::AbstractTimescaleModel, param_dict=nothing)\n\nFit the timescale model using the specified fitting method.\n\nArguments\n\nmodel: The timescale model instance to fit\nparam_dict: Optional dictionary of fitting parameters. If not provided, default parameters will be used.\n\nReturns\n\nFor ADVI fitting method:\n\nsamples: Array of posterior samples\nmap_estimate: Maximum a posteriori estimate of parameters\nvi_result: Full variational inference result object\n\nFor ABC fitting method:\n\nsamples: Array of accepted parameter samples\nweights: Importance weights for the samples\ndistances: Distances between simulated and observed summary statistics\n\n\n\n\n\n","category":"function"},{"location":"#IntrinsicTimescales.Models.generate_data","page":"API","title":"IntrinsicTimescales.Models.generate_data","text":"generate_data(model::AbstractTimescaleModel, theta)\n\nGenerate synthetic data using the forward model with given parameters.\n\nArguments\n\nmodel: Model instance\ntheta: Array of model parameters\n\nReturns\n\nSynthetic dataset with same structure as the original data\n\n\n\n\n\n","category":"function"},{"location":"#IntrinsicTimescales.Models.generate_data_and_reduce-Tuple{AbstractTimescaleModel, Any}","page":"API","title":"IntrinsicTimescales.Models.generate_data_and_reduce","text":"generate_data_and_reduce(model::AbstractTimescaleModel, theta)\n\nCombined function to generate synthetic data and compute distance from observed data. This is a convenience function commonly used in ABC algorithms.\n\nArguments\n\nmodel: Model instance\ntheta: Array of model parameters\n\nReturns\n\nDistance value between synthetic and observed summary statistics\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Models.summary_stats","page":"API","title":"IntrinsicTimescales.Models.summary_stats","text":"summary_stats(model::AbstractTimescaleModel, data)\n\nCompute summary statistics (PSD or ACF) from the data.\n\nArguments\n\nmodel: Model instance\ndata: Input data (original or synthetic)\n\nReturns\n\nArray of summary statistics computed according to model.summary_method\n\n\n\n\n\n","category":"function"},{"location":"#IntrinsicTimescales.ABC.ABCResults","page":"API","title":"IntrinsicTimescales.ABC.ABCResults","text":"ABCResults\n\nContainer for ABC results to standardize plotting interface.\n\nFields\n\ntheta_history::Vector{Matrix{Float64}}: History of parameter values across iterations\nepsilon_history::Vector{Float64}: History of epsilon values\nacc_rate_history::Vector{Float64}: History of acceptance rates\nweights_history::Vector{Vector{Float64}}: History of weights\nfinal_theta::Matrix{Float64}: Final accepted parameter values\nfinal_weights::Vector{Float64}: Final weights\n\n\n\n\n\n","category":"type"},{"location":"#IntrinsicTimescales.ABC.abc_results-Tuple{Vector{NamedTuple}}","page":"API","title":"IntrinsicTimescales.ABC.abc_results","text":"abc_results(output_record::Vector{NamedTuple})\n\nConstruct abc_results from PMC-ABC output record.\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.ABC.basic_abc-Tuple{AbstractTimescaleModel}","page":"API","title":"IntrinsicTimescales.ABC.basic_abc","text":"basic_abc(model::Models.AbstractTimescaleModel; kwargs...)\n\nPerform basic ABC rejection sampling.\n\nArguments\n\nmodel::Models.AbstractTimescaleModel: Model to perform inference on\nepsilon::Float64: Acceptance threshold\nmax_iter::Integer: Maximum number of iterations\nmin_accepted::Integer: Minimum number of accepted samples required\npmc_mode::Bool=false: Whether to use PMC proposal distribution\nweights=Array{Float64}: Importance weights (used in PMC mode)\ntheta_prev=Array{Float64}: Previous parameters (used in PMC mode)\ntau_squared=Array{Float64}: Covariance matrix (used in PMC mode)\nshow_progress::Bool=true: Whether to show progress bar\n\nReturns\n\nNamedTuple containing:\n\nsamples: All proposed parameters\nisaccepted: Boolean mask of accepted samples\ntheta_accepted: Accepted parameters\ndistances: Distances for all proposals\nn_accepted: Number of accepted samples\nn_total: Total number of iterations\nepsilon: Acceptance threshold used\nweights: Sample weights (uniform in basic ABC)\ntau_squared: Covariance matrix (zeros in basic ABC)\neff_sample: Effective sample size\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.ABC.calc_weights-Tuple{VecOrMat{Float64}, VecOrMat{Float64}, Matrix{Float64}, Vector{Float64}, Union{Distributions.Distribution, Vector}}","page":"API","title":"IntrinsicTimescales.ABC.calc_weights","text":"calc_weights(theta_prev, theta, tau_squared, weights, prior)\n\nCalculate importance weights for PMC-ABC algorithm.\n\nArguments\n\ntheta_prev: Previously accepted parameters\ntheta: Current parameters\ntau_squared: Covariance matrix for proposal distribution\nweights: Previous importance weights\nprior: Prior distribution(s)\n\nReturns\n\nVector of normalized importance weights\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.ABC.compute_adaptive_alpha-Tuple{Integer, Float64, Float64}","page":"API","title":"IntrinsicTimescales.ABC.compute_adaptive_alpha","text":"Compute adaptive alpha value based on iteration and convergence metrics\n\nArguments\n\niteration: Current iteration number\ncurrent_acc_rate: Current acceptance rate\ntarget_acc_rate: Target acceptance rate\nalpha_max: Maximum alpha value (default: 0.9)\nalpha_min: Minimum alpha value (default: 0.1)\ntotal_iterations: Total number of iterations\nacc_rate_far: Threshold for \"far from target\" adjustment (default: 2.0)\nacc_rate_close: Threshold for \"close to target\" adjustment (default: 0.2)\nalpha_far_mult: Multiplier for alpha when far from target (default: 1.5)\nalpha_close_mult: Multiplier for alpha when close to target (default: 0.5)\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.ABC.draw_theta_pmc-NTuple{4, Any}","page":"API","title":"IntrinsicTimescales.ABC.draw_theta_pmc","text":"draw_theta_pmc(model, theta_prev, weights, tau_squared; jitter::Float64=1e-5)\n\nDraw new parameter values using the PMC proposal distribution.\n\nArguments\n\nmodel: Model instance\ntheta_prev: Previously accepted parameters\nweights: Importance weights from previous iteration\ntau_squared: Covariance matrix for proposal distribution\njitter::Float64=1e-5: Small value added to covariance diagonal for numerical stability\n\nReturns\n\nVector of proposed parameters\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.ABC.effective_sample_size-Tuple{Vector{Float64}}","page":"API","title":"IntrinsicTimescales.ABC.effective_sample_size","text":"effective_sample_size(w::Vector{Float64})\n\nCalculate effective sample size from importance weights.\n\nArguments\n\nw: Vector of importance sampling weights\n\nReturns\n\nFloat64: Effective sample size\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.ABC.find_MAP","page":"API","title":"IntrinsicTimescales.ABC.find_MAP","text":"find_MAP(theta_accepted::Matrix{Float64}, N::Int)\n\nFind the MAP estimates from posteriors with grid search.\n\nArguments\n\ntheta_accepted::Matrix{Float64}: Matrix of accepted samples from the final step of ABC\nN::Int: Number of samples for grid search\n\nReturns\n\ntheta_map::Vector{Float64}: MAP estimates of the parameters\n\n\n\n\n\n","category":"function"},{"location":"#IntrinsicTimescales.ABC.get_param_dict_abc-Tuple{}","page":"API","title":"IntrinsicTimescales.ABC.get_param_dict_abc","text":"get_param_dict_abc()\n\nGet default parameter dictionary for ABC algorithm.\n\nReturns\n\nDictionary containing default values for all ABC parameters including:\n\nBasic ABC parameters (epsilon0, maxiter, etc.)\nAcceptance rate parameters\nDisplay parameters\nNumerical stability parameters\nEpsilon selection parameters\nAdaptive alpha parameters\nEarly stopping parameters\nMAP estimation parameters\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.ABC.pmc_abc-Tuple{AbstractTimescaleModel}","page":"API","title":"IntrinsicTimescales.ABC.pmc_abc","text":"pmc_abc(model::Models.AbstractTimescaleModel; epsilon_0=1.0, max_iter=10000, min_accepted=100, steps=10, sample_only=false, minAccRate=0.01, target_acc_rate=0.01)\n\nPerform Population Monte Carlo Approximate Bayesian Computation (PMC-ABC) inference.\n\nArguments\n\nmodel::Models.AbstractTimescaleModel: Model to perform inference on\nepsilon_0::Float64=1.0: Initial epsilon threshold for acceptance\nmax_iter::Int=10000: Maximum number of iterations per step\nmin_accepted::Int=100: Minimum number of accepted samples required\nsteps::Int=10: Number of PMC steps to perform\nsample_only::Bool=false: If true, only perform sampling without adaptation\nminAccRate::Float64=0.01: Minimum acceptance rate before stopping\ntarget_acc_rate::Float64=0.01: Target acceptance rate for epsilon adaptation\n\nReturns\n\nVector of NamedTuples containing results for each PMC step, including:\n\nAccepted parameters (theta_accepted)\nDistances (D_accepted) \nNumber of accepted/total samples\nEpsilon threshold\nSample weights\nCovariance matrix (tau_squared)\nEffective sample size\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.ABC.select_epsilon-Tuple{Vector{Float64}, Float64}","page":"API","title":"IntrinsicTimescales.ABC.select_epsilon","text":"Adaptively selects epsilon based on acceptance rate and distance distribution\n\nArguments\n\ndistances: Vector of distances from ABC\ncurrent_epsilon: Current epsilon value\ntarget_acc_rate: Target acceptance rate\ncurrent_acc_rate: Current acceptance rate\niteration: Current iteration number\ntotal_iterations: Total number of iterations\ndistance_max: Maximum distance to consider valid (default: 10.0)\nquantile_lower: Lower quantile for epsilon adjustment (default: 25)\nquantile_upper: Upper quantile for epsilon adjustment (default: 75)\nquantile_init: Initial quantile when no acceptance rate (default: 50)\nacc_rate_buffer: Buffer around target acceptance rate (default: 0.1)\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.ABC.weighted_covar-Tuple{Matrix{Float64}, Vector{Float64}}","page":"API","title":"IntrinsicTimescales.ABC.weighted_covar","text":"weighted_covar(x::Matrix{Float64}, w::Vector{Float64})\n\nCalculate weighted covariance matrix.\n\nArguments\n\nx: Matrix of values where each row is an observation\nw: Vector of weights corresponding to each observation\n\nReturns\n\nWeighted covariance matrix\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.ACW","page":"API","title":"IntrinsicTimescales.ACW","text":"ACW\n\nModule providing autocorrelation width (ACW) calculations for time series analysis, including:\n\nACW-0 (zero-crossing)\nACW-50 (50% decay)\nACW-euler (1/e decay)\nExponential decay timescale (tau)\nKnee frequency estimation\n\n\n\n\n\n","category":"module"},{"location":"#IntrinsicTimescales.ACW.ACWResults","page":"API","title":"IntrinsicTimescales.ACW.ACWResults","text":"ACWResults\n\nStructure holding ACW analysis inputs and results.\n\nFields\n\ndata::AbstractArray{<:Real}: Input time series data\nfs::Real: Sampling frequency\nacwtypes::Union{Vector{<:Symbol}, Symbol, Nothing}: Types of ACW to compute\nn_lags::Union{Int, Nothing}: Number of lags for ACF calculation\nfreqlims::Union{Tuple{Real, Real}, Nothing}: Frequency limits for spectral analysis\nacw_results::Vector{<:Real}: Computed ACW values\n\nNotes\n\nSupported ACW types: :acw0, :acw50, :acweuler, :tau, :knee\nResults order matches input acwtypes order\n\n\n\n\n\n","category":"type"},{"location":"#IntrinsicTimescales.ACW.acw-Tuple{Any, Any}","page":"API","title":"IntrinsicTimescales.ACW.acw","text":"acw(data, fs; acwtypes=possible_acwtypes, n_lags=nothing, freqlims=nothing, time=nothing, \n    dims=ndims(data), return_acf=true, return_psd=true, average_over_trials=false,\n    trial_dims::Int=setdiff([1, 2], dims)[1], max_peaks::Int=1)\n\nCompute various autocorrelation width measures for time series data.\n\nArguments\n\ndata::AbstractArray{<:Real}: Input time series data\nfs::Real: Sampling frequency\nacwtypes::Union{Vector{Symbol}, Symbol}=possible_acwtypes: Types of ACW to compute\nn_lags::Union{Int, Nothing}=nothing: Number of lags for ACF calculation\nfreqlims::Union{Tuple{Real, Real}, Nothing}=nothing: Frequency limits for spectral analysis\ntime::Union{Vector{Real}, Nothing}=nothing: Time vector. This is required for Lomb-Scargle method in the case of missing data.\ndims::Int=ndims(data): Dimension along which to compute ACW (Dimension of time)\nreturn_acf::Bool=true: Whether to return the ACF\nreturn_psd::Bool=true: Whether to return the PSD\naverage_over_trials::Bool=false: Whether to average the ACF or PSD over trials\ntrial_dims::Int=setdiff([1, 2], dims)[1]: Dimension along which to average the ACF or PSD over trials (Dimension of trials)\nmax_peaks::Int=1: Maximum number of oscillatory peaks to fit in spectral analysis\noscillation_peak::Bool=true: Whether to fit an oscillation peak in the spectral analysis\n\nReturns\n\nVector of computed ACW measures, ordered according to input acwtypes\n\nNotes\n\nSupported ACW types:\n:acw0 - Time to first zero crossing\n:acw50 - Time to 50% decay\n:acweuler - Time to 1/e decay\n:tau - Exponential decay timescale\n:knee - Knee frequency from spectral analysis\nIf n_lags is not specified, uses 1.1 * ACW0\nFor spectral measures, freqlims defaults to full frequency range\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.TuringBackend.ADVIResults","page":"API","title":"IntrinsicTimescales.TuringBackend.ADVIResults","text":"ADVIResults{T<:Real}\n\nContainer for ADVI (Automatic Differentiation Variational Inference) results.\n\nFields\n\nsamples::AbstractArray{T}: Matrix of posterior samples\nMAP::AbstractVector{T}: Maximum a posteriori estimates\nvariances::AbstractVector{T}: Posterior variances for each parameter\nchain: Turing chain object containing full inference results\n\n\n\n\n\n","category":"type"},{"location":"#IntrinsicTimescales.TuringBackend.create_turing_model-Tuple{Any, Any}","page":"API","title":"IntrinsicTimescales.TuringBackend.create_turing_model","text":"create_turing_model(model, data_sum_stats; Ïƒ_prior=Exponential(1))\n\nCreate a Turing probabilistic model for variational inference.\n\nArguments\n\nmodel: Model instance containing prior distributions and data generation methods\ndata_sum_stats: Summary statistics of the observed data\nÏƒ_prior=Exponential(1): Prior distribution for the uncertainty parameter Ïƒ\n\nReturns\n\nTuring model object ready for inference\n\nNotes\n\nThe created model includes:\n\nParameter sampling from truncated priors (positive values only)\nData generation using the model's forward simulation\nLikelihood computation using Normal distribution\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.TuringBackend.fit_vi-Tuple{Any}","page":"API","title":"IntrinsicTimescales.TuringBackend.fit_vi","text":"fit_vi(model; n_samples=4000, n_iterations=10, n_elbo_samples=20, \n       optimizer=AutoForwardDiff())\n\nPerform variational inference using ADVI (Automatic Differentiation Variational Inference).\n\nArguments\n\nmodel: Model instance to perform inference on\nn_samples::Int=4000: Number of posterior samples to draw\nn_iterations::Int=10: Number of ADVI iterations\nn_elbo_samples::Int=20: Number of samples for ELBO estimation\noptimizer=AutoForwardDiff(): Optimization algorithm for ADVI\n\nReturns\n\nADVIResults: Container with inference results including:\nPosterior samples\nMAP estimates\nParameter variances\nFull Turing chain\n\nNotes\n\nUses Turing.jl's ADVI implementation for fast approximate Bayesian inference. The model is automatically constructed with appropriate priors and likelihood.\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.TuringBackend.get_param_dict_advi-Tuple{}","page":"API","title":"IntrinsicTimescales.TuringBackend.get_param_dict_advi","text":"get_param_dict_advi()\n\nGet default parameter dictionary for ADVI (Automatic Differentiation Variational Inference) algorithm.\n\nReturns\n\nDictionary containing default values for ADVI parameters including:\n\nn_samples: Number of posterior samples to draw (default: 4000)\nn_iterations: Number of ADVI iterations (default: 50) \nn_elbo_samples: Number of samples for ELBO estimation (default: 20)\nautodiff: Automatic differentiation backend (default: AutoForwardDiff())\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats","page":"API","title":"IntrinsicTimescales.SummaryStats","text":"SummaryStats\n\nModule for computing various summary statistics from time series data. Includes functions for:\n\nAutocorrelation (FFT and time-domain methods)\nPower spectral density (periodogram and Welch methods)\nCross-correlation\nSpecial handling for missing data (NaN values)\n\n\n\n\n\n","category":"module"},{"location":"#IntrinsicTimescales.SummaryStats._comp_psd_lombscargle-Tuple{AbstractVector{<:Number}, AbstractVector{<:Number}, AbstractVector{<:Number}}","page":"API","title":"IntrinsicTimescales.SummaryStats._comp_psd_lombscargle","text":"_comp_psd_lombscargle(times, data, frequency_grid)\n\nInternal function to compute Lomb-Scargle periodogram for a single time series.\n\nArguments\n\ntimes: Time points vector (without NaN)\ndata: Time series data (without NaN)\nfrequency_grid: Pre-computed frequency grid\n\nReturns\n\npower: Lomb-Scargle periodogram values\nfrequency_grid: Input frequency grid\n\nNotes\n\nUses LombScargle.jl for core computation\nAssumes data has been pre-processed and doesn't contain NaN values\nNormalizes power spectrum by variance\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats.acf_statsmodels-Union{Tuple{Vector{T}}, Tuple{T}} where T<:Real","page":"API","title":"IntrinsicTimescales.SummaryStats.acf_statsmodels","text":"acf_statsmodels(x::Vector{T}; kwargs...) where {T <: Real}\n\nJulia implementation of statsmodels.tsa.stattools.acf function. Only for testing.\n\nArguments\n\nx: Time series data vector\nadjusted=false: Use n-k denominators if true\nnlags=nothing: Number of lags (default: min(10*log10(n), n-1))\nqstat=false: Return Ljung-Box Q-statistics\nisfft=false: Use FFT method\nalpha=nothing: Confidence level for intervals\nbartlett_confint=false: Use Bartlett's formula\nmissing_handling=\"conservative\": NaN handling method\n\nReturns\n\nVector of autocorrelation values\n\nNotes\n\nSupports multiple missing data handling methods:\n\"none\": No checks\n\"raise\": Error on NaN\n\"conservative\": NaN-aware computations\n\"drop\": Remove NaN values\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats.acovf-Union{Tuple{Vector{T}}, Tuple{T}} where T<:Real","page":"API","title":"IntrinsicTimescales.SummaryStats.acovf","text":"Estimate autocovariances. Translated to Julia from statsmodels.tsa.stattools\n\nParameters\n\nx : array_like     Time series data. Must be 1d. adjusted : bool, default False     If True, then denominators is n-k, otherwise n. demean : bool, default True     If True, then subtract the mean x from each element of x. fft : bool, default True     If True, use FFT convolution.  This method should be preferred     for long time series. missing : str, default \"none\"     A string in [\"none\", \"raise\", \"conservative\", \"drop\"] specifying how     the NaNs are to be treated. \"none\" performs no checks. \"raise\" raises     an exception if NaN values are found. \"drop\" removes the missing     observations and then estimates the autocovariances treating the     non-missing as contiguous. \"conservative\" computes the autocovariance     using nan-ops so that nans are removed when computing the mean     and cross-products that are used to estimate the autocovariance.     When using \"conservative\", n is set to the number of non-missing     observations. nlag : {int, None}, default None     Limit the number of autocovariances returned.  Size of returned     array is nlag + 1.  Setting nlag when fft is False uses a simple,     direct estimator of the autocovariances that only computes the first     nlag + 1 values. This can be much faster when the time series is long     and only a small number of autocovariances are needed.\n\nReturns\n\nndarray     The estimated autocovariances.\n\nReferences\n\n.. [1] Parzen, E., 1963. On spectral analysis with missing observations        and amplitude modulation. Sankhya: The Indian Journal of        Statistics, Series A, pp.383-392.\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats.bat_autocorr-Tuple{AbstractVector{<:Real}}","page":"API","title":"IntrinsicTimescales.SummaryStats.bat_autocorr","text":"bat_autocorr(x::AbstractVector{<:Real})\n\nCompute the normalized autocorrelation function of a 1D time series using FFT. Returns a vector containing the autocorrelation values.\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats.bat_autocorr-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T<:Real","page":"API","title":"IntrinsicTimescales.SummaryStats.bat_autocorr","text":"bat_autocorr(x::AbstractMatrix{<:Real})\n\nCompute the normalized autocorrelation function of a 2D time series using FFT. data is a matrix with dimensions (nseries Ã— ntimepoints)\n\nreturns a matrix with dimensions (nseries Ã— nlags)\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats.bat_integrated_autocorr_len","page":"API","title":"IntrinsicTimescales.SummaryStats.bat_integrated_autocorr_len","text":"bat_integrated_autocorr_len(\n    v::AbstractVectorOfSimilarVectors{<:Real};\n    c::Integer = 5, tol::Integer = 50, strict = true\n)\n\nEstimate the integrated autocorrelation length of variate series v.\n\nc: Step size for window search.\ntol: Minimum number of autocorrelation times needed to trust the estimate.\nstrict: Throw exception if result is not trustworthy\n\nThis estimate uses the iterative procedure described on page 16 of Sokal's notes to determine a reasonable window size.\n\nPorted to Julia from the emcee Python package, under MIT License. Original authors Dan Foreman-Mackey et al.\n\n\n\n\n\n","category":"function"},{"location":"#IntrinsicTimescales.SummaryStats.bat_integrated_autocorr_weight","page":"API","title":"IntrinsicTimescales.SummaryStats.bat_integrated_autocorr_weight","text":"bat_integrated_autocorr_weight(\n    samples::DensitySampleVector;\n    c::Integer = 5, tol::Integer = 50, strict = true\n)\n\nEstimate the integrated autocorrelation weight of samples.\n\nUses bat_integrated_autocorr_len.     \n\n\n\n\n\n","category":"function"},{"location":"#IntrinsicTimescales.SummaryStats.comp_ac_fft-Union{Tuple{AbstractArray{T}}, Tuple{T}} where T<:Real","page":"API","title":"IntrinsicTimescales.SummaryStats.comp_ac_fft","text":"comp_ac_fft(data::AbstractArray{T}; dims::Int=ndims(data), n_lags::Integer=size(data, dims)) where {T <: Real}\n\nCompute autocorrelation using FFT along specified dimension.\n\nArguments\n\ndata: Array of time series data\ndims: Dimension along which to compute autocorrelation (defaults to last dimension)\nn_lags: Number of lags to compute (defaults to size of data along specified dimension)\n\nReturns\n\nArray with autocorrelation values, the specified dimension becomes the dimension of lags while the other dimensions denote ACF values\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats.comp_ac_fft-Union{Tuple{Vector{T}}, Tuple{T}} where T<:Real","page":"API","title":"IntrinsicTimescales.SummaryStats.comp_ac_fft","text":"comp_ac_fft(data::Vector{T}; n_lags::Real=length(data)) where {T <: Real}\n\nCompute autocorrelation using FFT method.\n\nArguments\n\ndata: Input time series vector\nn_lags: Number of lags to compute (defaults to length of data)\n\nReturns\n\nVector of autocorrelation values from lag 0 to n_lags-1\n\nNotes\n\nUses FFT for efficient computation\nPads data to next power of 2 for FFT efficiency\nNormalizes by variance (first lag)\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats.comp_ac_time-Union{Tuple{Vector{T}}, Tuple{T}} where T<:Real","page":"API","title":"IntrinsicTimescales.SummaryStats.comp_ac_time","text":"comp_ac_time(data::AbstractArray{T}, max_lag::Integer; dims::Int=ndims(data)) where {T <: Real}\n\nCompute autocorrelation in time domain along specified dimension.\n\nArguments\n\ndata: Array of time series data\nmax_lag: Maximum lag to compute\ndims: Dimension along which to compute autocorrelation (defaults to last dimension)\n\nReturns\n\nArray with autocorrelation values, the specified dimension becomes the dimension of lags while the other dimensions denote ACF values\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats.comp_ac_time_missing-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T<:Real","page":"API","title":"IntrinsicTimescales.SummaryStats.comp_ac_time_missing","text":"comp_ac_time_missing(data::AbstractArray{T}; kwargs...) where {T <: Real}\n\nCompute autocorrelation for data with missing values.\n\nArguments\n\ndata: Time series data (may contain NaN)\ndims=ndims(data): Dimension along which to compute\nn_lags=size(data,dims): Number of lags to compute\n\nReturns\n\nArray of autocorrelation values\n\nNotes\n\nHandles missing data using missing=\"conservative\" approach of \n\nstatsmodels.tsa.stattools.acf. See https://www.statsmodels.org/dev/generated/statsmodels.tsa.stattools.acf.html  for details. \n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats.comp_cc-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}, Integer}} where T<:Real","page":"API","title":"IntrinsicTimescales.SummaryStats.comp_cc","text":"comp_cc(data1::AbstractArray{T}, data2::AbstractArray{T}, max_lag::Integer;\n       dims::Int=ndims(data1)) where {T <: Real}\n\nCompute cross-correlation between two arrays along specified dimension.\n\nArguments\n\ndata1: First array of time series data\ndata2: Second array of time series data\nmax_lag: Maximum lag to compute\ndims: Dimension along which to compute cross-correlation (defaults to last dimension)\n\nReturns\n\nArray with cross-correlation values, reduced along specified dimension\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats.comp_psd-Union{Tuple{T}, Tuple{AbstractArray{T}, Real}} where T<:Real","page":"API","title":"IntrinsicTimescales.SummaryStats.comp_psd","text":"comp_psd(x::AbstractArray{T}, fs::Real; kwargs...) where {T <: Real}\n\nCompute power spectral density using periodogram or welch method.\n\nArguments\n\nx: Time series data (time Ã— channels)\nfs: Sampling frequency\ndims=ndims(x): Dimension along which to compute PSD\nmethod=\"periodogram\": Method to use (\"periodogram\" or \"welch\")\nwindow=dsp.hamming: Window function\nn=div(size(x,dims),8): Window size for Welch method\nnoverlap=div(n,2): Overlap for Welch method\n\nReturns\n\npower: Power spectral density values\nfreqs: Corresponding frequencies\n\nNotes\n\nFor Welch method, carefully consider window size and overlap\nUses DSP.jl for underlying computations\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats.comp_psd_adfriendly-Tuple{AbstractArray{<:Real}, Real}","page":"API","title":"IntrinsicTimescales.SummaryStats.comp_psd_adfriendly","text":"comp_psd_adfriendly(x::AbstractArray{<:Real}, fs::Real; dims::Int=ndims(x))\n\nCompute power spectral density using an automatic differentiation (AD) friendly implementation.\n\nArguments\n\nx: Time series data\nfs: Sampling frequency\ndims=ndims(x): Dimension along which to compute PSD\n\nReturns\n\npower: Power spectral density values\nfreqs: Corresponding frequencies\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats.comp_psd_lombscargle-Tuple{AbstractVector{<:Number}, AbstractVector{<:Number}, AbstractVector{Bool}, Real}","page":"API","title":"IntrinsicTimescales.SummaryStats.comp_psd_lombscargle","text":"comp_psd_lombscargle(times, data, nanmask, dt; dims=ndims(data))\n\nCompute Lomb-Scargle periodogram for data with missing values.\n\nArguments\n\ntimes: Time points vector\ndata: Time series data (may contain NaN)\nnanmask: Boolean mask indicating NaN positions\ndt: Time step\ndims=ndims(data): Dimension along which to compute\n\nReturns\n\npower: Lomb-Scargle periodogram values\nfrequency_grid: Corresponding frequencies\n\nNotes\n\nHandles irregular sampling due to missing data\nUses frequency grid based on shortest valid time series\nAutomatically determines appropriate frequency range\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats.prepare_lombscargle-Union{Tuple{S}, Tuple{T}, Tuple{AbstractVector{T}, AbstractMatrix{S}, AbstractMatrix{Bool}, Real}} where {T<:Number, S<:Number}","page":"API","title":"IntrinsicTimescales.SummaryStats.prepare_lombscargle","text":"prepare_lombscargle(times, data, nanmask)\n\nPrepare data for Lomb-Scargle periodogram computation by handling missing values.\n\nArguments\n\ntimes: Time points vector\ndata: Time series data (may contain NaN)\nnanmask: Boolean mask indicating NaN positions\n\nReturns\n\nvalid_times: Time points with NaN values removed\nvalid_data: Data points with NaN values removed\nfrequency_grid: Suggested frequency grid for analysis\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Distances","page":"API","title":"IntrinsicTimescales.Distances","text":"Distances\n\nModule providing distance metrics for comparing summary statistics in ABC inference. Currently implements linear (L2) and logarithmic distances.\n\n\n\n\n\n","category":"module"},{"location":"#IntrinsicTimescales.Distances.linear_distance-Tuple{Union{Real, AbstractArray}, Union{Real, AbstractArray}}","page":"API","title":"IntrinsicTimescales.Distances.linear_distance","text":"linear_distance(data, synth_data)\n\nCompute mean squared (L2) distance between summary statistics.\n\nArguments\n\ndata::Union{AbstractArray, Real}: Observed data summary statistics\nsynth_data::Union{AbstractArray, Real}: Simulated data summary statistics\n\nReturns\n\nFloat64: Mean squared difference between data and synth_data\n\nNotes\n\nHandles both scalar and array inputs\nFor arrays, computes element-wise differences before averaging\nUseful for comparing summary statistics on linear scales\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Distances.logarithmic_distance-Tuple{Union{Real, AbstractArray}, Union{Real, AbstractArray}}","page":"API","title":"IntrinsicTimescales.Distances.logarithmic_distance","text":"logarithmic_distance(data, synth_data)\n\nCompute mean squared distance between logarithms of summary statistics.\n\nArguments\n\ndata::Union{AbstractArray, Real}: Observed data summary statistics\nsynth_data::Union{AbstractArray, Real}: Simulated data summary statistics\n\nReturns\n\nFloat64: Mean squared difference between log(data) and log(synth_data)\n\nNotes\n\nHandles both scalar and array inputs\nFor arrays, computes element-wise log differences before averaging\nUseful for comparing summary statistics spanning multiple orders of magnitude\nAssumes all values are positive\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils","page":"API","title":"IntrinsicTimescales.Utils","text":"Utils\n\nModule providing utility functions for time series analysis, including:\n\nExponential decay fitting\nOscillation peak detection\nKnee frequency estimation\nLorentzian fitting\nACF width calculations\n\n\n\n\n\n","category":"module"},{"location":"#IntrinsicTimescales.Utils.acw0-Union{Tuple{S}, Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{S}}} where {T<:Real, S<:Real}","page":"API","title":"IntrinsicTimescales.Utils.acw0","text":"acw0(lags, acf; dims=ndims(acf))\n\nCompute the ACW0 (autocorrelation width at zero crossing) along specified dimension.\n\nArguments\n\nlags::AbstractVector{T}: Vector of lag values\nacf::AbstractArray{T}: Array of autocorrelation values\ndims::Int=ndims(acf): Dimension along which to compute ACW0\n\nReturns\n\nFirst lag where autocorrelation crosses zero\n\nNotes\n\nAlternative measure of characteristic timescale\nMore sensitive to noise than ACW50\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.acw50-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T<:Real","page":"API","title":"IntrinsicTimescales.Utils.acw50","text":"acw50(lags, acf; dims=ndims(acf))\n\nCompute the ACW50 (autocorrelation width at 50%) along specified dimension.\n\nArguments\n\nlags::AbstractVector{T}: Vector of lag values\nacf::AbstractArray{T}: Array of autocorrelation values\ndims::Int=ndims(acf): Dimension along which to compute ACW50\n\nReturns\n\nFirst lag where autocorrelation falls below 0.5\n\nNotes\n\nUsed for estimating characteristic timescales\nRelated to tau by: tau = -acw50/log(0.5)\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.acw_romberg-Union{Tuple{S}, Tuple{Real, AbstractVector{S}}} where S<:Real","page":"API","title":"IntrinsicTimescales.Utils.acw_romberg","text":"acw_romberg(lags, acf)\n\nCalculate the area under the curve of ACF using Romberg integration.\n\nArguments\n\ndt::Real: Time step\nacf::AbstractVector: Array of autocorrelation values\n\nReturns\n\nAUC of ACF\n\nNotes\n\nReturns only the integral value, discarding the error estimate\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.acweuler-Union{Tuple{S}, Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{S}}} where {T<:Real, S<:Real}","page":"API","title":"IntrinsicTimescales.Utils.acweuler","text":"acweuler(lags, acf; dims=ndims(acf))\n\nCompute the ACW at 1/e (â‰ˆ 0.368) along specified dimension.\n\nArguments\n\nlags::AbstractVector{T}: Vector of lag values\nacf::AbstractArray{S}: Array of autocorrelation values\ndims::Int=ndims(acf): Dimension along which to compute\n\nReturns\n\nFirst lag where autocorrelation falls below 1/e\n\nNotes\n\nFor exponential decay, equals the timescale parameter tau\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.expdecay-Tuple{Any, Any}","page":"API","title":"IntrinsicTimescales.Utils.expdecay","text":"expdecay(tau, lags)\n\nCompute exponential decay function.\n\nArguments\n\ntau::Real: Timescale parameter\nlags::AbstractVector: Time lags\n\nReturns\n\nVector of exp(-t/tau) values\n\nNotes\n\nUsed for fitting autocorrelation functions\nAssumes exponential decay model: acf = exp(-t/tau)\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.find_knee_frequency-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T<:Real","page":"API","title":"IntrinsicTimescales.Utils.find_knee_frequency","text":"find_knee_frequency(psd, freqs; dims=ndims(psd), min_freq=freqs[1], max_freq=freqs[end])\n\nFind knee frequency by fitting Lorentzian to power spectral density.\n\nArguments\n\npsd::AbstractArray{T}: Power spectral density values\nfreqs::Vector{T}: Frequency values\ndims::Int=ndims(psd): Dimension along which to compute\nmin_freq::T=freqs[1]: Minimum frequency to consider\nmax_freq::T=freqs[end]: Maximum frequency to consider\n\nReturns\n\nKnee frequency values (frequency at half power)\n\nNotes\n\nUses Lorentzian fitting with NonlinearSolve.jl\nInitial guess based on half-power point\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.find_oscillation_peak-Tuple{AbstractVector{<:Real}, AbstractVector{<:Real}}","page":"API","title":"IntrinsicTimescales.Utils.find_oscillation_peak","text":"find_oscillation_peak(psd, freqs; min_freq=5.0/1000.0, max_freq=50.0/1000.0, min_prominence_ratio=0.1)\n\nFind dominant oscillatory peak in power spectral density.\n\nArguments\n\npsd::AbstractVector: Power spectral density values\nfreqs::AbstractVector: Frequency values\nmin_freq::Real=5.0/1000.0: Minimum frequency to consider\nmax_freq::Real=50.0/1000.0: Maximum frequency to consider\nmin_prominence_ratio::Real=0.1: Minimum peak prominence as fraction of max PSD\n\nReturns\n\nFrequency of most prominent peak, or NaN if no significant peak found\n\nNotes\n\nUses peak prominence for robustness\nFilters peaks by minimum prominence threshold\nReturns NaN if no peaks meet criteria\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.fit_expdecay-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T<:Real","page":"API","title":"IntrinsicTimescales.Utils.fit_expdecay","text":"fit_expdecay(lags, acf; dims=ndims(acf))\n\nFit exponential decay to autocorrelation function.\n\nArguments\n\nlags::AbstractVector{T}: Time lags\nacf::AbstractArray{T}: Autocorrelation values\ndims::Int=ndims(acf): Dimension along which to fit\n\nReturns\n\nFitted timescale parameter(s)\n\nNotes\n\nUses NonlinearSolve.jl with FastShortcutNLLSPolyalg\nInitial guess based on ACW50\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.fit_gaussian-Tuple{AbstractVector{<:Real}, AbstractVector{<:Real}, Real}","page":"API","title":"IntrinsicTimescales.Utils.fit_gaussian","text":"fit_gaussian(psd, freqs, initial_peak; min_freq=freqs[1], max_freq=freqs[end])\n\nFit Gaussian to power spectral density around a peak.\n\nArguments\n\npsd::AbstractVector{<:Real}: Power spectral density values\nfreqs::AbstractVector{<:Real}: Frequency values\ninitial_peak::Real: Initial guess for center frequency\nmin_freq::Real: Minimum frequency to consider\nmax_freq::Real: Maximum frequency to consider\n\nReturns\n\nVector{Float64}: Fitted parameters [amplitude, centerfreq, stddev]\n\nNotes\n\nUses initial peak location from findoscillationpeak\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.fooof_fit-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T<:Real","page":"API","title":"IntrinsicTimescales.Utils.fooof_fit","text":"fooof_fit(psd, freqs; dims=ndims(psd), min_freq=freqs[1], max_freq=freqs[end], \n          oscillation_peak=true, max_peaks=3)\n\nPerform FOOOF-style fitting of power spectral density. The main difference from FOOOF is that FOOOF allows for PLE != 2.  Whereas here we are confined to PLE = 2. \n\nArguments\n\npsd::AbstractArray{T}: Power spectral density values\nfreqs::Vector{T}: Frequency values\ndims::Int=ndims(psd): Dimension along which to compute\nmin_freq::T=freqs[1]: Minimum frequency to consider\nmax_freq::T=freqs[end]: Maximum frequency to consider\noscillation_peak::Bool=true: Whether to compute oscillation peaks\nmax_peaks::Int=3: Maximum number of oscillatory peaks to fit\n\nReturns\n\nIf oscillation_peak=true:\n\nTuple of (kneefrequency, oscillationparameters) where oscillationparameters is Vector of (centerfreq, amplitude, std_dev) for each peak\n\nIf oscillation_peak=false:\n\nknee_frequency only\n\nNotes\n\nImplements iterative FOOOF-style fitting:\nFit initial Lorentzian to PSD\nFind and fit Gaussian peaks iteratively\nSubtract all Gaussians from original PSD\nRefit Lorentzian to cleaned PSD\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.gaussian-Tuple{Any, Any}","page":"API","title":"IntrinsicTimescales.Utils.gaussian","text":"gaussian(f, u)\n\nGaussian function for fitting oscillations. \n\nArguments\n\nf::AbstractVector: Frequency values\nu::Vector: Parameters [amplitude, centerfreq, stddev]\n\nReturns\n\nVector of Gaussian values: amp * exp(-(f-center)Â²/(2*stdÂ²))\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.lorentzian-Tuple{Any, Any}","page":"API","title":"IntrinsicTimescales.Utils.lorentzian","text":"lorentzian(f, u)\n\nCompute Lorentzian function values.\n\nArguments\n\nf::AbstractVector: Frequency values\nu::Vector: Parameters [amplitude, knee_frequency]\n\nReturns\n\nVector of Lorentzian values: amp/(1 + (f/knee)Â²)\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.lorentzian_initial_guess-Tuple{AbstractVector{<:Real}, AbstractVector{<:Real}}","page":"API","title":"IntrinsicTimescales.Utils.lorentzian_initial_guess","text":"lorentzian_initial_guess(psd, freqs; min_freq=freqs[1], max_freq=freqs[end])\n\nEstimate initial parameters for Lorentzian fitting.\n\nArguments\n\npsd::AbstractVector{<:Real}: Power spectral density values\nfreqs::AbstractVector{<:Real}: Frequency values\nmin_freq::Real: Minimum frequency to consider\nmax_freq::Real: Maximum frequency to consider\n\nReturns\n\nVector{Float64}: Initial guess for [amplitude, knee_frequency]\n\nNotes\n\nEstimates amplitude from maximum PSD value\nEstimates knee frequency from half-power point\nUsed as starting point for nonlinear fitting\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.residual_expdecay!-Tuple{Any, Any, Any}","page":"API","title":"IntrinsicTimescales.Utils.residual_expdecay!","text":"Residual function for expdecay du: residual u: parameters p: data\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.OrnsteinUhlenbeck","page":"API","title":"IntrinsicTimescales.OrnsteinUhlenbeck","text":"OrnsteinUhlenbeck\n\nModule for generating Ornstein-Uhlenbeck processes with various configurations. Uses DifferentialEquations.jl. \n\n\n\n\n\n","category":"module"},{"location":"#IntrinsicTimescales.OrnsteinUhlenbeck.generate_ou_process-Tuple{Union{Real, Vector{<:Real}}, Vararg{Real, 4}}","page":"API","title":"IntrinsicTimescales.OrnsteinUhlenbeck.generate_ou_process","text":"generate_ou_process(tau, true_D, dt, duration, num_trials; standardize=true)\n\nGenerate an Ornstein-Uhlenbeck process with a single timescale\n\nArguments\n\ntau::Union{Real, Vector{<:Real}}: Timescale(s) of the OU process\ntrue_D::Real: Target variance for scaling the process\ndt::Real: Time step size\nduration::Real: Total time length\nnum_trials::Real: Number of trials/trajectories\nstandardize::Bool=true: Whether to standardize output to match true_D\n\nReturns\n\nMatrix{Float64}: Generated OU process data with dimensions (numtrials, numtimesteps)\n\nNotes\n\nUses generateouprocess_sciml internally\nReturns NaN matrix if SciML solver fails\nStandardizes output to have specified variance if standardize=true\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.OrnsteinUhlenbeck.generate_ou_process_sciml-Union{Tuple{T}, Tuple{Union{Vector{T}, T}, Real, Real, Real, Integer}, Tuple{Union{Vector{T}, T}, Real, Real, Real, Integer, Bool}} where T<:Real","page":"API","title":"IntrinsicTimescales.OrnsteinUhlenbeck.generate_ou_process_sciml","text":"generate_ou_process_sciml(tau, true_D, dt, duration, num_trials, standardize=true)\n\nGenerate an Ornstein-Uhlenbeck process using DifferentialEquations.jl.\n\nArguments\n\ntau::Union{T, Vector{T}}: Timescale(s) of the OU process\ntrue_D::Real: Target variance for scaling\ndt::Real: Time step size\nduration::Real: Total time length\nnum_trials::Integer: Number of trials/trajectories\nstandardize::Bool=true: Whether to standardize output\n\nReturns\n\nTuple{Matrix{Float64}, ODESolution}: \nScaled OU process data\nFull SDE solution object\n\nNotes\n\nUses SOSRA solver for efficiency\nSwitches between static and dynamic arrays based on num_trials\nStandardizes output to match true_D if standardize=true\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.OrnsteinUhlenbeck.generate_ou_with_oscillation-Union{Tuple{T}, Tuple{Vector{T}, Real, Real, Integer, Real, Real}} where T<:Real","page":"API","title":"IntrinsicTimescales.OrnsteinUhlenbeck.generate_ou_with_oscillation","text":"generate_ou_with_oscillation(theta, dt, duration, num_trials, data_mean, data_var)\n\nGenerate a one-timescale OU process with an additive oscillation.\n\nArguments\n\ntheta::Vector{T}: Parameters [timescale, frequency, coefficient]\ndt::Real: Time step size\nduration::Real: Total time length\nnum_trials::Integer: Number of trials\ndata_mean::Real: Target mean value\ndata_var::Real: Target variance\n\nReturns\n\nMatrix{Float64}: Generated data with dimensions (numtrials, numtimesteps)\n\nNotes\n\nCoefficient is bounded between 0 and 1\nCombines OU process with sinusoidal oscillation\nStandardizes and scales output to match target mean and variance\nReturns NaN matrix if SciML solver fails\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.OneTimescale","page":"API","title":"IntrinsicTimescales.OneTimescale","text":"OneTimescale\n\nModule for inferring a single timescale from time series data using the Ornstein-Uhlenbeck process.\n\n\n\n\n\n","category":"module"},{"location":"#IntrinsicTimescales.OneTimescale.OneTimescaleModel","page":"API","title":"IntrinsicTimescales.OneTimescale.OneTimescaleModel","text":"OneTimescaleModel <: AbstractTimescaleModel\n\nModel for inferring a single timescale from time series data using the Ornstein-Uhlenbeck process. We recommend using the one_timescale_model constructor function rather than creating directly.\n\nFields\n\ndata::AbstractArray{<:Real}: Input time series data\ntime::AbstractVector{<:Real}: Time points corresponding to the data\nfit_method::Symbol: Fitting method (:abc or :advi)\nsummary_method::Symbol: Summary statistic type (:psd or :acf)\nlags_freqs: Lags (for ACF) or frequencies (for PSD)\nprior: Prior distribution(s) for parameters\ndistance_method::Symbol: Distance metric type (:linear or :logarithmic)\ndata_sum_stats: Pre-computed summary statistics\ndt::Real: Time step between observations\nT::Real: Total time span\nnumTrials::Real: Number of trials/iterations\ndata_mean::Real: Mean of input data\ndata_sd::Real: Standard deviation of input data\nfreqlims: Frequency limits for PSD analysis\nn_lags: Number of lags for ACF\nfreq_idx: Boolean mask for frequency selection\ndims::Int: Dimension along which to compute statistics\ndistance_combined::Bool: Whether to use combined distance metric\nweights::Vector{Real}: Weights for combined distance\ndata_tau::Union{Real, Nothing}: Pre-computed timescale\nu0::Union{Vector{Real}, Nothing}: Initial parameter guess\n\n\n\n\n\n","category":"type"},{"location":"#IntrinsicTimescales.Models.distance_function-Tuple{OneTimescaleModel, Any, Any}","page":"API","title":"IntrinsicTimescales.Models.distance_function","text":"Models.distance_function(model::OneTimescaleModel, sum_stats, data_sum_stats)\n\nCalculate the distance between summary statistics of simulated and observed data.\n\nArguments\n\nmodel::OneTimescaleModel: Model instance\nsum_stats: Summary statistics from simulated data\ndata_sum_stats: Summary statistics from observed data\n\nReturns\n\nDistance value based on model.distancemethod (:linear or :logarithmic) or combined distance if model.distancecombined is true\n\nNotes\n\nIf distance_combined is true:\n\nFor ACF: Combines ACF distance with fitted exponential decay timescale distance\nFor PSD: Combines PSD distance with knee frequency timescale distance\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Models.fit-2","page":"API","title":"IntrinsicTimescales.Models.fit","text":"Models.fit(model::OneTimescaleModel, param_dict=nothing)\n\nPerform inference using the specified fitting method.\n\nArguments\n\nmodel::OneTimescaleModel: Model instance\nparam_dict=nothing: Optional dictionary of algorithm parameters. If nothing, uses defaults.\n\nReturns\n\nFor ABC method:\n\nposterior_samples: Matrix of accepted parameter samples\nposterior_MAP: Maximum a posteriori estimate\nabc_record: Full record of ABC iterations\n\nFor ADVI method:\n\nADVIResults: Container with samples, MAP estimates, variances, and full chain\n\nNotes\n\nFor ABC: Uses Population Monte Carlo ABC with adaptive epsilon selection\nFor ADVI: Uses Automatic Differentiation Variational Inference via Turing.jl\nParameter dictionary can be customized for each method (see getparamdict_abc())\n\n\n\n\n\n","category":"function"},{"location":"#IntrinsicTimescales.Models.generate_data-Tuple{OneTimescaleModel, Any}","page":"API","title":"IntrinsicTimescales.Models.generate_data","text":"Models.generate_data(model::OneTimescaleModel, theta)\n\nGenerate synthetic data from the Ornstein-Uhlenbeck process with given timescale.\n\nArguments\n\nmodel::OneTimescaleModel: Model instance containing simulation parameters\ntheta: Vector containing single timescale parameter (Ï„)\n\nReturns\n\nSynthetic time series data with same dimensions as model.data\n\nNotes\n\nUses the model's stored parameters:\n\ndata_sd: Standard deviation for the OU process\ndt: Time step\nT: Total time span\nnumTrials: Number of trials/trajectories\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Models.summary_stats-Tuple{OneTimescaleModel, Any}","page":"API","title":"IntrinsicTimescales.Models.summary_stats","text":"Models.summary_stats(model::OneTimescaleModel, data)\n\nCompute summary statistics (ACF or PSD) from time series data.\n\nArguments\n\nmodel::OneTimescaleModel: Model instance specifying summary statistic type\ndata: Time series data to analyze\n\nReturns\n\nFor ACF (summary_method = :acf):\n\nMean autocorrelation function up to n_lags\n\nFor PSD (summary_method = :psd):\n\nMean power spectral density within specified frequency range\n\nNotes\n\nACF is computed using FFT-based method\nPSD is computed and filtered according to model.freq_idx\nThrows ArgumentError if summary_method is invalid\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.OneTimescale.combined_distance-Tuple{OneTimescaleModel, Vararg{Any, 5}}","page":"API","title":"IntrinsicTimescales.OneTimescale.combined_distance","text":"combined_distance(model::OneTimescaleModel, simulation_summary, data_summary,\n                 weights, data_tau, simulation_tau)\n\nCompute combined distance metric between simulated and observed data.\n\nArguments\n\nmodel: OneTimescaleModel instance\nsimulation_summary: Summary statistics from simulation\ndata_summary: Summary statistics from observed data\nweights: Weights for combining distances\ndata_tau: Timescale from observed data\nsimulation_tau: Timescale from simulation\n\nReturns\n\nWeighted combination of summary statistic distance and timescale distance\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.OneTimescale.one_timescale_model-Tuple{Any, Any, Any}","page":"API","title":"IntrinsicTimescales.OneTimescale.one_timescale_model","text":"one_timescale_model(data, time, fit_method; kwargs...)\n\nConstruct a OneTimescaleModel for time series analysis.\n\nArguments\n\ndata: Input time series data\ntime: Time points corresponding to the data\nfit_method: Fitting method to use (:abc or :advi)\n\nKeyword Arguments\n\nsummary_method=:acf: Summary statistic type (:psd or :acf)\ndata_sum_stats=nothing: Pre-computed summary statistics\nlags_freqs=nothing: Custom lags or frequencies\nprior=nothing: Prior distribution(s) for parameters\nn_lags=nothing: Number of lags for ACF\ndistance_method=nothing: Distance metric type\ndt=time[2]-time[1]: Time step\nT=time[end]: Total time span\nnumTrials=size(data,1): Number of trials\ndata_mean=mean(data): Data mean\ndata_sd=std(data): Data standard deviation\nfreqlims=nothing: Frequency limits for PSD\nfreq_idx=nothing: Frequency selection mask\ndims=ndims(data): Analysis dimension\ndistance_combined=false: Use combined distance\nweights=[0.5, 0.5]: Distance weights\ndata_tau=nothing: Pre-computed timescale\nu0=nothing: Initial parameter guess\n\nReturns\n\nOneTimescaleModel: Model instance configured for specified analysis method\n\nNotes\n\nTwo main usage patterns:\n\nACF-based inference: summary_method=:acf, fit_method=:abc/:advi\nPSD-based inference: summary_method=:psd, fit_method=:abc/:advi\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.OneTimescaleAndOsc.OneTimescaleAndOscModel","page":"API","title":"IntrinsicTimescales.OneTimescaleAndOsc.OneTimescaleAndOscModel","text":"OneTimescaleAndOscModel <: AbstractTimescaleModel\n\nModel for inferring a single timescale and oscillation from time series data using the Ornstein-Uhlenbeck process. Parameters: [tau, freq, coeff] representing timescale, oscillation frequency, and oscillation coefficient.\n\nFields\n\ndata::AbstractArray{<:Real}: Input time series data\ntime::AbstractVector{<:Real}: Time points corresponding to the data\nfit_method::Symbol: Fitting method (:abc or :advi)\nsummary_method::Symbol: Summary statistic type (:psd or :acf)\nlags_freqs: Lags (for ACF) or frequencies (for PSD)\nprior: Prior distribution(s) for parameters\ndistance_method::Symbol: Distance metric type (:linear or :logarithmic)\ndata_sum_stats: Pre-computed summary statistics\ndt::Real: Time step between observations\nT::Real: Total time span\nnumTrials::Real: Number of trials/iterations\ndata_mean::Real: Mean of input data\ndata_sd::Real: Standard deviation of input data\nfreqlims: Frequency limits for PSD analysis\nn_lags: Number of lags for ACF\nfreq_idx: Boolean mask for frequency selection\ndims::Int: Dimension along which to compute statistics\ndistance_combined::Bool: Whether to use combined distance metric\nweights::Vector{Real}: Weights for combined distance\ndata_tau::Union{Real, Nothing}: Pre-computed timescale\ndata_osc::Union{Real, Nothing}: Pre-computed oscillation frequency\n\n\n\n\n\n","category":"type"},{"location":"#IntrinsicTimescales.Models.distance_function-Tuple{OneTimescaleAndOscModel, Any, Any}","page":"API","title":"IntrinsicTimescales.Models.distance_function","text":"Models.distance_function(model::OneTimescaleAndOscModel, sum_stats, data_sum_stats)\n\nCalculate the distance between summary statistics of simulated and observed data.\n\nArguments\n\nmodel::OneTimescaleAndOscModel: Model instance\nsum_stats: Summary statistics from simulated data\ndata_sum_stats: Summary statistics from observed data\n\nReturns\n\nDistance value based on model.distancemethod (:linear or :logarithmic) or combined distance if model.distancecombined is true\n\nNotes\n\nIf distance_combined is true:\n\nFor ACF: Combines ACF distance with fitted exponential decay timescale distance\nFor PSD: Combines PSD distance with knee frequency timescale distance and oscillation frequency distance\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Models.fit-3","page":"API","title":"IntrinsicTimescales.Models.fit","text":"Models.fit(model::OneTimescaleAndOscModel, param_dict=nothing)\n\nPerform inference using the specified fitting method.\n\nArguments\n\nmodel::OneTimescaleAndOscModel: Model instance\nparam_dict=nothing: Optional dictionary of algorithm parameters. If nothing, uses defaults.\n\nReturns\n\nFor ABC method:\n\nposterior_samples: Matrix of accepted parameter samples\nposterior_MAP: Maximum a posteriori estimate\nabc_record: Full record of ABC iterations\n\nFor ADVI method:\n\n`ADVIResults: Container with samples, MAP estimates, variances, and full variational posterior\n\nNotes\n\nFor ABC: Uses Population Monte Carlo ABC with adaptive epsilon selection\nFor ADVI: Uses Automatic Differentiation Variational Inference via Turing.jl\nParameter dictionary can be customized for each method (see getparamdict_abc())\n\n\n\n\n\n","category":"function"},{"location":"#IntrinsicTimescales.Models.generate_data-Tuple{OneTimescaleAndOscModel, AbstractVector{<:Real}}","page":"API","title":"IntrinsicTimescales.Models.generate_data","text":"Models.generate_data(model::OneTimescaleAndOscModel, theta::AbstractVector{<:Real})\n\nGenerate synthetic data from the Ornstein-Uhlenbeck process with oscillation.\n\nArguments\n\nmodel::OneTimescaleAndOscModel: Model instance containing simulation parameters\ntheta::AbstractVector{<:Real}: Vector containing parameters [tau, freq, coeff]\n\nReturns\n\nSynthetic time series data with same dimensions as model.data\n\nNotes\n\nUses the model's stored parameters:\n\ndt: Time step\nT: Total time span\nnumTrials: Number of trials/trajectories\ndata_mean: Mean of the process\ndata_sd: Standard deviation of the process\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Models.summary_stats-Tuple{OneTimescaleAndOscModel, Any}","page":"API","title":"IntrinsicTimescales.Models.summary_stats","text":"Models.summary_stats(model::OneTimescaleAndOscModel, data)\n\nCompute summary statistics (ACF or PSD) from time series data.\n\nArguments\n\nmodel::OneTimescaleAndOscModel: Model instance specifying summary statistic type\ndata: Time series data to analyze\n\nReturns\n\nFor ACF (summary_method = :acf):\n\nMean autocorrelation function up to n_lags\n\nFor PSD (summary_method = :psd):\n\nMean power spectral density within specified frequency range\n\nNotes\n\nACF is computed using FFT-based method\nPSD is computed using AD-friendly implementation\nThrows ArgumentError if summary_method is invalid\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.OneTimescaleAndOsc.combined_distance-Tuple{OneTimescaleAndOscModel, Vararg{Any, 8}}","page":"API","title":"IntrinsicTimescales.OneTimescaleAndOsc.combined_distance","text":"combined_distance(model::OneTimescaleAndOscModel, simulation_summary, data_summary,\n                 weights, distance_method, data_tau, simulation_tau, \n                 data_osc, simulation_osc)\n\nCompute combined distance metric between simulated and observed data.\n\nArguments\n\nmodel: OneTimescaleAndOscModel instance\nsimulation_summary: Summary statistics from simulation\ndata_summary: Summary statistics from observed data\nweights: Weights for combining distances\ndistance_method: Distance metric type\ndata_tau: Timescale from observed data\nsimulation_tau: Timescale from simulation\ndata_osc: Oscillation frequency from observed data\nsimulation_osc: Oscillation frequency from simulation\n\nReturns\n\nFor ACF:\n\nWeighted combination of summary statistic distance and timescale distance\n\nFor PSD:\n\nWeighted combination of summary statistic distance, timescale distance, and oscillation frequency distance\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.OneTimescaleWithMissing","page":"API","title":"IntrinsicTimescales.OneTimescaleWithMissing","text":"OneTimescaleWithMissing\n\nModule for handling time series analysis with missing data. Uses specialized methods for handling NaN values:\n\nFor ACF: Uses compactime_missing (equivalent to statsmodels.tsa.statstools.acf with missing=\"conservative\")\nFor PSD: Uses Lomb-Scargle periodogram to handle irregular sampling\n\n\n\n\n\n","category":"module"},{"location":"#IntrinsicTimescales.OneTimescaleWithMissing.OneTimescaleWithMissingModel","page":"API","title":"IntrinsicTimescales.OneTimescaleWithMissing.OneTimescaleWithMissingModel","text":"OneTimescaleWithMissingModel <: AbstractTimescaleModel\n\nModel for inferring a single timescale from time series data with missing values.\n\nFields\n\ndata::AbstractArray{<:Real}: Input time series data (may contain NaN)\ntime::AbstractVector{<:Real}: Time points corresponding to the data\nfit_method::Symbol: Fitting method (:abc or :advi)\nsummary_method::Symbol: Summary statistic type (:psd or :acf)\nlags_freqs: Lags (for ACF) or frequencies (for PSD)\nprior: Prior distribution(s) for parameters\ndistance_method::Symbol: Distance metric type (:linear or :logarithmic)\ndata_sum_stats: Pre-computed summary statistics\ndt::Real: Time step between observations\nT::Real: Total time span\nnumTrials::Real: Number of trials/iterations\ndata_mean::Real: Mean of input data (excluding NaN)\ndata_sd::Real: Standard deviation of input data (excluding NaN)\nfreqlims: Frequency limits for PSD analysis\nn_lags: Number of lags for ACF\nfreq_idx: Boolean mask for frequency selection\ndims::Int: Dimension along which to compute statistics\ndistance_combined::Bool: Whether to use combined distance metric\nweights::Vector{Real}: Weights for combined distance\ndata_tau::Union{Real, Nothing}: Pre-computed timescale\nmissing_mask::AbstractArray{Bool}: Boolean mask indicating NaN positions\n\n\n\n\n\n","category":"type"},{"location":"#IntrinsicTimescales.Models.distance_function-Tuple{OneTimescaleWithMissingModel, Any, Any}","page":"API","title":"IntrinsicTimescales.Models.distance_function","text":"Models.distance_function(model::OneTimescaleWithMissingModel, sum_stats, data_sum_stats)\n\nCalculate the distance between summary statistics of simulated and observed data.\n\nArguments\n\nmodel::OneTimescaleWithMissingModel: Model instance\nsum_stats: Summary statistics from simulated data\ndata_sum_stats: Summary statistics from observed data\n\nReturns\n\nDistance value based on model.distancemethod (:linear or :logarithmic) or combined distance if model.distancecombined is true\n\nNotes\n\nIf distance_combined is true:\n\nFor ACF: Combines ACF distance with fitted exponential decay timescale distance\nFor PSD: Combines PSD distance with knee frequency timescale distance\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Models.fit-4","page":"API","title":"IntrinsicTimescales.Models.fit","text":"Models.fit(model::OneTimescaleWithMissingModel, param_dict=nothing)\n\nPerform inference using the specified fitting method.\n\nArguments\n\nmodel::OneTimescaleWithMissingModel: Model instance\nparam_dict=nothing: Optional dictionary of algorithm parameters. If nothing, uses defaults.\n\nReturns\n\nFor ABC method:\n\nposterior_samples: Matrix of accepted parameter samples\nposterior_MAP: Maximum a posteriori estimate\nabc_record: Full record of ABC iterations\n\nFor ADVI method:\n\nADVIResults: Container with samples, MAP estimates, variances, and full chain\n\nNotes\n\nFor ABC: Uses Population Monte Carlo ABC with adaptive epsilon selection\nFor ADVI: Uses Automatic Differentiation Variational Inference via Turing.jl\nParameter dictionary can be customized for each method (see getparamdict_abc())\n\n\n\n\n\n","category":"function"},{"location":"#IntrinsicTimescales.Models.generate_data-Tuple{OneTimescaleWithMissingModel, Any}","page":"API","title":"IntrinsicTimescales.Models.generate_data","text":"Models.generate_data(model::OneTimescaleWithMissingModel, theta)\n\nGenerate synthetic data from the Ornstein-Uhlenbeck process and apply missing data mask.\n\nArguments\n\nmodel::OneTimescaleWithMissingModel: Model instance containing simulation parameters\ntheta: Vector containing single timescale parameter (Ï„)\n\nReturns\n\nSynthetic time series data with NaN values at positions specified by model.missing_mask\n\nNotes\n\nGenerates complete OU process data\nApplies missing data mask from original data\nReturns data with same missing value pattern as input\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Models.summary_stats-Tuple{OneTimescaleWithMissingModel, Any}","page":"API","title":"IntrinsicTimescales.Models.summary_stats","text":"Models.summary_stats(model::OneTimescaleWithMissingModel, data)\n\nCompute summary statistics (ACF or PSD) from time series data with missing values.\n\nArguments\n\nmodel::OneTimescaleWithMissingModel: Model instance specifying summary statistic type\ndata: Time series data to analyze (may contain NaN)\n\nReturns\n\nFor ACF (summary_method = :acf):\n\nMean autocorrelation function up to n_lags, computed with missing data handling\n\nFor PSD (summary_method = :psd):\n\nMean Lomb-Scargle periodogram within specified frequency range\n\nNotes\n\nACF uses compactime_missing for proper handling of NaN values\nPSD uses Lomb-Scargle periodogram for irregular sampling\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.OneTimescaleWithMissing.one_timescale_with_missing_model-Tuple{Any, Any, Any}","page":"API","title":"IntrinsicTimescales.OneTimescaleWithMissing.one_timescale_with_missing_model","text":"one_timescale_with_missing_model(data, time, fit_method; kwargs...)\n\nConstruct a OneTimescaleWithMissingModel for time series analysis with missing data.\n\nArguments\n\ndata: Input time series data (may contain NaN)\ntime: Time points corresponding to the data\nfit_method: Fitting method to use (:abc or :advi)\n\nKeyword Arguments\n\nsummary_method=:acf: Summary statistic type (:psd or :acf)\ndata_sum_stats=nothing: Pre-computed summary statistics\nlags_freqs=nothing: Custom lags or frequencies\nprior=nothing: Prior distribution(s) for parameters\nn_lags=nothing: Number of lags for ACF\ndistance_method=nothing: Distance metric type\ndt=time[2]-time[1]: Time step\nT=time[end]: Total time span\nnumTrials=size(data,1): Number of trials\ndata_mean=nanmean(data): Data mean (excluding NaN)\ndata_sd=nanstd(data): Data standard deviation (excluding NaN)\nfreqlims=nothing: Frequency limits for PSD\nfreq_idx=nothing: Frequency selection mask\ndims=ndims(data): Analysis dimension\ndistance_combined=false: Use combined distance\nweights=[0.5, 0.5]: Distance weights\ndata_tau=nothing: Pre-computed timescale\n\nReturns\n\nOneTimescaleWithMissingModel: Model instance configured for specified analysis method\n\nNotes\n\nFour main usage patterns:\n\nACF-based ABC/ADVI: summary_method=:acf, fit_method=:abc/:advi\nPSD-based ABC/ADVI: summary_method=:psd, fit_method=:abc/:advi\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.OneTimescaleAndOscWithMissing","page":"API","title":"IntrinsicTimescales.OneTimescaleAndOscWithMissing","text":"OneTimescaleAndOscWithMissing\n\nModule for handling time series analysis with both oscillations and missing data. Uses specialized methods for handling NaN values:\n\nFor ACF: Uses compactime_missing for proper handling of gaps\nFor PSD: Uses Lomb-Scargle periodogram for irregular sampling\n\n\n\n\n\n","category":"module"},{"location":"#IntrinsicTimescales.OneTimescaleAndOscWithMissing.OneTimescaleAndOscWithMissingModel","page":"API","title":"IntrinsicTimescales.OneTimescaleAndOscWithMissing.OneTimescaleAndOscWithMissingModel","text":"OneTimescaleAndOscWithMissingModel <: AbstractTimescaleModel\n\nModel for inferring a single timescale and oscillation from time series data with missing values. Parameters: [tau, freq, coeff] representing timescale, oscillation frequency, and oscillation coefficient.\n\nFields\n\ndata::AbstractArray{<:Real}: Input time series data (may contain NaN)\ntime::AbstractVector{<:Real}: Time points corresponding to the data\nfit_method::Symbol: Fitting method (:abc, :advi)\nsummary_method::Symbol: Summary statistic type (:psd or :acf)\nlags_freqs: Lags (for ACF) or frequencies (for PSD)\nprior: Prior distribution(s) for parameters\noptalg: Optimization algorithm for :optimization method\ndistance_method::Symbol: Distance metric type (:linear or :logarithmic)\ndata_sum_stats: Pre-computed summary statistics\ndt::Real: Time step between observations\nT::Real: Total time span\nnumTrials::Real: Number of trials/iterations\ndata_mean::Real: Mean of input data (excluding NaN)\ndata_sd::Real: Standard deviation of input data (excluding NaN)\nfreqlims: Frequency limits for PSD analysis\nn_lags: Number of lags for ACF\nfreq_idx: Boolean mask for frequency selection\ndims::Int: Dimension along which to compute statistics\ndistance_combined::Bool: Whether to use combined distance metric\nweights::Vector{Real}: Weights for combined distance\ndata_tau::Union{Real, Nothing}: Pre-computed timescale\ndata_osc::Union{Real, Nothing}: Pre-computed oscillation frequency\nmissing_mask::AbstractArray{Bool}: Boolean mask indicating NaN positions\n\n\n\n\n\n","category":"type"},{"location":"#IntrinsicTimescales.Models.fit-5","page":"API","title":"IntrinsicTimescales.Models.fit","text":"Models.fit(model::OneTimescaleAndOscWithMissingModel, param_dict=nothing)\n\nPerform inference using the specified fitting method.\n\nArguments\n\nmodel::OneTimescaleAndOscWithMissingModel: Model instance\nparam_dict=nothing: Optional dictionary of algorithm parameters. If nothing, uses defaults.\n\nReturns\n\nFor ABC method:\n\nposterior_samples: Matrix of accepted parameter samples\nposterior_MAP: Maximum a posteriori estimate\nabc_record: Full record of ABC iterations\n\nFor ADVI method:\n\nADVIResults: Container with samples, MAP estimates, variances, and full chain\n\nNotes\n\nFor ABC: Uses Population Monte Carlo ABC with adaptive epsilon selection\nFor ADVI: Uses Automatic Differentiation Variational Inference via Turing.jl\nParameter dictionary can be customized for each method (see getparamdict_abc())\n\n\n\n\n\n","category":"function"},{"location":"#IntrinsicTimescales.Models.generate_data-Tuple{OneTimescaleAndOscWithMissingModel, Any}","page":"API","title":"IntrinsicTimescales.Models.generate_data","text":"Models.generate_data(model::OneTimescaleAndOscWithMissingModel, theta)\n\nGenerate synthetic data from the Ornstein-Uhlenbeck process with oscillation and apply missing data mask.\n\nArguments\n\nmodel::OneTimescaleAndOscWithMissingModel: Model instance containing simulation parameters\ntheta: Vector containing parameters [tau, freq, coeff]\n\nReturns\n\nSynthetic time series data with oscillations and NaN values at positions specified by model.missing_mask\n\nNotes\n\nGenerates complete OU process data with oscillation\nApplies missing data mask from original data\nReturns data with same missing value pattern as input\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Models.summary_stats-Tuple{OneTimescaleAndOscWithMissingModel, Any}","page":"API","title":"IntrinsicTimescales.Models.summary_stats","text":"Models.summary_stats(model::OneTimescaleAndOscWithMissingModel, data)\n\nCompute summary statistics (ACF or PSD) from time series data with missing values.\n\nArguments\n\nmodel::OneTimescaleAndOscWithMissingModel: Model instance specifying summary statistic type\ndata: Time series data to analyze (may contain NaN)\n\nReturns\n\nFor ACF (summary_method = :acf):\n\nMean autocorrelation function up to n_lags, computed with missing data handling\n\nFor PSD (summary_method = :psd):\n\nMean Lomb-Scargle periodogram within specified frequency range\n\nNotes\n\nACF uses compactime_missing for proper handling of NaN values\nPSD uses Lomb-Scargle periodogram for irregular sampling\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.OneTimescaleAndOscWithMissing.combined_distance-Tuple{OneTimescaleAndOscWithMissingModel, Vararg{Any, 8}}","page":"API","title":"IntrinsicTimescales.OneTimescaleAndOscWithMissing.combined_distance","text":"combined_distance(model::OneTimescaleAndOscWithMissingModel, simulation_summary, data_summary,\n                 weights, distance_method, data_tau, simulation_tau, data_osc, simulation_osc)\n\nCompute combined distance metric between simulated and observed data.\n\nArguments\n\nmodel: OneTimescaleAndOscWithMissingModel instance\nsimulation_summary: Summary statistics from simulation\ndata_summary: Summary statistics from observed data\nweights: Weights for combining distances\ndistance_method: Distance metric type\ndata_tau: Timescale from observed data\nsimulation_tau: Timescale from simulation\ndata_osc: Oscillation frequency from observed data\nsimulation_osc: Oscillation frequency from simulation\n\nReturns\n\nFor ACF:\n\nWeighted combination of ACF distance and timescale distance\n\nFor PSD:\n\nWeighted combination of PSD distance, timescale distance, and oscillation frequency distance\n\n\n\n\n\n","category":"method"},{"location":"#Index","page":"API","title":"Index","text":"","category":"section"},{"location":"","page":"API","title":"API","text":"","category":"page"}]
}
