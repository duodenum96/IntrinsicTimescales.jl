var documenterSearchIndex = {"docs":
[{"location":"fit_result/#Results","page":"Results","title":"Results","text":"","category":"section"},{"location":"fit_result/","page":"Results","title":"Results","text":"The fit function returns a ADVIResults or ABCResults object. ","category":"page"},{"location":"fit_result/#ABCResults","page":"Results","title":"ABCResults","text":"","category":"section"},{"location":"fit_result/","page":"Results","title":"Results","text":"The ABC algorithm returns an ABCResults type containing the full history and final results of the inference process. The type has the following fields:","category":"page"},{"location":"fit_result/","page":"Results","title":"Results","text":"MAP::Vector: Maximum a posteriori estimates of the parameters.\ntheta_history::Vector{Matrix}: History of parameter values across all PMC iterations. Each matrix contains the accepted parameters for that iteration with columns being parameters and rows being samples.\nepsilon_history::Vector: History of acceptance thresholds (epsilon values) used in each iteration.\nacc_rate_history::Vector: History of acceptance rates achieved in each iteration.\nweights_history::Vector{Vector}: History of importance weights for accepted samples in each iteration.\nfinal_theta::Matrix: Final accepted parameter values from the last iteration.\nfinal_weights::Vector: Final importance weights from the last iteration.","category":"page"},{"location":"fit_result/","page":"Results","title":"Results","text":"You can access these fields directly from the results type:","category":"page"},{"location":"fit_result/","page":"Results","title":"Results","text":"results = int_fit(model, param_dict)\n\n# Maximum a posteriori estimates\nmap_estimates = results.MAP\n\n# Access final parameter values\nfinal_params = results.final_theta\n\n# Get acceptance rates across iterations\nacc_rates = results.acc_rate_history\n\n# Get MAP estimates\nmap_estimates = results.MAP","category":"page"},{"location":"fit_result/#ADVIResults","page":"Results","title":"ADVIResults","text":"","category":"section"},{"location":"fit_result/","page":"Results","title":"Results","text":"The ADVI algorithm returns an ADVIResults type containing the inference results. The type has the following fields:","category":"page"},{"location":"fit_result/","page":"Results","title":"Results","text":"samples::AbstractArray: Matrix of posterior samples drawn after fitting. Each row represents a sample and each column represents a parameter.\nMAP::AbstractVector: Maximum a posteriori estimates of the parameters.\nvariational_posterior: The fitted variational posterior distribution containing the full inference results. In Turing.jl, this is obtained via q = vi(model, vi_alg).   ","category":"page"},{"location":"fit_result/","page":"Results","title":"Results","text":"You can access these fields directly from the results object:","category":"page"},{"location":"fit_result/","page":"Results","title":"Results","text":"results = int_fit(model, param_dict)\nposterior_samples = results.samples # Access posterior samples\nmap_estimates = results.MAP # Get MAP estimates\nposterior = results.variational_posterior # Full variational posterior","category":"page"},{"location":"practice/practice_intro/#Practice","page":"Practice","title":"Practice","text":"","category":"section"},{"location":"practice/practice_intro/","page":"Practice","title":"Practice","text":"In the practice section we will build the basic knowledge for timescale estimation. We'll start with building up the autocorrelation function (ACF) and how it relates to INTs.  Then we'll move on to more advanced concepts such as linking the ACF to the power spectrum. If you are familiar with INTs, you can directly jump to the section Model-Free Timescale Estimation and Simulation Based Timescale Estimation. If you are just starting or you want to brush up on the fundamentals, be my guest. ","category":"page"},{"location":"contributing/#contributing","page":"Contributing","title":"Contributing Guidelines","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"First of all, thanks for the interest!","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"We welcome all kinds of contribution, including, but not limited to code, documentation, examples, configuration, issue creating, etc. Every minute of an open source developer's time is considered the most valuable gift: their time and appreciated. ","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Needless to say, be polite and respectful when interacting with strangers while you are contributing. Currently, IntrinsicTimescales.jl is maintained by only one person, it would be great to have other developers / contributers. Therefore please be patient if I can't address your problems in a timely manner. ","category":"page"},{"location":"contributing/#Bug-reports-and-issues","page":"Contributing","title":"Bug reports and issues","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"If you think you found a bug, feel free to open an issue. Focused suggestions and requests can also be opened as issues. Before opening a pull request, start an issue or a discussion on the topic, please.","category":"page"},{"location":"contributing/#Working-on-an-issue","page":"Contributing","title":"Working on an issue","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"If you found an issue that interests you, comment on that issue what your plans are. If the solution to the issue is clear, you can immediately create a pull request. Otherwise, say what your proposed solution is and wait for a discussion around it.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"tip: Tip\nFeel free to ping us after a few days if there are no responses.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"If your solution involves code (or something that requires running the package locally), check the developer documentation. Otherwise, you can use the GitHub interface directly to create your pull request. You can contact me directly at catalyasir@gmail.com if you need any help navigating the codebase. ","category":"page"},{"location":"contributing/#Discussions","page":"Contributing","title":"Discussions","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"IntrinsicTimescales.jl has a discussions page on github which is intended for more relaxed discussions around the package and INTs in general. Feel free to open free-form discussions there. ","category":"page"},{"location":"citations/#Citations","page":"Citations","title":"Citations","text":"","category":"section"},{"location":"citations/","page":"Citations","title":"Citations","text":"The paper for IntrinsicTimescales.jl is currently in review in Journal of Open Source Software. Once it is published, I'll add the reference here for citation. ","category":"page"},{"location":"citations/","page":"Citations","title":"Citations","text":"For specific methods, you can cite the following papers:","category":"page"},{"location":"citations/","page":"Citations","title":"Citations","text":":acw50:\nHoney, C. J., Thesen, T., Donner, T. H., Silbert, L. J., Carlson, C. E., Devinsky, O., Doyle, W. K., Rubin, N., Heeger, D. J., & Hasson, U. (2012). Slow Cortical Dynamics and the Accumulation of Information over Long Timescales. Neuron, 76(2), 423–434. https://doi.org/10.1016/j.neuron.2012.08.011\nWolman, A., Çatal, Y., Wolff, A., Wainio-Theberge, S., Scalabrini, A., Ahmadi, A. E., & Northoff, G. (2023). Intrinsic neural timescales mediate the cognitive bias of self—Temporal integration as key mechanism. NeuroImage, 268, 119896. https://doi.org/10.1016/j.neuroimage.2023.119896\nÇatal, Y., Wolman, A., Buccellato, A., Keskin, K., & Northoff, G. (2025). How Intrinsic Neural Timescales Relate To Event-Related Activity -  Key Role For Intracolumnar Connections (p. 2025.01.10.632350). bioRxiv. https://doi.org/10.1101/2025.01.10.632350\n:acw0:\nGolesorkhi, M., Gomez-Pilar, J., Tumati, S., Fraser, M., & Northoff, G. (2021). Temporal hierarchy of intrinsic neural timescales converges with spatial core-periphery organization. Communications Biology, 4(1), Article 1. https://doi.org/10.1038/s42003-021-01785-z\nWolman, A., Çatal, Y., Wolff, A., Wainio-Theberge, S., Scalabrini, A., Ahmadi, A. E., & Northoff, G. (2023). Intrinsic neural timescales mediate the cognitive bias of self—Temporal integration as key mechanism. NeuroImage, 268, 119896. https://doi.org/10.1016/j.neuroimage.2023.119896\nTang, X., Wang, S., Xu, X., Luo, W., & Zhang, M. (n.d.). Test–retest reliability of resting-state EEG intrinsic neural timescales. Cerebral Cortex. Retrieved June 3, 2025, from https://dx.doi.org/10.1093/cercor/bhaf034","category":"page"},{"location":"citations/","page":"Citations","title":"Citations","text":":acweuler:\nCusinato, R., Alnes, S. L., Maren, E. van, Boccalaro, I., Ledergerber, D., Adamantidis, A., Imbach, L. L., Schindler, K., Baud, M. O., & Tzovara, A. (2023). Intrinsic Neural Timescales in the Temporal Lobe Support an Auditory Processing Hierarchy. Journal of Neuroscience, 43(20), 3696–3707. https://doi.org/10.1523/JNEUROSCI.1941-22.2023\n:auc: \nManea, A. M. G., Maisson, D. J.-N., Voloh, B., Zilverstand, A., Hayden, B., & Zimmermann, J. (2024). Neural timescales reflect behavioral demands in freely moving rhesus macaques. Nature Communications, 15(1), 2151. https://doi.org/10.1038/s41467-024-46488-1\nRaut, R. V., Mitra, A., Marek, S., Ortega, M., Snyder, A. Z., Tanenbaum, A., Laumann, T. O., Dosenbach, N. U. F., & Raichle, M. E. (2020). Organization of Propagated Intrinsic Brain Activity in Individual Humans. Cerebral Cortex, 30(3), 1716–1734. https://doi.org/10.1093/cercor/bhz198\n-Watanabe, T., Rees, G., & Masuda, N. (2019). Atypical intrinsic neural timescale in autism. eLife, 8, e42256. https://doi.org/10.7554/eLife.42256\nWu, K., & Gollo, L. L. (2025). Mapping and modeling age-related changes in intrinsic neural timescales. Communications Biology, 8(1), 1–16. https://doi.org/10.1038/s42003-025-07517-x\n:tau:\nMurray, J. D., Bernacchia, A., Freedman, D. J., Romo, R., Wallis, J. D., Cai, X., Padoa-Schioppa, C., Pasternak, T., Seo, H., Lee, D., & Wang, X.-J. (2014). A hierarchy of intrinsic timescales across primate cortex. Nature Neuroscience, 17(12), Article 12. https://doi.org/10.1038/nn.3862\nIto, T., Hearne, L. J., & Cole, M. W. (2020). A cortical hierarchy of localized and distributed processes revealed via dissociation of task activations, connectivity changes, and intrinsic timescales. NeuroImage, 221, 117141. https://doi.org/10.1016/j.neuroimage.2020.117141\nÇatal, Y., Keskin, K., Wolman, A., Klar, P., Smith, D., & Northoff, G. (2024). Flexibility of intrinsic neural timescales during distinct behavioral states. Communications Biology, 7(1), 1667. https://doi.org/10.1038/s42003-024-07349-1\n:knee:\nGao, R., van den Brink, R. L., Pfeffer, T., & Voytek, B. (2020). Neuronal timescales are functionally dynamic and shaped by cortical microarchitecture. eLife, 9, e61277. https://doi.org/10.7554/eLife.61277\nManea, A. M. G., Maisson, D. J.-N., Voloh, B., Zilverstand, A., Hayden, B., & Zimmermann, J. (2024). Neural timescales reflect behavioral demands in freely moving rhesus macaques. Nature Communications, 15(1), 2151. https://doi.org/10.1038/s41467-024-46488-1\nIf you are using Bayesian methods, cite\nZeraati, R., Engel, T. A., & Levina, A. (2022). A flexible Bayesian framework for unbiased estimation of timescales. Nature Computational Science, 2(3), 193–204. https://doi.org/10.1038/s43588-022-00214-3\nZeraati, R., Shi, Y.-L., Steinmetz, N. A., Gieselmann, M. A., Thiele, A., Moore, T., Levina, A., & Engel, T. A. (2023). Intrinsic timescales in the visual cortex change with selective attention and reflect spatial connectivity. Nature Communications, 14(1), Article 1. https://doi.org/10.1038/s41467-023-37613-7\nIf you are using :abc as fitting method in Bayesian methods, cite:\nBeaumont, M. A., Cornuet, J.-M., Marin, J.-M., & Robert, C. P. (2009). Adaptive approximate Bayesian computation. Biometrika, 96(4), 983–990.\nIf you are using :advi, \nKucukelbir, A., Tran, D., Ranganath, R., Gelman, A., & Blei, D. M. (2017). Automatic Differentiation Variational Inference. Journal of Machine Learning Research, 18(14), 1–45.","category":"page"},{"location":"one_timescale_and_osc_with_missing/#one_timescale_and_osc_with_missing","page":"One Timescale Model with Oscillations and Missing Data","title":"One Timescale and Oscillation with Missing Data (one_timescale_and_osc_with_missing_model)","text":"","category":"section"},{"location":"one_timescale_and_osc_with_missing/","page":"One Timescale Model with Oscillations and Missing Data","title":"One Timescale Model with Oscillations and Missing Data","text":"Uses the same syntax as one_timescale_model and has the same implementation details (i.e. three priors and three results) as one_timescale_and_osc. We refer the users to the respective documentations. The only difference of one_timescale_and_osc_with_missing_model from one_timescale_and_osc is that missing data points is replaced with NaNs in the generative model, as in one_timescale_with_missing_model. ","category":"page"},{"location":"one_timescale_and_osc/#one_timescale_and_osc","page":"One Timescale Model with Oscillations","title":"One Timescale and Oscillation Model (one_timescale_and_osc_model)","text":"","category":"section"},{"location":"one_timescale_and_osc/","page":"One Timescale Model with Oscillations","title":"One Timescale Model with Oscillations","text":"Uses the same syntax as one_timescale_model. We refer the user to the documentation of one_timescale_model for details and point out the differences here. ","category":"page"},{"location":"one_timescale_and_osc/","page":"One Timescale Model with Oscillations","title":"One Timescale Model with Oscillations","text":"The generative model: ","category":"page"},{"location":"one_timescale_and_osc/","page":"One Timescale Model with Oscillations","title":"One Timescale Model with Oscillations","text":"fracdydt = -fracytau + xi(t) \n\n\nx(t) = sqrtay(t) + sqrt1-a sin(2 pi f t + phi)","category":"page"},{"location":"one_timescale_and_osc/","page":"One Timescale Model with Oscillations","title":"One Timescale Model with Oscillations","text":"where f is the frequency, a is the weight of the Ornstein-Uhlenbeck (OU) process and $ \\phi $ is a random number drawn from a normal distribution to reflect a random phase offset for each trial. Note that now we need to fit three parameters: $ \\tau $ for timescale, f for the oscillation frequency and $ a $ for how strong the oscillations are (with a smaller a indicating larger oscillations). Note that a is bounded between 0 and 1. Similarly, the maximum a posteriori estimates (MAP) also has three elements: one for each prior. Due to the three parameters needed, the fitting is more difficult compared to  one_timescale_model. ","category":"page"},{"location":"one_timescale_and_osc/","page":"One Timescale Model with Oscillations","title":"One Timescale Model with Oscillations","text":"If the user wishes to set the priors, they need to specify a prior for each of the parameters. The ordering is 1) the prior for timescale, 2) the prior for frequency second and 3) the prior for the coefficient. An example:","category":"page"},{"location":"one_timescale_and_osc/","page":"One Timescale Model with Oscillations","title":"One Timescale Model with Oscillations","text":"using Distributions, IntrinsicTimescales\n\ndata_mean = 0.0 # desired mean\ndata_sd = 1.0 # desired sd\nduration = 10.0 # duration of data\nn_trials = 10 # How many trials\nfs = 500.0 # Sampling rate\ntimescale = 0.05 # 50 ms\noscillation_freq = 10.0 # 10 Hz alpha oscillation\ncoefficient = 0.95\ntheta = [timescale, oscillation_freq, coefficient] # vector of parameters\n\ndata = generate_ou_with_oscillation(theta, 1/fs, duration, n_trials, data_mean, data_sd)\n\n\npriors = [\n        Normal(0.1, 0.1),    # a prior for a 0.1 second timescale with an uncertainty of 0.1\n        Normal(10.0, 5.0),   # 10 Hz frequency with uncertainty of 5 Hz\n        Uniform(0.0, 1.0)    # Uniform distribution for coefficient\n    ]\n\ntime = (1/fs):(1/fs):duration\nmodel = one_timescale_and_osc_model(data, time, :abc, summary_method=:acf, prior=priors)\nresults = int_fit(model)\nint = results.MAP[1]  # max a posterori for INT\nfreq = results.MAP[2] # for frequency\ncoef = results.MAP[3] # for coefficient","category":"page"},{"location":"one_timescale_and_osc/","page":"One Timescale Model with Oscillations","title":"One Timescale Model with Oscillations","text":"If the user does not specify a prior or sets prior=\"informed_prior\", IntrinsicTimescales.jl generates priors from data. The prior for the coefficient in this case is Uniform(0.0, 1.0). For summary_method=:acf, the timescale prior is an exponential decay fit to the ACF from data whereas summary_method=:psd fits a Lorentzian function to the PSD from data, obtains the knee frequency and estimates the timescale from it as in one_timescale_model. The prior for the frequency is obtained with first fitting a Lorentzian to the PSD, then subtracting the lorentzian to eliminate aperiodic component as in [FOOOF] and finally obtains the peak frequency with find_oscillation_peak. ","category":"page"},{"location":"one_timescale_and_osc/","page":"One Timescale Model with Oscillations","title":"One Timescale Model with Oscillations","text":"Similarly, the argument combine_distance=true not only calculates the RMSE between PSDs or ACFs, but also combines that distance with the RMSE between timescale and frequency estimates between the model and data. ","category":"page"},{"location":"one_timescale_and_osc/","page":"One Timescale Model with Oscillations","title":"One Timescale Model with Oscillations","text":"The other arguments are the same as one_timescale_model. We refer the reader to that section of the documentation for details. ","category":"page"},{"location":"theory/th1_phenomena/#Explaining-the-Phenomenon","page":"Explaining the Phenomenon","title":"Explaining the Phenomenon","text":"","category":"section"},{"location":"theory/th1_phenomena/","page":"Explaining the Phenomenon","title":"Explaining the Phenomenon","text":"While there were some earlier papers about this, to my knowledge, the landmark starting point about the modern study of intrinsic neural timescales (INTs) is Hasson et al.'s 2008 paper, published in Journal of Neuroscience, titled A Hierarchy of Temporal Receptive Windows in Human Cortex. The experimental setup is participants watching movies. The movies are scrambled with durations of 4 (±1), 12 (±3) and 36 (±6) seconds. ","category":"page"},{"location":"home/#IntrinsicTimescales.jl-Documentation","page":"Getting Started","title":"IntrinsicTimescales.jl Documentation","text":"","category":"section"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"Welcome to the documentation of IntrinsicTimescales.jl. IntrinsicTimescales.jl is a software package for estimating Intrinsic Neural Timescales (INTs) from time-series data. It uses model-free methods (ACW-50, ACW-0, fitting an exponential decay function etc.) and simulation-based methods (adaptive approximate Bayesian computation: aABC, currently experimental automatic differentiation variational inference: ADVI) to estimate INTs.","category":"page"},{"location":"home/#Installation","page":"Getting Started","title":"Installation","text":"","category":"section"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"This package is written in Julia. If you do not have Julia installed, you can install it from here. Once you have Julia installed, you can install IntrinsicTimescales.jl by running the following command in the Julia REPL:","category":"page"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"using Pkg\nPkg.add(\"IntrinsicTimescales\")","category":"page"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"Soon, there will also be a Python wrapper called INTpy, which will allow you to use IntrinsicTimescales.jl from Python. ","category":"page"},{"location":"home/#Quickstart","page":"Getting Started","title":"Quickstart","text":"","category":"section"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"IntrinsicTimescales.jl uses two ways to estimate INTs: model-free methods and simulation-based inference. Model-free methods include ACW-50, ACW-0, ACW-e, decay rate of an exponential fit to ACF and knee freqency of a lorentzian fit to PSD. Simulation-based methods are based on Zeraati et al. (2022) paper and do parameter estimation by assuming the data came from an Ornstein-Uhlenbeck process. For estimation, in addition to the aABC method used in Zeraati et al. (2022), we also present ADVI. ","category":"page"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"For model-free methods, simply use ","category":"page"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"using IntrinsicTimescales\n\ndata = randn(10, 5000) # Data in the form of (trials x time) \nfs = 100.0 # Sampling frequency\n\nacwresults = acw(data, fs; acwtypes = [:acw0, :acw50, :acweuler, :auc, :tau, :knee], dims=ndims(data))\n# or even simpler:\nacwresults = acw(data, fs)","category":"page"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"where fs is sampling frequency, optional parameters acwtypes is a vector of  symbols (indicated with :) telling which methods to use and dims is indicating the dimension of time in your array (by default, the last dimension). The resulting acwresults gives the results in the same order of acwtypes. ","category":"page"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"For simulation based methods, pick one of the one_timescale_model, one_timescale_with_missing_model, one_timescale_and_osc_model and one_timescale_and_osc_with_missing_model functions. These models correspond to different generative models depending on whether there is an oscillation or not. For each generative model, there are with or without missing variants which use different ways to calculate ACF and PSD. Once you pick the model, the syntax is ","category":"page"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"# Simulate some data\nusing Random\ntimescale = 0.3 # true timescales\nvariance = 1.0 # variance of data\nduration = 10.0 # duration of data\nn_trials = 2 # How many trials\nfs = 500.0 # Sampling rate\nrng = Xoshiro(123); deq_seed = 123 # For reproducibility\ndata = generate_ou_process(timescale, variance, 1/fs, duration, n_trials, rng=rng, deq_seed=deq_seed) # Data in the form of (trials x time)\ntime = (1/fs):(1/fs):duration # Vector of time points\nmodel = one_timescale_model(data, time, :abc)\nresult = int_fit(model)","category":"page"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"or ","category":"page"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"model = one_timescale_model(data, time, :advi)\nresult = int_fit(model)","category":"page"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"These functions are highly customizable, see the page Simulation Based Timescale Estimation. ","category":"page"},{"location":"home/#Organization-of-the-package","page":"Getting Started","title":"Organization of the package","text":"","category":"section"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"The diagram below shows the rough organization of the package:","category":"page"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"(Image: )","category":"page"},{"location":"home/#Where-to-go-from-here?","page":"Getting Started","title":"Where to go from here?","text":"","category":"section"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"This documentation is divided into three parts. Explanation part is divided into theory and practice sections. The first section is Practice. It is usually easier to understand something after you do it, therefore, I placed the practice section before theory. In Practice, we carefully build our way towards estimating INTs by starting from the autocorrelation function and slowly proceeding to more and more advanced methods. If you never calculated INTs before, this is where you should start. The second part is Theory. This part aims to delve into the history of INT research, what it means in the brain and what it is good for with a particular emphasis on theoretical research, summarizing the cutting edge in this frontier. It is especially useful for researchers working on INT itself. Right now, it is in construction. I will deploy it as soon as it is ready. ","category":"page"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"The third part is the Reference. This part contains the Implementation and API. The API is an exhaustive list of functions and their signatures in the package. It is boring and most of the functions are not intended for end-user (you). The implementation part documents model-free and simulation-based methods that are used in the package, with the full function signatures. This part should serve as the reference for the user. If you are already familiar with INTs and want to see how to use this package, you can start here. ","category":"page"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"Finally, the Tutorial contains practical considerations for picking which INT metric to use in which situation, as well as using the package with MNE-Python and with FieldTrip packages. ","category":"page"},{"location":"home/#Getting-Help-and-Making-Contributions","page":"Getting Started","title":"Getting Help and Making Contributions","text":"","category":"section"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"Questions and contributions are welcome. Use the issues section of our github page to report bugs and make feature requests and ask questions. Please see Contributing Guidelines before contributing. ","category":"page"},{"location":"home/#Statement-of-Need","page":"Getting Started","title":"Statement of Need","text":"","category":"section"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"Intrinsic neural timescales (INTs) were found to be an important metric to probe the brain dynamics and function. On the neuroscientific side, INTs were found to follow the large-scale gradients in the cortex ranging from uni to transmodal areas including local and long-range excitation and proxies of myelination. From a cognitive science perspective, INTs were found to be related to reward, behavior, self, consciousness among others. Proper estimation of INTs to make sure the estimates are not affected by limited data, missingness of the data and oscillatory artifacts is crucial. While several methods exist for estimating INTs, there is a lack of standardized, open-source tools that implement both traditional model-free approaches and modern Bayesian estimation techniques. Existing software solutions are often limited to specific estimation methods, lack proper uncertainty quantification, or are not optimized for large-scale neuroimaging data.","category":"page"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"IntrinsicTimescales.jl addresses these limitations by providing a comprehensive, high-performance toolbox for INT estimation. The package implements both established model-free methods and novel Bayesian approaches, allowing researchers to compare and validate results across different methodologies with a simple API. Its implementation in Julia ensures computational efficiency, crucial for analyzing large neuroimaging datasets. The package's modular design facilitates easy extension and integration with existing neuroimaging workflows, while its rigorous testing and documentation make it accessible to researchers across different levels of programming expertise.","category":"page"},{"location":"home/#About","page":"Getting Started","title":"About","text":"","category":"section"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"This package is developed by me, Yasir Çatal during my PhD. I got nerdsniped by Zeraati et al., 2021 paper and started writing the package. the rest evolved from the simple motivation of reimplementing abcTau in Julia with various performance optimizations. ","category":"page"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"I am doing my PhD on INTs and our lab is specialized on the topic. As a result, I had many conversations with almost every member of our lab about INTs. I designed this documentation while keeping those conversations in mind. My goal was not only to document the package, but also to build up the knowledge to grasp the concept of INTs especially for new researchers starting their journey and active researchers in the trenches if they wish to brush up their basics. ","category":"page"},{"location":"home/#Citations","page":"Getting Started","title":"Citations","text":"","category":"section"},{"location":"home/","page":"Getting Started","title":"Getting Started","text":"See Citations to see the papers you can cite specific to methods. ","category":"page"},{"location":"tutorial/tutorial_1_acw/#Navigating-the-Forest-of-INT-Metrics","page":"Navigating the Forest of INT Metrics","title":"Navigating the Forest of INT Metrics","text":"","category":"section"},{"location":"tutorial/tutorial_1_acw/","page":"Navigating the Forest of INT Metrics","title":"Navigating the Forest of INT Metrics","text":"The main user-facing function in IntrinsicTimescales.jl is acw. For reference, see the section Model-Free Timescale Estimation. acw offers quite a number of different ways to estimate timescales. The situation might be confusing for newcomers and even seasoned researchers. INTs had been a part of my life for the last 5 years and even I occasionally find myself unsure on what to do. As experimental researchers, we do not have the luxury of a ground truth that we can use to validate our estimations. If we had, we would not need to estimate anything in the first place. The best we can do is to compare our results to the simulations where the ground truth is known, literature and theory. I believe that experimental researchers are entitled to look at computational researchers and package developers and ask them \"Okay, this is all well and good but what I should do?\". The problem is, there is almost never a gold standard / best practice that we can use and get the best results. Therefore I opted to avoid recommending one. Nonetheless, there are certain rules of thumb and certain things to not do. The purpose of this section is to talk about them. ","category":"page"},{"location":"tutorial/tutorial_1_acw/","page":"Navigating the Forest of INT Metrics","title":"Navigating the Forest of INT Metrics","text":"I should note that this section will not go into detailed discussions about each metric. This is done in the Practice section. If you are not familiar with the INTs, I would highly recommend you to go through it to build your intuition slowly. The timescales of understanding are unfortunately slow :upsidedownface:. ","category":"page"},{"location":"tutorial/tutorial_1_acw/","page":"Navigating the Forest of INT Metrics","title":"Navigating the Forest of INT Metrics","text":"In the function acw, one can set the different methods to use as simply:","category":"page"},{"location":"tutorial/tutorial_1_acw/","page":"Navigating the Forest of INT Metrics","title":"Navigating the Forest of INT Metrics","text":"acwresults = acw(data, fs; acwtypes=[:acw0, :acw50, :acweuler, :auc, :tau, :knee])","category":"page"},{"location":"tutorial/tutorial_1_acw/","page":"Navigating the Forest of INT Metrics","title":"Navigating the Forest of INT Metrics","text":"Different metrics specified in acwtypes is automatically calculated and returned in acwresults. Below, I'll try to show you the mental workflow on deciding which metric to use. ","category":"page"},{"location":"tutorial/tutorial_1_acw/#Getting-ACW-0-Out-of-the-Way:","page":"Navigating the Forest of INT Metrics","title":"Getting ACW-0 Out of the Way:","text":"","category":"section"},{"location":"tutorial/tutorial_1_acw/","page":"Navigating the Forest of INT Metrics","title":"Navigating the Forest of INT Metrics","text":"Never use :acw0 in serious research. I considered to not include it in the final package but I know that some people will ask for it anyway. So I decided to put it here but warn against it. There are serious problems with ACW-0. Consider when an exponential decay function f(x) = e^lambda x reaches zero. There are two cases here. In the first case lambda is a real number. This is the pure exponential decay function without oscillations we usually encounter in fMRI or calcium imaging (after averaging over channels). In this case, f(x) never touches zero. You might protest this suggestion by saying \"but look, in my data I plot the ACF and it clearly touches zero\". This is due to finite data bias. In all data, you have this bias and ACW-0 amplifies it. Now let's consider the second case where lambda is complex. This is the case we deal with usually in EEG / MEG data with oscillatorions. Let lambda = a + bi  then by Eulers formula we have  f(x) = e^ax e^ibx = e^ax (cos(bx) + i sin (bx) ). Since our data is real valued, the actual autocorrelation function is the real part of this. Here, the real part of lambda, mathscrR(lambda) = a is the decay rate of the ACF and therefore, the timescale. Now we can ask ourselves when the ACF touches zero. Solving cos(bx) = 0 for x, we will get x = fracpi2b. Clearly ACW-0 here does not show the timescale, it just shows the oscillatory artifact. If you are interested in oscillations, that is perfectly fine but there are way better tools for that, such as the Fourier transform. This is the case when you have long data. In the case of smaller data, finite data bias might beat oscillatory artifacts. Now there is one more case I haven't talked about: multiple oscillations in ACF. But it is clear that that will suffer from the same problem. I used to think ACW-0 is justified in data with very low sampling rate, such as fMRI, but area under the curve (:auc) and estimating the decay rate directly (:tau) especially with the option skip_first_lag = true are much better methods. Seriously, I can tell you horror stories about ACW-0. Stay away from it if you don't know what you are doing. ","category":"page"},{"location":"tutorial/tutorial_1_acw/#ACF-vs-PSD-based-methods","page":"Navigating the Forest of INT Metrics","title":"ACF vs PSD based methods","text":"","category":"section"},{"location":"tutorial/tutorial_1_acw/","page":"Navigating the Forest of INT Metrics","title":"Navigating the Forest of INT Metrics","text":"With that out of the way, we face three branches: 1) ACF based methods which do not involve fitting (:acw50, :acweuler, :auc), 2) ACF based methods which do involve fitting (:tau) and 3) PSD based methods (:knee). I would recommend looking at the PSDs and ACFs to decide whether to use ACF or PSD based methods. Here is a quick example:","category":"page"},{"location":"tutorial/tutorial_1_acw/","page":"Navigating the Forest of INT Metrics","title":"Navigating the Forest of INT Metrics","text":"using IntrinsicTimescales\nusing Plots\nusing Random\n\ntau = 0.5\nf = 10.0\noscillation_coefficient = 0.9\nduration = 10.0\ndt = 1 / 1000.0\ndata_mean = 0.0\ndata_sd = 1.0\nn_trials = 30\n\n# For reproducibility\nseed = 666\nrng = Xoshiro(seed)\n\ndata = generate_ou_with_oscillation([tau, f, oscillation_coefficient], dt, duration, n_trials, data_mean, data_sd, rng=rng, deq_seed=seed)\n\nacwresults = acw(data, 1 / dt; acwtypes=[:acweuler, :knee], average_over_trials=true)\np = acwplot(acwresults)\ntitle!(p[1], \"Big oscillation, timescale from ACF: $(round(acwresults.acw_results[1], digits=3))\")\ntitle!(p[2], \"timescale from PSD: $(round(acwresults.acw_results[2], digits=3))\")\n\noscillation_coefficient_2 = 0.99\ndata_2 = generate_ou_with_oscillation([tau, f, oscillation_coefficient_2], dt, duration, n_trials, data_mean, data_sd, rng=rng, deq_seed=seed)\nacwresults_2 = acw(data_2, 1 / dt; acwtypes=[:acweuler, :knee], average_over_trials=true)\np2 = acwplot(acwresults_2)\ntitle!(p2[1], \"Small oscillation, timescale from ACF: $(round(acwresults_2.acw_results[1], digits=3))\")\ntitle!(p2[2], \"timescale from PSD: $(round(acwresults_2.acw_results[2], digits=3))\")\n\n\ndata_3 = generate_ou_process(tau, data_sd, dt, duration, n_trials, rng=rng, deq_seed=seed)\nacwresults_3 = acw(data_3, 1 / dt; acwtypes=[:acweuler, :knee], average_over_trials=true)\np3 = acwplot(acwresults_3)\ntitle!(p3[1], \"No oscillation, timescale from ACF: $(round(acwresults_3.acw_results[1], digits=3))\")\ntitle!(p3[2], \"timescale from PSD: $(round(acwresults_3.acw_results[2], digits=3))\")\n\nplot(p, p2, p3, layout=(3, 1), size=(900, 600))\n","category":"page"},{"location":"tutorial/tutorial_1_acw/","page":"Navigating the Forest of INT Metrics","title":"Navigating the Forest of INT Metrics","text":"(Image: ACW Plots)","category":"page"},{"location":"tutorial/tutorial_1_acw/","page":"Navigating the Forest of INT Metrics","title":"Navigating the Forest of INT Metrics","text":"We can see that in the case of strong oscillations, the PSD based method (:knee) performs better. In the cases of no oscillation (which is usually the case in fMRI data), it doesn't matter which metric to use. This gives the impression that we should always use :knee no matter the scenario. The simulations give this impression but empirical data can be tricky. I've had cases where the PSD based method failed to converge or gave negative timescales. My advice would be to look at the PSDs and see if the estimated knee frequency (indicated with a vertical line) looks okay. ","category":"page"},{"location":"tutorial/tutorial_1_acw/","page":"Navigating the Forest of INT Metrics","title":"Navigating the Forest of INT Metrics","text":"The bottom line is that there is no one-size-fits-all method. It all depends on the data. A good practice is to use multiple metrics and see if they both give the same empirical result and when writing your paper, reporting one of them in the supplementary material. Reviewers these days often ask for this anyway. One advantage of IntrinsicTimescales.jl is that it makes it very easy to do that. ","category":"page"},{"location":"tutorial/tutorial_1_acw/","page":"Navigating the Forest of INT Metrics","title":"Navigating the Forest of INT Metrics","text":"Then on the question of :acweuler and :acw50: for the most part it doesn't really matter. In my experience they almost always very highly correlate, to the degree that they are interchangable. Note that if the ACF fits an exponential decay function very well, :acweuler directly gives the inverse decay rate of ACF whereas :acw50 gives the inverse decay rate up to a constant (which happens to be log2). ","category":"page"},{"location":"tutorial/tutorial_1_acw/#:tau-vs-:acw*","page":"Navigating the Forest of INT Metrics","title":":tau vs :acw*","text":"","category":"section"},{"location":"tutorial/tutorial_1_acw/","page":"Navigating the Forest of INT Metrics","title":"Navigating the Forest of INT Metrics","text":"In the case where sampling rate is very low (fMRI data, certain calcium imaging data) :tau works much better. The reason is that in the case where the decay rate of ACF is faster than the sampling rate, the temporal resolution of ACF (which has the same temporal resolution as your time-series data) can't catch up with the decay and in practice you end up in every ROI / subject showing an ACW of 1-2 lags (with occasional rare exception of 4-5 lags). In the case of super-low sampling rate (fMRI) I can also recommend setting the option skip_zero_lag=true. ","category":"page"},{"location":"tutorial/tutorial_1_acw/#When-to-use-:auc?","page":"Navigating the Forest of INT Metrics","title":"When to use :auc?","text":"","category":"section"},{"location":"tutorial/tutorial_1_acw/","page":"Navigating the Forest of INT Metrics","title":"Navigating the Forest of INT Metrics","text":"For low sampling rates, for example, fMRI. Regarding :tau versus :auc, I prefer :tau with skip_zero_lag=true but that's my personal preference. I'm writing up a short piece on this which I'll put to this documentation soon. You can see two very good papers using these approaches in Ito et al., 2020 (using :tau with skip_zero_lag=true) and Manea et al., 2022 (using :auc). ","category":"page"},{"location":"tutorial/tutorial_1_acw/#Should-I-average_over_trials?","page":"Navigating the Forest of INT Metrics","title":"Should I average_over_trials?","text":"","category":"section"},{"location":"tutorial/tutorial_1_acw/","page":"Navigating the Forest of INT Metrics","title":"Navigating the Forest of INT Metrics","text":"In general, yes. The option average_over_trials averages PSDs and/or ACFs across trials. In my experience, averaging ACFs or PSDs and then estimating the timescale (which is what average_over_trials does) works much better than estimating one timescale from each trial and averaging over them. If you suspect that there might be a nonstationarity in your timescales (i.e. your timescales change over time), you can investigate timescales in each trial by setting average_over_trials off. Needless to say, this depends on having trials, which means you should not use this if you have, say, one long fMRI recording. ","category":"page"},{"location":"tutorial/tutorial_1_acw/#Should-I-skip_zero_lag?","page":"Navigating the Forest of INT Metrics","title":"Should I skip_zero_lag?","text":"","category":"section"},{"location":"tutorial/tutorial_1_acw/","page":"Navigating the Forest of INT Metrics","title":"Navigating the Forest of INT Metrics","text":"Yes for fMRI, no for EEG / MEG. ","category":"page"},{"location":"tutorial/tutorial_1_acw/#Should-I-allow_variable_exponent?","page":"Navigating the Forest of INT Metrics","title":"Should I allow_variable_exponent?","text":"","category":"section"},{"location":"tutorial/tutorial_1_acw/","page":"Navigating the Forest of INT Metrics","title":"Navigating the Forest of INT Metrics","text":"This is an engineering problem. If we allow variable exponent, we are adding one more parameter to estimate which makes the fitting more difficult and in practice, most EEG / MEG spectra have a power law exponent of approximately 2. But in the cases where this is not correct, it is necessary to allow variable exponent. My recommendation is to plot the result using acwplot and see if the knee frequency is correctly estimated (check vertical line) visually. In the next version of IntrinsicTimescales.jl (v0.6.0), I'll add more diagnostic plotting capabilities which will make this process much smoother. Meanwhile also consider using the python package FOOOF which is not a general INT toolbox but can estimate INTs using the knee frequency method. In fact, the :knee option in IntrinsicTimescales.jl is trying to mimic FOOOF package's behavior, with slight differences (e.g. using SciML environment as opposed to SciPy). ","category":"page"},{"location":"tutorial/tutorial_1_acw/#Should-I-set-constrainedtrue?","page":"Navigating the Forest of INT Metrics","title":"Should I set constrained=true?","text":"","category":"section"},{"location":"tutorial/tutorial_1_acw/","page":"Navigating the Forest of INT Metrics","title":"Navigating the Forest of INT Metrics","text":"In certain cases it might be beneficial to set constrained to true for :knee. This effectively sets bounds on the values each parameter can take. But in my experience, NonlinearSolve.jl (which is the engine behind constrained=false) performs better than Optimization.jl (which is used when constrained=true) (note that this is entirely anecdotal). Furthermore, if the fitting is problematic and gives unreasonable results (such as negative timescales), I would first look at the PSDs to see if they are indeed Lorentzian shaped (i.e. in the log-log space they start out as a flat line and then decay with a slope of -2). Most of the time, computational problems are actually model problems, in the sense of using the wrong model (see Folk Theorem of Statistical Computing). ","category":"page"},{"location":"tutorial/tutorial_1_acw/","page":"Navigating the Forest of INT Metrics","title":"Navigating the Forest of INT Metrics","text":"Some practical advices in this case: 1) average_over_trials if you haven't done so. This results in a smoother PSD which is more amenable to fitting. 2) Check your lower frequency limit in the PSD. Remember that your PSD should look like a Lorentzian, the low frequency section should be flat. If the full PSD is just a line (i.e. scale-free), :knee can not be used. 3) Check the length of each of your trials. Let's assume that during the preprocessing stage you applied a high-pass filter of 1 Hz. As a rule of thumb, you should contain at least 3 cycles of the lowest frequency in your data for each trial. This means that you should have at least 3 seconds of data in each trial. Nonetheless, it will still be quite noisy. I suggest at least 10 seconds per trial. If your timescale is at, say, 3 Hz and your data is only 1 second, you will barely see the knee frequency in your PSD and wrongly conclude that your data is scale-free. ","category":"page"},{"location":"tutorial/tutorial_1_acw/#Should-I-use-paralleltrue?","page":"Navigating the Forest of INT Metrics","title":"Should I use parallel=true?","text":"","category":"section"},{"location":"tutorial/tutorial_1_acw/","page":"Navigating the Forest of INT Metrics","title":"Navigating the Forest of INT Metrics","text":"Depends on the size of your data. If you have a multi-core machine, it might speed up the computation significantly. But if your data is small, the overhead of parallelization might actually slow down the computation. You can see the number of threads by running Threads.nthreads(). If it is 1, you are not using parallelization. To start Julia with multiple threads (say, 4), you can either run julia -t 4 or set the environment variable JULIA_NUM_THREADS=4. In VsCode, you can set the environment variable by going to File -> Preferences -> Settings -> Julia: Num Threads and setting to the number of threads you want to use. ","category":"page"},{"location":"tutorial/tutorial_1_acw/#When-to-use-Bayesian-methods?","page":"Navigating the Forest of INT Metrics","title":"When to use Bayesian methods?","text":"","category":"section"},{"location":"tutorial/tutorial_1_acw/","page":"Navigating the Forest of INT Metrics","title":"Navigating the Forest of INT Metrics","text":"Bayesian methods are very novel and experimental. They can work terrificly good especially in the case of short data and missing samples. But they come at a cost of significantly increased computation time. I would consider them in cutting-edge research, for example, an experimental scenario where you have to compare timescales of two different conditions with different trial lengths (standard ACF / PSD based methods can be sensitive to data length). In any case, don't forget to do posterior predictive check if you are using Bayesian methods. ","category":"page"},{"location":"tutorial/tutorial_1_acw/#Can-I-calculate-INTs-in-task-trials-(ERP/ERF/ASSR-and-so-on)?","page":"Navigating the Forest of INT Metrics","title":"Can I calculate INTs in task trials (ERP/ERF/ASSR and so on)?","text":"","category":"section"},{"location":"tutorial/tutorial_1_acw/","page":"Navigating the Forest of INT Metrics","title":"Navigating the Forest of INT Metrics","text":"No. Both ACF and PSD assume stationarity. Task trials are nonstationary (i.e. the signal's mean value is changing over time). I would recommend calculating INTs only in resting state or continuous tasks such as listening to a story or some continuous behavioral tasks (see Manea et al., 2022 or Çatal et al., 2024). ","category":"page"},{"location":"tutorial/tutorial_1_acw/#TL;DR","page":"Navigating the Forest of INT Metrics","title":"TL;DR","text":"","category":"section"},{"location":"tutorial/tutorial_1_acw/","page":"Navigating the Forest of INT Metrics","title":"Navigating the Forest of INT Metrics","text":"Don't use ACW-0 for anything serious.\nThere is no one-size-fits-all method. Experiment with different measures and most importantly, look at your data. \nIf there are strong oscillatory components in your PSD, consider using :knee.\nIf your sampling rate is very low (fMRI), consider using :tau with skip_zero_lag or :auc.\nIf you have trials, average_over_trials. \nIf your knee frequencies are at the wrong place after visual inspection, consider using allow_variable_exponent. \nLook at your PSD before setting constrained=true. \nIf you are considering using Bayesian methods, make sure to check the posteriors. \nCalculate INTs only in stationary data. ","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"In construction","category":"page"},{"location":"tutorial/tutorial_2_mne/#Usage-with-MNE-Python","page":"Usage with MNE-Python","title":"Usage with MNE-Python","text":"","category":"section"},{"location":"tutorial/tutorial_2_mne/","page":"Usage with MNE-Python","title":"Usage with MNE-Python","text":"In this tutorial, we will give an example of using MNE-Python with IntrinsicTimescales.jl. ","category":"page"},{"location":"tutorial/tutorial_2_mne/","page":"Usage with MNE-Python","title":"Usage with MNE-Python","text":"We will start with downloading an example resting state dataset. The details about the dataset can be found here. We won't detail the preprocessing here, MNE (as well as other packages) already have excellent tutorials on it. ","category":"page"},{"location":"tutorial/tutorial_2_mne/","page":"Usage with MNE-Python","title":"Usage with MNE-Python","text":"The first part is in python. Let's download the data and read it. ","category":"page"},{"location":"tutorial/tutorial_2_mne/","page":"Usage with MNE-Python","title":"Usage with MNE-Python","text":"import mne\nimport mne\nimport os.path as op\nimport numpy as np\n\n# Download example data and specify the path\ndata_path = mne.datasets.brainstorm.bst_resting.data_path()\nraw_fname = op.join(data_path, \"MEG\", \"bst_resting\", \"subj002_spontaneous_20111102_01_AUX.ds\")\n\n# Read data\nraw = mne.io.read_raw_ctf(raw_fname)","category":"page"},{"location":"tutorial/tutorial_2_mne/","page":"Usage with MNE-Python","title":"Usage with MNE-Python","text":"We will epoch the data into 10 second epochs, from each of these epochs, we will compute one ACF/PSD. Then, we'll put the data into a numpy array.","category":"page"},{"location":"tutorial/tutorial_2_mne/","page":"Usage with MNE-Python","title":"Usage with MNE-Python","text":"epochs = mne.make_fixed_length_epochs(raw, 10)\ndata = epochs.get_data()","category":"page"},{"location":"tutorial/tutorial_2_mne/","page":"Usage with MNE-Python","title":"Usage with MNE-Python","text":"Finally, we will save the data to computer so that we can read it in Julia. We'll also note the sampling rate.","category":"page"},{"location":"tutorial/tutorial_2_mne/","page":"Usage with MNE-Python","title":"Usage with MNE-Python","text":"save_path = <path to save the data>\nnp.save(op.join(save_path, \"data.npy\"), data)\nsampling_rate = raw.info['sfreq']\n# 2400.0","category":"page"},{"location":"tutorial/tutorial_2_mne/","page":"Usage with MNE-Python","title":"Usage with MNE-Python","text":"Now we will switch to Julia. To read .npy files, we can use the npzread function from the NPZ package. Let's note the size of the data.","category":"page"},{"location":"tutorial/tutorial_2_mne/","page":"Usage with MNE-Python","title":"Usage with MNE-Python","text":"using IntrinsicTimescales\nusing NPZ\n\nfs = 2400.0 # Sampling rate\n\ndata_path = <path to the data>\ndata = npzread(joinpath(data_path, \"data.npy\"))\nprintln(size(data))","category":"page"},{"location":"tutorial/tutorial_2_mne/","page":"Usage with MNE-Python","title":"Usage with MNE-Python","text":"The data is in the shape of (nepochs, nchannels, n_samples). Let's compute the INT metrics using the acw function. We'll average over the trials dimension (epochs in the MNE jargon) and use parallel processing.","category":"page"},{"location":"tutorial/tutorial_2_mne/","page":"Usage with MNE-Python","title":"Usage with MNE-Python","text":"results = acw(data, fs, acwtypes=[:acw50, :tau], dims=3, trial_dims=1, parallel=true, average_over_trials=true)","category":"page"},{"location":"tutorial/tutorial_2_mne/","page":"Usage with MNE-Python","title":"Usage with MNE-Python","text":"That's it. Note that you can also use PythonCall.jl to use MNE inside Julia. ","category":"page"},{"location":"developer/#Developer-documentation","page":"Developer Documentation","title":"Developer documentation","text":"","category":"section"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"note: Contributing guidelines\nIf you haven't, please read the Contributing guidelines first.","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"If you want to make contributions to this package that involves code, then this guide is for you.","category":"page"},{"location":"developer/#First-time-clone","page":"Developer Documentation","title":"First time clone","text":"","category":"section"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"tip: If you have writing rights\nIf you have writing rights, you don't have to fork. Instead, simply clone and skip ahead. Whenever upstream is mentioned, use origin instead.","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"If this is the first time you work with this repository, follow the instructions below to clone the repository.","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"Fork this repo\nClone your repo (this will create a git remote called origin)\nAdd this repo as a remote:\ngit remote add upstream https://github.com/your_username/IntrinsicTimescales.jl","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"This will ensure that you have two remotes in your git: origin and upstream. You will create branches and push to origin, and you will fetch and update your local main branch from upstream.","category":"page"},{"location":"developer/#Testing","page":"Developer Documentation","title":"Testing","text":"","category":"section"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"As with most Julia packages, you can just open Julia in the repository folder, activate the environment, and run test:","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"julia> # press ]\npkg> activate .\npkg> test","category":"page"},{"location":"developer/#Branch-naming","page":"Developer Documentation","title":"Branch naming","text":"","category":"section"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"If there is an associated issue, add the issue number.\nIf there is no associated issue, and the changes are small, add a prefix such as \"typo\", \"hotfix\", \"small-refactor\", according to the type of update.\nIf the changes are not small and there is no associated issue, then create the issue first, so we can properly discuss the changes.\nUse dash separated imperative wording related to the issue (e.g., 14-add-tests, 15-fix-model, 16-remove-obsolete-files).","category":"page"},{"location":"developer/#Commit-message","page":"Developer Documentation","title":"Commit message","text":"","category":"section"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"Use imperative or present tense, for instance: Add feature or Fix bug.\nHave informative titles.\nWhen necessary, add a body with details.\nIf there are breaking changes, add the information to the commit message.","category":"page"},{"location":"developer/#Before-creating-a-pull-request","page":"Developer Documentation","title":"Before creating a pull request","text":"","category":"section"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"tip: Atomic git commits\nTry to create \"atomic git commits\" (recommended reading: The Utopic Git History).","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"Make sure the tests pass.\nFetch any main updates from upstream and rebase your branch, if necessary:\ngit fetch upstream\ngit rebase upstream/main BRANCH_NAME\nThen you can open a pull request and work with the reviewer to address any issues.","category":"page"},{"location":"developer/#Building-and-viewing-the-documentation-locally","page":"Developer Documentation","title":"Building and viewing the documentation locally","text":"","category":"section"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"Following the latest suggestions, we recommend using LiveServer to build the documentation. Here is how you do it:","category":"page"},{"location":"developer/","page":"Developer Documentation","title":"Developer Documentation","text":"Run julia --project=docs to open Julia in the environment of the docs.\nIf this is the first time building the docs\nPress ] to enter pkg mode\nRun pkg> dev . to use the development version of your package\nPress backspace to leave pkg mode\nRun julia> using LiveServer\nRun julia> servedocs()","category":"page"},{"location":"practice/practice_1_acf/#Building-the-Autocorrelation-Function","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"","category":"section"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"Data is noisy. Each time point has a random deviation. It is meaningless to ask anything about a single time point. However, certain statistical properties of random data are not random. For example, if I flip a coin 1000 times it is meaningless to ask whether the 348th flip will be heads or tails but on average, half the time I will get heads and half the time I will get tails. Correlation time is a statistical property of time-series data. It is not random: you can get many different random time series with the same correlation time. Correlation time measures how long does it take for a signal to lose similarity to itself. Why should we care? It is the basis of intrinsic neural timescales (INTs) and since you are here, I am assuming that you care about INTs. I'll explain more in the Theory sections. For now, let's just assume that it matters and learn how to calculate it. ","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"To quantify the similarity between two things, we can use correlation. The higher the correlation, more similar two things are. The assumption that something loses similarity with itself implies that initially there was a similarity but over time we lost it. To quantify similarity of something with itself at a later time, we can calculate the correlation between that thing and that thing pushed forward in time. It is easier to see this with a figure. ","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"(Image: )","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"We took the time series x and shifted it forward in time by an amount Deltat. Then we need to take the correlation between them. To take a correlation between two things, you need to have equal number of data points in each of them. This is due to the definition of correlation, correlation is the average value of multiplication normalized by variance. You need to multiply corresponding data points. Take a look at the code example below. Throughout the documentation, there will be many code examples. I encourage you to run them on your computer and play around with them. Even better, take a pen and piece of paper and do the calculation below yourself. There is no better way to train intuition other than grinding your way through a calculation but I digress. Here is the code:","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"using Statistics # Import Statistics package for cor function\nx1 = [-1, 0, 1] # example data\nx2 = [2, -2, 0]\nvariance_x1 = sqrt(sum(x1 .^ 2) / 3) # Calculate variance of each dataset\nvariance_x2 = sqrt(sum(x2 .^ 2) / 3)\n# Covariance is the average value of multiplication\ncovariance_x1_x2 = (x1[1]*x2[1] + x1[2]*x2[2] + x1[3]*x2[3]) / 3\n# Correlation is normalized covariance\ncorrelation_x1_x2 = covariance_x1_x2 / (variance_x1 * variance_x2)\nisapprox(correlation_x1_x2, cor(x1, x2)) # Compare with cor function from Statistics package","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"This looks basic, but makes an important point. As you go forward in time, you need to match the time points in your time series and shifted version of it. In the figure above, the only usable part is the part indicated in black vertical lines. This means as we shift further in time, we have less time points at our disposal and our correlation results are less reliable. We will return back to this point later. ","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"We took a time-series, shifted it by an amount Deltat, calculated the correlation and if the result is not zero, then we can say that the time series still hasn't lost similarity to itself in Deltat amount of time. Take a moment to ponder about this sentence. We are insinuating that there is such a Deltat where the correlation is zero, or close to zero and this is the time it takes for a signal to lose similarity with itself. This is our INT. ","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"Then a good strategy to calculate INT is simply calculating the correlation at various Deltat values and detecting which Deltat is the time where we lose correlation. Let's code this. We'll use the function generate_ou_process from the IntrinsicTimescales.jl package. This function simulates time series with a known timescale. I'll explain more about what it is doing in Theory section. For now, just know that this exists and is a good toy to play with. In IntrinsicTimescales.jl package, we have more optimized ways to do the operation I'll write below. I am doing this below explicitly and in detail so that we know exactly what we are doing when we compute these things. ","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"using IntrinsicTimescales # import INT package\nusing Random \nusing Plots # to plot the results\nRandom.seed!(1) # for replicability\n\ntimescale = 1.0\nsd = 1.0 # sd of data we'll simulate\ndt = 0.001 # Time interval between two time points\nduration = 10.0 # 10 seconds of data\nnum_trials = 1 # Number of trials\n\ndata = generate_ou_process(timescale, sd, dt, duration, num_trials)\ndata = data[:] # Go from a (1, time) matrix to (time) vector","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"The resulting data from generate_ou_process is a matrix where rows are different trials and columns are time points. In order to simplify the code below, I do the operation data = data[:] to turn it into a one dimensional vector. ","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"The next step is shifting forward in time and correlating on this data. Look at the code below, take a piece of pen and paper and explicitly write down the indexing operations for different values of Deltat to get a sense of how we are implementing this. Essentially, we are finding the indices corresponding to the data between the black vertical lines shown in the figure above. ","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"n_timepoints = length(data)\nn_lags = 4000 # Calculate the first 4000 lags.\ncorrelation_results = zeros(n_lags) # Initialize empty vector to fill the results\n# Start from no shifting (0) and end at number of time points - 1. \nlags = 0:(n_lags-1)\nfor DeltaT in lags\n    # Get the indices for the data in vertical lines\n    indices_data = (DeltaT+1):n_timepoints\n    indices_shifted_data = 1:(n_timepoints - DeltaT)\n    correlation_results[DeltaT+1] = cor(data[indices_data], data[indices_shifted_data])\nend\nplot(lags, correlation_results, label=\"\") \nhline!([0], color=:black, label=\"\") # Indicate the zero point of correlations","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"(Image: )","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"This is called an autocorrelation function (ACF). On x axis, we have lags. One lag means we shifted one of the time series by one data point. On y axis, we plot the correlation values. Note that it starts from 1. Because when lag is zero, we did not shift any time series. We are correlating a time series with exactly itself and the correlation between one thing and itself is simply one. As we expected, the ACF decays as we shift lags. We can identify the lag where the correlation reaches zero. This is the first estimate of our timescale. This measure is called ACW-0 which stands for autocorrelation window-0. It was first used by Mehrshad Golesorkhi in his 2021 paper and he found that ACW-0 differentiates brain regions better than previously used methods. Let's calculate the ACW-0 and indicate it in the plot with a vertical red line. ","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"acw_0 = findfirst(correlation_results .< 0)\nplot(correlation_results, xlabel=\"Lags\", ylabel=\"Correlation\", label=\"\")\nhline!([0], color=:black, label=\"\")\nvline!([acw_0], color=:red, label=\"ACW-0\")","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"(Image: )","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"So our work is done, right? We started with 1) the definition that INT is the time it takes for a time-series to lose its similarity with itself, 2) operationalized similarity with correlation, 3) operationalized similarity with itself as correlation with itself shifted some time lags and 4) identified the INT as the number of time lags required to lose similarity. There is one problem. Remember the problem of number of time points we talked about above. As we go further in lags, we have less and less number of data points to calculate the correlation, the portion inside vertical black lines is getting smaller and smaller. If we do not have enough number of data points to calculate ACW-0, then we will get a noisy estimate. ","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"Let's try to see how big of a problem this is. Below, we will simulate the time-series again and again and overlay plots of ACFs. In a different panel, we'll do a histogram of ACW-0 values. To calculate ACF, we will use the function comp_ac_fft from  IntrinsicTimescales.jl package. This function is faster and uses a different technique to calculate ACF which I'll explain in the next section. For now, it should suffice to know that it takes the data and optionally the number of lags we want as input and gives back the ACF. If number of lags is not specified, it goes through all possible lags. To get the ACW-0 from the ACF, we'll use acw0 function which takes lags and ACF as input and gives ACW-0 value. ","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"acw0_results = [] # Initialize empty vectors to hold the results\nacfs = []\nn_simulations = 10\nfor _ in 1:n_simulations\n    data = generate_ou_process(timescale, sd, dt, duration, num_trials)[:]\n    acf = comp_ac_fft(data; n_lags=n_lags)\n    i_acw0 = acw0(lags, acf)\n    push!(acw0_results, i_acw0) # Same as .append method in python\n    push!(acfs, acf)\nend\np1 = plot(lags, acfs, xlabel=\"Lags\", ylabel=\"Correlation\", \n          label=\"\", title=\"ACF\", alpha=0.5)\nhline!([0], color=:black, label=\"\")\n\np2 = histogram(acw0_results, xlabel=\"ACW-0\", ylabel=\"Count\",\n               label=\"\", title=\"Distribution of ACW-0\")\n\n# Combine the plots side by side\nplot(p1, p2, layout=(1,2))","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"(Image: )","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"What's going on  here? We simulated the same process 10 times and each time we got a different result. All simulations had the same timescale, which we set as 1.0 above. Why did we get different results? Didn't we start by saying that even the data is random, statistical properties of it are not? That we can flip a coin 1000 times and on average half the time it will be heads and half the time it will be tails? Well, not quite. We said that on average, half the time it will be heads and the other half, it will be tails. Let's define an experiment as flipping the coin 1000 times. If you do this experiment once and look at the results, perhaps it will be 498 heads and 502 tails. Then do the experiment again, it will maybe give you 505 heads and 495 tails. You do the experiment again and again and keep track of the results. Then if you average over experiments, you'll see that there are 500 heads and 500 tails in the end. You can do a mini version of this experiment at home with 10 coin flips. The more experiments you do, the better the results will be. ","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"Here is the central insight: When we calculate ACW-0 from limited data, we are not doing a perfect calculation. We are making an estimation. Based on the data we know, this is the timescale we think. And estimations are noisy. The noisiness of the estimation depends on the properties of data. The more number of data points we have, the better the estimations are. This is why I stressed that as we calculate ACF in later and later lags, our estimations become less and less reliable simply because we have less number of data points at our disposal. To see it clearly, look at the figure in the left panel and observe that at earlier lags, the variance between ACF estimates are low and it progressively increases as you go along later lags. Feel free to change the parameters dt,  timescale and duration to see how they change results. ","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"This is why it is crucial to not only know your research problem, be it cognitive or basic neuroscience, but also the estimators you use to tackle the problem. How noisy are they? How much they are vulnerable to the number of data points? Are there other things in the data that might bias the results? Just because you are getting a number out of some algorithm does not mean that number has any meaning. It is the responsibility of the researcher, you to make sure your numbers make sense. ","category":"page"},{"location":"practice/practice_1_acf/","page":"Building the Autocorrelation Function","title":"Building the Autocorrelation Function","text":"In the next section, we will explore various kinds of autocorrelation windows, their motivation and how they address the bias. ","category":"page"},{"location":"tutorial/tutorial_3_ft/#Usage-with-FieldTrip","page":"Usage with FieldTrip","title":"Usage with FieldTrip","text":"","category":"section"},{"location":"tutorial/tutorial_3_ft/","page":"Usage with FieldTrip","title":"Usage with FieldTrip","text":"This tutorial will show how to use IntrinsicTimescales.jl with FieldTrip. We won't go into the details of preprocessing, since it is out of the scope of our package. We'll use the example data from this Fieldtrip tutorial. ","category":"page"},{"location":"tutorial/tutorial_3_ft/","page":"Usage with FieldTrip","title":"Usage with FieldTrip","text":"% Add fieldtrip to the path and read data\nrestoredefaultpath\naddpath /path/to/fieldtrip\nft_defaults\n\n% Read data\nsubj = 'sub-22';\nrootpath = '/path/to/workshop/data/madrid2019'; \n\n% Read data\ncfg = [];\ncfg.dataset    = [rootpath, '/tutorial_cleaning/single_subject_resting/' subj '_task-rest_run-3_eeg.vhdr'];\ncfg.channel    = 'all';\ncfg.demean     = 'yes';\ncfg.detrend    = 'no';\ncfg.reref      = 'yes';\ncfg.refchannel = 'all';\ncfg.refmethod  = 'avg';\ndata = ft_preprocessing(cfg);","category":"page"},{"location":"tutorial/tutorial_3_ft/","page":"Usage with FieldTrip","title":"Usage with FieldTrip","text":"Similar to MNE tutorial, we'll make 10 second trials and compute the ACF/PSD for each trial. ","category":"page"},{"location":"tutorial/tutorial_3_ft/","page":"Usage with FieldTrip","title":"Usage with FieldTrip","text":"cfg = [];\ncfg.length = 10;\ncfg.overlap = 0;\ndata_segmented = ft_redefinetrial(cfg, data);","category":"page"},{"location":"tutorial/tutorial_3_ft/","page":"Usage with FieldTrip","title":"Usage with FieldTrip","text":"To save the data in .MAT format, we'll first extract the data from FieldTrip struct, then use save function. Let's also note the sampling rate.","category":"page"},{"location":"tutorial/tutorial_3_ft/","page":"Usage with FieldTrip","title":"Usage with FieldTrip","text":"fs = data.fsample % 250.0\n\ndata_array = cat(3, data_segmented.trial{:});\nsavepath = '/path/to/save/data'\nsave(fullfile(savepath, 'data.mat'), 'data_array', 'fs')","category":"page"},{"location":"tutorial/tutorial_3_ft/","page":"Usage with FieldTrip","title":"Usage with FieldTrip","text":"Now we will switch to Julia. To read .MAT files, we will use the MAT.jl package.","category":"page"},{"location":"tutorial/tutorial_3_ft/","page":"Usage with FieldTrip","title":"Usage with FieldTrip","text":"using MAT\nusing IntrinsicTimescales\n\ndata_path = \"/path/to/data.mat\"\ndata_dict = matread(data_path)\ndata = data_dict[\"data_array\"]\nfs = data_dict[\"fs\"]","category":"page"},{"location":"tutorial/tutorial_3_ft/","page":"Usage with FieldTrip","title":"Usage with FieldTrip","text":"We'll note the size of the data, and compute the INT metrics using the acw function.","category":"page"},{"location":"tutorial/tutorial_3_ft/","page":"Usage with FieldTrip","title":"Usage with FieldTrip","text":"println(size(data)) # channels x time x trials\n\nresults = acw(data, fs, acwtypes=[:acw50, :auc, :tau], parallel=true, dims=2, trial_dims=3, average_over_trials=true)","category":"page"},{"location":"tutorial/tutorial_3_ft/","page":"Usage with FieldTrip","title":"Usage with FieldTrip","text":"That's all. ","category":"page"},{"location":"simbasedinference/#sim","page":"Overview","title":"Simulation Based Timescale Estimation","text":"","category":"section"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"In simulation based methods, your data is assumed to come from a generative model and IntrinsicTimescales.jl performs Bayesian parameter estimation (via approximate Bayesian computation, ABC or automatic differentiation variational inference, ADVI) on that model. The goal is to match the autocorrelation function (ACF) or equivalently, power spectral density (PSD) of the generative model and data. The simplest generative model is an Ornstein-Uhlenbeck (OU) process with only one parameter to estimate. In case of oscillations, an oscillation is linearly added to the output of the Ornstein-Uhlenbeck process. If some of your data is missing, indicated by NaN or missing, the data points from the generative model are replaced by NaNs. We note that the variance of noise in the OU process is not fit to data as we scale the output of simulations to match the variance of data in order to reduce the burden or parameter fitting procedure. ","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"All methods assume that your data has one dimension for trials and one dimension for time points. From each trial, IntrinsicTimescales.jl calculates one summary statistic (ACF or PSD) and averages them across trials to get a less noisy estimate. The simulations from the generative model have the same data structure (same number of data points, trials and time resolution) as your data. The goal of the simulation based methods is minimizing the distance between the ACF or PSD of your model and data. Then the parameter corresponding to INT in your model is hopefully the real INT. ","category":"page"},{"location":"simbasedinference/#Model-Types","page":"Overview","title":"Model Types","text":"","category":"section"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"There are four main functions in IntrinsicTimescales.jl to perform simulation based timescale estimation: one_timescale_model, one_timescale_and_osc_model, one_timescale_with_missing_model, one_timescale_and_osc_with_missing_model. For each model, one can choose between :abc or :advi as the inference method and :acf or :psd as the summary method. All models have the same syntax with differences in implementation. The detailed usage is documented in one_timescale_model - other model pages focus on specific differences. ","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"The following table summarizes the four models. ","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"Model Generative Model Summary Method (:acf or :psd) Supported Inference Methods (:abc or :advi)\none_timescale_model Ornstein-Uhlenbeck process comp_ac_fft or comp_psd_adfriendly ABC and ADVI\none_timescale_and_osc_model Sinusoid added on Ornstein-Uhlenbeck process comp_ac_fft or comp_psd_adfriendly ABC and ADVI\none_timescale_with_missing_model Ornstein-Uhlenbeck process with missing data replaced by NaNs comp_ac_time_missing or comp_psd_lombscargle ABC (for both ACF and PSD), ADVI (only ACF)\none_timescale_and_osc_with_missing_model Sinusoid added on Ornstein-Uhlenbeck process with missing data replaced by NaNs comp_ac_time_missing or comp_psd_lombscargle ABC (for both ACF and PSD), ADVI (only ACF)","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"All models are fit with int_fit function and return ADVIResults or ABCResults type. See the Fitting and Results section for details. ","category":"page"},{"location":"simbasedinference/#Fitting-Methods-ABC","page":"Overview","title":"Fitting Methods - ABC","text":"","category":"section"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"Approximate Bayesian Computation (ABC) is a method to approximate the posterior without solving the likelihood function. The algorithm has two steps: ABC (basic_abc) and population monte carlo (PMC, pmc_abc). In pseudocode, ABC is as follows:","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"summary = summary_statistic(empirical_data)\naccepted_samples = []\nWHILE length(accepted_samples) < min_accepted\n    theta = sample_from_prior()\n    model_data = simulate_data(model, theta)\n    distance = compute_distance(summary, model_data)\n    IF distance < epsilon\n        push!(accepted_samples, theta)\n    END IF\nEND WHILE","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"PMC uses ABC samples as the initial population and iteratively updates. For more details, refer to Zeraati et al, 2021. ","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"To change the parameters of the ABC algorithm, first use the function get_param_dict_abc to get the default parameters. Then modify the parameters and pass them to the function int_fit. For example, ","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"using IntrinsicTimescales\ntimescale = 0.3 # true timescales\nvariance = 1.0 # variance of data\nduration = 10.0 # duration of data\nn_trials = 10 # How many trials\nfs = 500.0 # Sampling rate\ndata = generate_ou_process(timescale, variance, 1/fs, duration, n_trials)\ntime = (1/fs):(1/fs):duration\nmodel = one_timescale_model(data, time, :abc)\nparam_dict = get_param_dict_abc()\nparam_dict[:convergence_window] = 10\nresult = int_fit(model, param_dict)\nint_map = result.MAP[1] # Maximum a posteriori ","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"The parameters are detailed in Parameters for Approximate Bayesian Computation section.","category":"page"},{"location":"simbasedinference/#Fitting-Methods-ADVI","page":"Overview","title":"Fitting Methods - ADVI","text":"","category":"section"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"Note: ADVI functionality is experimental. Proceed with caution. ","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"Automatic Differentiation Variational Inference (ADVI) approximates the posterior using variational methods. Instead of using MCMC directly, ADVI uses gradient descent to find the optimal parameters that minimize the Kullback-Leibler divergence between the variational posterior and the true posterior. IntrinsicTimescales.jl uses the Turing.jl package to perform ADVI. For more details, refer to Turing documentation or Kucukelbir et al, 2017. ","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"IntrinsicTimescales.jl states the probabilistic problem as the likelihood of each data point in the summary statistic of the data coming from a Gaussian distribution with mean generative model's summary statistic and some uncertainty around it. More clearly:","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"textrmdata summary statistic_i sim N(textrmmodel summary statistic_i sigma)","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"Similar to ABC, in order to change the parameters of the ADVI algorithm, use the function get_param_dict_advi to get the default parameters, modify them, and pass them to the function fit. Example:","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"model = one_timescale_and_osc_model(data, time, :advi)\nparam_dict = get_param_dict_advi()\nparam_dict[:n_iterations] = 20\nresults = int_fit(model, param_dict)","category":"page"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"See Parameters for Automatic Differentiation Variational Inference section for details on parameters.","category":"page"},{"location":"simbasedinference/#Notes-on-Summary-Statistics","page":"Overview","title":"Notes on Summary Statistics","text":"","category":"section"},{"location":"simbasedinference/","page":"Overview","title":"Overview","text":"Each model uses either ACF or PSD as the summary statistic. As can be seen from the table above, with no missing data, comp_ac_fft and comp_psd_adfriendly are used. comp_ac_fft calculates the ACF using the fast fourier transform (FFT). comp_psd_adfriendly is an autodifferentiable implementation of comp_psd; both use Periodogram method with a Hamming window. In case of missing data, ACF is calculated in the time domain with the same techniques used in statsmodels.tsa.stattools.acf with missing=conservative option. For PSD, Lomb-Scargle method (via LombScargle.jl) with the function  comp_psd_lombscargle is used but currently it is not autodifferentiable. If you wish to use PSD with simulation-based inference in the case of missing data, you would need to use ABC. ","category":"page"},{"location":"practice/practice_2_acw/#Autocorrelation-Windows","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"","category":"section"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"We finished the previous section with a discussion about how a determinstic statistic can be influenced by the limitations of our data. In this section, we will generalize the problem and discuss various autocorrelation window (ACW) types. ","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"First, some nomenclature. When we make an analysis on the data, for example, calculate event-related potentials, ACWs and so on, we are aiming for an estimand. In event-related potentials, our estimand is the stereotypical response of the brain to some cognitive task. In ACWs, our estimand is intrinsic neural timescales (INTs). The ACW we get is not INT per se, it is the estimate of INT. To obtain the estimate, we use an estimator. The schema below shows the relationship. ","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"(Image: )","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"Our first note about the noise of estimators was the finiteness of the data. We noted that as we go along further lags, we have less data points at our hand to calculate the correlation values, making the estimate noisier. A first response to the problem is to use a different cutoff. Instead of waiting the autocorrelation function to reach exactly to 0 thus completely losing the similarity, we can cut it off when it reaches 0.5 and say losing half of the similarity. After all, a time-series with a longer timescale should take longer to lose half of it. This method is called ACW-50. It is  older than ACW-0. To my knowledge, used first in Honey et al., 2012. This was a time when the phrase intrinsic neural timescale had not been established. The term at that time was temporal receptive windows (TRW). I will discuss the evolution of the term more in the Theory section. For now, we will make simulations from two processes with different timescales and see how well we can distinguish their INTs using ACW-50 versus ACW-0. To quickly get many simulations with the same timescale, I will set num_trials to 1000 in the function generate_ou_process. ","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"using IntrinsicTimescales # import IntrinsicTimescales package\nusing Random \nusing Plots # to plot the results\nRandom.seed!(1)\n\ntimescale_1 = 1.0\ntimescale_2 = 3.0\nsd = 1.0 # sd of data we'll simulate\ndt = 0.001 # Time interval between two time points\nduration = 10.0 # 10 seconds of data\nnum_trials = 1000 # Number of trials\n\ndata_1 = generate_ou_process(timescale_1, sd, dt, duration, num_trials)\ndata_2 = generate_ou_process(timescale_2, sd, dt, duration, num_trials)\nprintln(size(data_1)) # == 1000, 10000: 1000 trials and 10000 time points","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"To streamline the ACW calculation, I will use the acw function from IntrinsicTimescales.jl. This function takes your time series data, sampling rate and ACW types you want to calculate and returns the ACW values in the same shape of the data. Along with ACW results it also returns additional information that will be useful later. To extract ACW values, we will extract the field acw_results from the output of acw. It is best to demonstrate with an example. ","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"fs = 1 / dt # sampling rate\nacwresults_1 = acw(data_1, fs, acwtypes=[:acw50, :acw0]) \nacwresults_2 = acw(data_2, fs, acwtypes=[:acw50, :acw0])\n# Since we used the order [:acw50, :acw0], the first element of results is ACW-50, the second is ACW-0.\nacw50_1 = acwresults_1.acw_results[1]\nacw0_1 = acwresults_1.acw_results[2]\nacw50_2 = acwresults_2.acw_results[1]\nacw0_2 = acwresults_2.acw_results[2]","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"How to quantify the sensitivity of the estimator (to changes in the timescale)? Let's assume an experimental scenario where we are comparing INTs of two conditions or two groups. We are calculating one timescale from each condition. The number of trials (num_trials above) can refer to either number of trials or subjects. Then we'll compare the INTs from two groups. We know for a fact that first condition has a shorter timescale than the second since we set them ourselves in the code above (timescale_1 and _2). We will calculate what percentage of the time we are wrong, that we are getting a longer or equal INT for the first condition and a shorter or equal INT for the second condition. ","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"Take a look at the code below, we will calculate what I described in the previous awful paragraph. Hopefully the code is cleaner than my English. Additionally, we will plot histograms to visualize the overlap between estimates. ","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"using Printf\n\nbad_acw50_timescale = mean(acw50_2 .<= acw50_1) * 100\nbad_acw0_timescale = mean(acw0_2 .<= acw0_1) * 100\n\n# Plot histograms\np1 = histogram(acw50_1, alpha=0.5, label=\"timescale 1 = $(timescale_1)\")\nhistogram!(p1, acw50_2, alpha=0.5, label=\"timescale 2 = $(timescale_2)\")\n# Plot the median since distributions are not normal\nvline!(p1, [median(acw50_1), median(acw50_2)], linewidth=3, color=:black, label=\"\") \ntitle!(p1, \"ACW-50\\n\")\n# Mad string manipulation\nannotate!(p1, 1, 100, \n    (@sprintf(\"Proportion of \\\"wrong\\\" timescale \\nestimates: %.2f%% \\n\", bad_acw50_timescale)), textfont=font(24), :left)\n# ACW-0\np2 = histogram(acw0_1, alpha=0.5, label=\"timescale 1 = $(timescale_1)\")\nhistogram!(p2, acw0_2, alpha=0.5, label=\"timescale 2 = $(timescale_2)\")\n\nvline!(p2, [median(acw0_1), median(acw0_2)], linewidth=3, color=:black, label=\"\")\ntitle!(p2, \"ACW-0\\n\")\nannotate!(p2, 2, 175, \n    (@sprintf(\"Proportion of \\\"wrong\\\" timescale \\nestimates: %.2f%% \\n\", bad_acw0_timescale)), textfont=font(24), :left)\nplot(p1, p2, size=(1600, 800))","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"(Image: )","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"It seems ACW-0 gives messier results. Needless to say, these results depend on the difference between real timescales and the amount of data. Feel free to change these parameters and investigate the results under different scenarios. So ACW-50 seems to be a better estimator at least in the setting we specified above. Is our work done then? First of all, we used a weird way to define \"wrong\". We've reduced the correctness to a binary choice of is something greater or smaller than other. We can be more principled than this and actually quantify how much we are off. We will do this in the next section. For now, let's consider another scenario. In the example above, we had dt = 0.001 implying our sampling rate (fs) is 1000 Hz and we have 10 seconds of data. This sounds like an EEG/MEG scenario. Let's try an fMRI scenario where we have a sampling rate of 0.5 Hz (corresponding TR=2 seconds) and 300 seconds of data. We'll set the timescales to 1 seconds and 3 seconds for short timescale and long timescale guys. ","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"Random.seed!(1)\ntimescale_1 = 1.0\ntimescale_2 = 3.0\nsd = 1.0 \ndt = 2.0 # Time interval between two time points\nduration = 300.0 # 5 minutes of data\nnum_trials = 1000 # Number of trials\n\ndata_1 = generate_ou_process(timescale_1, sd, dt, duration, num_trials)\ndata_2 = generate_ou_process(timescale_2, sd, dt, duration, num_trials)\n\nfs = 1 / dt # sampling rate\nacwresults_1 = acw(data_1, fs, acwtypes=[:acw50, :acw0]) \nacwresults_2 = acw(data_2, fs, acwtypes=[:acw50, :acw0])\nacw50_1 = acwresults_1.acw_results[1]\nacw0_1 = acwresults_1.acw_results[2]\nacw50_2 = acwresults_2.acw_results[1]\nacw0_2 = acwresults_2.acw_results[2]\n\nbad_acw50_timescale = mean(acw50_2 .<= acw50_1) * 100\nbad_acw0_timescale = mean(acw0_2 .<= acw0_1) * 100\n\n# Plot histograms\np1 = histogram(acw50_1, alpha=0.5, label=\"timescale 1 = $(timescale_1)\")\nhistogram!(p1, acw50_2, alpha=0.5, label=\"timescale 2 = $(timescale_2)\")\nvline!(p1, [median(acw50_1), median(acw50_2)], linewidth=3, color=:black, label=\"\") \ntitle!(p1, \"ACW-50\\n\")\nannotate!(p1, 3, 600, \n    (@sprintf(\"Proportion of \\\"wrong\\\" timescale \\nestimates: %.2f%% \\n\", bad_acw50_timescale)), textfont=font(24), :left)\n# ACW-0\np2 = histogram(acw0_1, alpha=0.5, label=\"timescale 1 = $(timescale_1)\")\nhistogram!(p2, acw0_2, alpha=0.5, label=\"timescale 2 = $(timescale_2)\")\n\nvline!(p2, [median(acw0_1), median(acw0_2)], linewidth=3, color=:black, label=\"\")\ntitle!(p2, \"ACW-0\\n\")\nannotate!(p2, 15, 175, \n    (@sprintf(\"Proportion of \\\"wrong\\\" timescale \\nestimates: %.2f%% \\n\", bad_acw0_timescale)), textfont=font(24), :left)\nplot(p1, p2, size=(1600, 800))","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"(Image: )","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"Half the time, we got the wrong result with ACW-50! To diagnose the problem, let's plot the autocorrelation functions. This is where the other information stored in the output of acw comes useful. We'll use the function acwplot to plot the ACFs. This function plots the ACFs and returns a plot object which we can modify later. Note that to reduce compilation time, this function is implemented in an extension in IntrinsicTimescales.jl package. What this means is that to use it, you need to make sure you ran using Plots somewhere in your code (and of course, installed the Plots.jl library by Pkg.add(\"Plots\")). We'll put vertical lines at the lags where we compute autocorrelation. Note that the lags are also stored in the output of acw. To not plot 1000 ACFs for each trial, let's resimulate data with a reasonable number of trials. ","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"using Plots\nnum_trials = 20 # Number of trials\ndata_1 = generate_ou_process(timescale_1, sd, dt, duration, num_trials)\ndata_2 = generate_ou_process(timescale_2, sd, dt, duration, num_trials)\nacwresults_1 = acw(data_1, fs, acwtypes=[:acw50, :acw0]) \nacwresults_2 = acw(data_2, fs, acwtypes=[:acw50, :acw0])\np = acwplot(acwresults_1)\nvline!(p, [acwresults_1.lags], linewidth=3, color=:black, label=\"\")","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"(Image: )","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"The autocorrelation is dropping below 0.5 before even 2 seconds pass. And because our time resolution was two seconds, most of the autocorrelation functions drop below 0.5 even before we can calculate ACW-50. ","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"There is one more method in case ACW-50 is not working well. Let's consider the case above: we want to be able to distinguish the processes but we don't have the time resolution to use ACW-50. We can use ACW-0 but the later lags are more noisy. Wouldn't it be great if we had a method that assigns higher weights to earlier lags and lower weight to less reliable later lags? Turns out there is one such method. We can calculate the area under the curve (AUC) of the ACF. Since later lags have less correlation, their contribution to the area under the curve will be less. In IntrinsicTimescales.jl, we can use :auc to calculate the AUC of ACF before ACF touches 0. Under the hood, this method uses Romberg.jl to use Romberg's method. This method is orders of magnitude more accurate than trapezoid (as in np.trapz or MATLAB trapz) and Simpson's methods (as in scipy.integrate.simpson). Let's repeat the above experiment to compare ACW-0 and AUC methods:","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"Random.seed!(123)\ntimescale_1 = 1.0\ntimescale_2 = 3.0\nsd = 1.0 \ndt = 2.0 # Time interval between two time points\nduration = 300.0 # 5 minutes of data\nnum_trials = 1000 # Number of trials\n\ndata_1 = generate_ou_process(timescale_1, sd, dt, duration, num_trials)\ndata_2 = generate_ou_process(timescale_2, sd, dt, duration, num_trials)\n\nfs = 1 / dt # sampling rate\nacwresults_1 = acw(data_1, fs, acwtypes=[:auc, :acw0]) \nacwresults_2 = acw(data_2, fs, acwtypes=[:auc, :acw0])\nauc_1 = acwresults_1.acw_results[1]\nacw0_1 = acwresults_1.acw_results[2]\nauc_2 = acwresults_2.acw_results[1]\nacw0_2 = acwresults_2.acw_results[2]\n\nbad_auc_timescale = mean(auc_2 .<= auc_1) * 100\nbad_acw0_timescale = mean(acw0_2 .<= acw0_1) * 100\n\n# Plot histograms\np1 = histogram(auc_1, alpha=0.5, label=\"timescale 1 = $(timescale_1)\")\nhistogram!(p1, auc_2, alpha=0.5, label=\"timescale 2 = $(timescale_2)\")\nvline!(p1, [median(auc_1), median(auc_2)], linewidth=3, color=:black, label=\"\") \ntitle!(p1, \"AUC\\n\")\nannotate!(p1, 3, 100, \n    (@sprintf(\"Proportion of \\\"wrong\\\" timescale \\nestimates: %.2f%% \\n\", bad_auc_timescale)), textfont=font(24), :left)\n# ACW-0\np2 = histogram(acw0_1, alpha=0.5, label=\"timescale 1 = $(timescale_1)\")\nhistogram!(p2, acw0_2, alpha=0.5, label=\"timescale 2 = $(timescale_2)\")\n\nvline!(p2, [median(acw0_1), median(acw0_2)], linewidth=3, color=:black, label=\"\")\ntitle!(p2, \"ACW-0\\n\")\nannotate!(p2, 20, 300, \n    (@sprintf(\"Proportion of \\\"wrong\\\" timescale \\nestimates: %.2f%% \\n\", bad_acw0_timescale)), textfont=font(24), :left)\nplot(p1, p2, size=(1600, 800))","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"(Image: )","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"We can see that AUC seems to be a better estimator than ACW-0. The result we see here strongly favors AUC over ACW-0 but do play with the random seed and see what happens. For the most part, AUC is better and in some cases AUC and ACW-0 give similar results. ","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"It seems different modalities and temporal resolutions call for different ways to calculate ACW. But even with the better estimator, we can still be off 1/5th of the time. Can we do better? Remember the coin flipping experiment from the previous section. We said that if we repeat the experiment again and again and take average across experiments, our estimates get better. Let's do this in the context of ACW-50. In the code below, I will first make 1000 simulations, then from each one of them, I'll calculate one autocorrelation function. Then I'll cumulatively average those autocorrelation functions, i.e. I'll average the first two ACFs, the first three, the first four... Then I'll calculate ACW-50 and ACW-0 from each step. Let's see if they are converging. ","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"using Statistics, IntrinsicTimescales, Plots, Random\ntimescale = 3.0\nsd = 1.0 # sd of data we'll simulate\ndt = 0.001 # Time interval between two time points\nfs = 1 / dt\nduration = 10.0 # 10 seconds of data\nnum_trials = 1\nacfs = []\nacw50s = []\nacw0s = []\nn_experiments = 1000\nfor i in 1:n_experiments\n    data = generate_ou_process(timescale, sd, dt, duration, num_trials, rng=Xoshiro(i), deq_seed=i)\n    acf = comp_ac_fft(data[:])\n    push!(acfs, acf)\n    current_mean_acf = mean(acfs)\n    lags = (0:(length(current_mean_acf)-1)) * dt\n    current_acw50 = acw50(lags, current_mean_acf)\n    current_acw0 = acw0(lags, current_mean_acf)\n    push!(acw50s, current_acw50)\n    push!(acw0s, current_acw0)\nend\np1 = plot(acw50s, label=\"ACW-50\", xlabel=\"Iterations\", ylabel=\"ACW\")\np2 = plot(acw0s, label=\"ACW-0\", xlabel=\"Iterations\", ylabel=\"ACW\")\nplot(p1, p2, size=(800, 400))","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"(Image: )","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"Note that it takes about 250 trials for the estimates to completely stabilize. This is a huge number. In the case above, we assumed that each trial is 10 seconds. 250 trials x 10 seconds is 41 minutes of data which we usually don't have. Nonetheless, even averaging across a couple trials makes the estimates much closer to the stabilized estimate. This is why the Honey et al. paper I mentioned above calculated one ACF from 20 seconds of data and averaged over ACFs. In IntrinsicTimescales.jl, this is handled by the argument average_over_trials in the acw function. It is your responsibility to put your data in a format where one dimension is trials and one dimension is time. This is usually handled with MNE or FieldTrip helper functions (see for example mne.make_fixed_length_epochs, this tutorial from MNE or ft_redefinetrial. If you think taking continuous data and segmenting with different overlap degrees would be useful for you in IntrinsicTimescales.jl, open an issue on github and I can add this as a feature.). Before finishing this section, let's run one final experiment where we now have 20 trials for each subject and 100 subjects. Let's do a t-test between the groups and see if we can capture the difference. To run the code below, you'll need to install the Julia package HypothesisTests. ","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"using HypothesisTests\nn_subjects = 100\nn_trials = 20\nnum_trials = 20\ntimescale_1 = 1.0\ntimescale_2 = 3.0\nsd = 1.0\ndt = 0.001\nfs = 1 / dt\nduration = 10.0\n\nacw50_1 = Float64[] # HypothesisTests doesn't accept Vector{Any} type, requires Vector{<:Real} type\nacw50_2 = Float64[]\nacw0_1 = Float64[]\nacw0_2 = Float64[]\nfor i in 1:n_subjects\n    data_1 = generate_ou_process(timescale_1, sd, dt, duration, num_trials, rng=Xoshiro(i), deq_seed=i)\n    data_2 = generate_ou_process(timescale_2, sd, dt, duration, num_trials, rng=Xoshiro(i), deq_seed=i)\n    acwresults_1 = acw(data_1, fs, acwtypes=[:acw50, :acw0], average_over_trials=true)\n    acwresults_2 = acw(data_2, fs, acwtypes=[:acw50, :acw0], average_over_trials=true)\n    current_acw50_1 = acwresults_1.acw_results[1]\n    current_acw50_2 = acwresults_2.acw_results[1]\n    current_acw0_1 = acwresults_1.acw_results[2]\n    current_acw0_2 = acwresults_2.acw_results[2]\n    push!(acw50_1, current_acw50_1)\n    push!(acw50_2, current_acw50_2)\n    push!(acw0_1, current_acw0_1)\n    push!(acw0_2, current_acw0_2)\nend\n\nbad_acw50_timescale = mean(acw50_2 .<= acw50_1) * 100\nbad_acw0_timescale = mean(acw0_2 .<= acw0_1) * 100\n\n# Plot histograms\np1 = histogram(acw50_1, alpha=0.5, label=\"timescale 1 = $(timescale_1)\")\nhistogram!(p1, acw50_2, alpha=0.5, label=\"timescale 2 = $(timescale_2)\")\n# Plot the median since distributions are not normal\nvline!(p1, [median(acw50_1), median(acw50_2)], linewidth=3, color=:black, label=\"\") \ntitle!(p1, \"ACW-50\\n\")\nannotate!(p1, 0.6, 15, \n    (@sprintf(\"Proportion of \\\"wrong\\\" timescale \\nestimates: %.2f%% \\n\", bad_acw50_timescale)), textfont=font(24), :left)\n# ACW-0\np2 = histogram(acw0_1, alpha=0.5, label=\"timescale 1 = $(timescale_1)\")\nhistogram!(p2, acw0_2, alpha=0.5, label=\"timescale 2 = $(timescale_2)\")\n\nvline!(p2, [median(acw0_1), median(acw0_2)], linewidth=3, color=:black, label=\"\")\ntitle!(p2, \"ACW-0\\n\")\nannotate!(p2, 2, 20, \n    (@sprintf(\"Proportion of \\\"wrong\\\" timescale \\nestimates: %.2f%% \\n\", bad_acw0_timescale)), textfont=font(12), :left)\nplot(p1, p2, size=(1600, 800))\n\nprintln(UnequalVarianceTTest(acw50_1, acw50_2))\nprintln(UnequalVarianceTTest(acw0_1, acw0_2))","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"(Image: )","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"Note that our wrong estimates for ACW-50 reduced to 0! ACW-0 is still slightly noisy but much better now. You can also check out the t-test results, both ACWs returned a significant difference. This approach offers a way to see how many subjects you need to get a significant difference if your hypothesis is right. You can copy-paste the script above to play around with it when designing experiments and figuring out the number of subjects you need for different effect sizes. ","category":"page"},{"location":"practice/practice_2_acw/","page":"Autocorrelation Windows","title":"Autocorrelation Windows","text":"This was a long tutorial. Take a deep breath, make a coffee for yourself, go for a walk and come back for the next one. There is still work to do: we need to figure out how to calculate exactly how wrong are we. Under certain assumptions, we can actually do this. But we need some theoretical tools. In the next section, We'll develop those theoretical tools and they will motivate us to calculate ACW in different ways.  ","category":"page"},{"location":"acw/#acw","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"","category":"section"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"Performed via the function acw in IntrinsicTimescales.jl. The acw function calculates ACF or PSD depending on the acwtypes you specify. If there is no missing data (indicated by NaN or missing), acw calculates ACF as the inverse fourier transform of the power spectrum, using comp_ac_fft internally. Otherwise it calculates ACF as correlations between a time-series and its lag-shifted variants, using comp_ac_time_missing. For PSD, it uses periodogram method (comp_psd) in the case of no missing data and Lomb-Scargle method (comp_psd_lombscargle) in the case of missing data. ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"acwresults = acw(data, fs; acwtypes=[:acw0, :acw50, :acweuler, :auc, :tau, :knee], \n                n_lags=nothing, freqlims=nothing, dims=ndims(data), \n                return_acf=true, return_psd=true, \n                average_over_trials=false, trial_dims=setdiff([1, 2], dims)[1], \n                skip_zero_lag=false,\n                max_peaks=1, oscillation_peak::Bool=true,\n                allow_variable_exponent::Bool=false,\n                constrained::Bool=false,\n                parallel::Bool=false)","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"Simple usage:","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"data = randn(10, 300) # 10 trials, 300 time points\nfs = 1.0 # sampling rate\nresults = acw(data, fs)\nacw_results = results.acw_results\nacw_0 = acw_results[1]\nacw_50 = acw_result[2]\n# And so on...","category":"page"},{"location":"acw/#Arguments","page":"Model-Free Timescale Estimation","title":"Arguments","text":"","category":"section"},{"location":"acw/#Mandatory-arguments:","page":"Model-Free Timescale Estimation","title":"Mandatory arguments:","text":"","category":"section"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"data: Your time-series data as a vector or n-dimensional array. ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"If it is n-dimensional, by default, the dimension of time is assumed to be the last dimension. For example, if you have a 2D array where rows are subjects and columns are time points, acw function will correctly assume that the last (2nd) dimension is time. If the dimension of time is any other dimension than the last, you can set it via dims argument. For example: ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"data = randn(100, 200, 5) # 100 trials, 200 time points, 5 channels\nacw(data, fs; dims=2)","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"fs: Sampling rate of your data. A floating point number. ","category":"page"},{"location":"acw/#Optional-arguments:","page":"Model-Free Timescale Estimation","title":"Optional arguments:","text":"","category":"section"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"acwtypes: A symbol or a vector of symbols denoting which ACW types to calculate. ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"In Julia, a symbol is written as :symbol. Example:","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"acw(data, fs; acwtypes=:acw0)\nacw(data, fs; acwtypes=[:acw0, :acw50])","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"Supported ACW types:","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":":acw0: The lag where autocorrelation function crosses 0.","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":":acw50: The lag where autocorrelation function crosses 0.5.","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":":acweuler: The lag where autocorrelation function crosses 1e. Corresponds to the inverse decay rate of an exponential decay function. ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":":tau: Fit an exponential decay function e^fracttau to the autocorrelation function and extract tau, which is the inverse decay rate. The parameter skip_zero_lag is used to specify whether to skip the zero lag for fitting an exponential decay function. When zero-lag is skipped, the function fits an exponential decay of the form A (exp(-lags  tau) + B) where A is the amplitude and B is the offset. See below for details and references. ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":":auc: Calculate the area under the autocorrelation function from lag 0 to the lag where autocorrelation function crosses 0. ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":":knee: Fit a lorentzian function fracA1 + (fa)^2 to the power spectrum using an iterative FOOOF-style approach. By Wiener-Khinchine theorem, this is the power spectrum of a time-series with an autocorrelation function of exponential decay form. The parameter a corresponds to the knee frequency. tau and a has the relationship tau = frac12 pi a. The :knee method uses this relationship to estimate tau from the knee frequency. In practice, first, an initial lorentzian fit is performed. Then, any oscillatory peaks are identified and fitted with gaussian functions. These gaussians are subtracted from the original power spectrum to ensure the remaining PSD is closer to a Lorentzian, and a final Lorentzian is fit to this \"cleaned\" spectrum. freqlims is used to specify the frequency limits to fit the lorentzian function. You can set the maximum number of oscillatory peaks to fit with the max_peaks argument. The argument oscillation_peak is used to specify whether to fit the oscillatory peaks or not. If set to false, just fit a Lorentzian and return the timescale estimated from the knee frequency. The parameter allow_variable_exponent is used to specify whether to allow a variable exponent in the lorentzian fit (i.e. b in fracA1 + (fa)^b). This might be useful for cases where the power spectrum does not conform to a simple Lorentzian. The parameter constrained is used to specify whether to use constrained optimization when fitting a lorentzian to the PSD for knee frequency estimation (see below). ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"n_lags: An integer. Only used when :tau is in acwtypes. The number of lags to be used for fitting an exponential decay function. ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"By default, this is set to 1.1*acw0. The reason for cutting the autocorrelation function is due to increased measurement noise for autocorrelation function for later lags. Intuitively, when we perform correlation of a time-series with a shifted version of itself, we have less and less number of time points to calculate the correlation. This is why if you plot the autocorrelation function, the portion of it after ACW-0 looks more noisy. For more details, see [Practice 1]. ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"freqlims: Only used when :knee is in acwtypes. The frequency limits to fit the lorentzian function. ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"By default, the lowest and highest frequencies that can be estimated from your data. A tuple of two floating point numbers, for example, (freq_low, freq_high) or (1.0, 50.0). ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"dims: The dimension of time in your data. See above, the data argument for the explanation. An integer. \nreturn_acf: Whether or not to return the autocorrelation function (ACF) in the results object. A boolean. \nreturn_psd: Whether or not to return the power spectrum (psd) in the results object. A boolean. \naverage_over_trials: Whether or not to average the ACF or PSD across trials, as in [Honey et al., 2012]. ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"Assuming that your data is stationary, averaging over trials can greatly reduce noise in your ACF/PSD estimations. By default, the dimension of trials is assumed to be the first dimension of your data. For example, if your data is two dimensional with rows as trials and columns as time points, the function will correctly infer the dimension of trials. If this is not the case, set the dimension of trials with the argument trial_dims. Below is an example of a three dimensional data with time as second and trials as third argument:","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"data = randn(10, 1000, 20) # 10 subjects, 1000 time points, 20 trials\nresult = acw(data, fs; dims=2, average_over_trials=true, trial_dims=3)","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"trial_dims: Dimension of trials to average over. See above (average_over_trials) for explanation. An integer.\nskip_zero_lag: Whether or not to skip the zero lag for fitting an exponential decay function. Default is false. If true, the function will fit an exponential decay of the form A (exp(-lags  tau) + B) where A is the amplitude and B is the offset. This can be useful for cases with very low sampling rate (e.g. fMRI). The technique is used in Ito et al., 2020 and Murray et al., 2014. \nmax_peaks: Maximum number of oscillatory peaks to fit when cleaning the PSD for knee frequency estimation. Default is 1. \noscillation_peak: Whether or not to fit the oscillatory peaks when cleaning the PSD for knee frequency estimation. Default is true.\nallow_variable_exponent: Whether or not to allow variable exponent (PLE) when fitting a lorentzian to the PSD for knee frequency estimation. Default is false. If true, the function will admit Lorentzian's of form fracA1 + (fa)^b where b is not confined to -2. \nconstrained: Whether or not to use constrained optimization when fitting a lorentzian to the PSD for knee frequency estimation. Default is false. If true, the function will use constrained optimization via Optim.jl (using Optimization.jl as a frontend). The lower constraints for amplitude, knee frequency and exponent are 0, freqs[1], 0 respectively. The upper constraints are Inf, freqs[end], 5.0. For optimization, LBFGS method is used. \nparallel: Whether or not to use parallel computation. Default is false. If true, the function will use the OhMyThreads library to parallelize the computation. ","category":"page"},{"location":"acw/#Returns","page":"Model-Free Timescale Estimation","title":"Returns","text":"","category":"section"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"acwresults: An ACWResults object. It has the fields fs, acw_results, acwtypes, n_lags, freqlims, acf, psd, freqs, lags, x_dim. You can access these fields as acwresults.field. The field acw_results contains the ACW results indicated by the input argument acwtypes in the same order you specify. Each element of acw_results is an array of the same size of your data minus the dimension of time, which will be dropped. See below for details. ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"The reason to not return the results directly but return the ACWResults object is 1) give access to ACF and PSDs  where the calculations are performed as well as n_lags and freqlims if the user is using defaults, 2) make plotting easy. You can simply type acwplot(acwresults) to plot ACF and PSDs. ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"Your primary interest should be the field acwresults.acw_results. This is a vector of arrays. Easiest way to explain this is via an example: ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"data = randn(2, 1000, 10) # assume 2 subjects, 1000 time points and 10 trials\nfs = 1.0\nacwresults = acw(data, fs; acwtypes=[:acw0, :tau, :knee], dims=2)\nacw_results = acwresults.acw_results ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"acw_results is a two element vector containing the results with the same order of acwtypes as you specify. Since we wrote :acw0 as the first element and :tau as the second element, and :knee as the third element, we can extract the results as ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"acw_0 = acw_results[1]\ntau = acw_results[2]\nknee = acw_results[3]","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"Let's check the dimensionality of these results. Remember that we specified 2 subjects, 1000 time points and 10 trials. The result collapses the dimension of time and gives the result as an 2x10 matrix. ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"size(acw_0) # should be (2, 10)\nsize(tau) # should be (2, 10)","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"Other fields:","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"fs: Sampling rate. Floating point number. \nacwtypes: The ACW types you specified. \nn_lags: The number of lags used to fit exponential decay function to the autocorrelation function. See above in the input arguments for details. An integer.\nfreqlims: Frequency limits to fit a lorentzian to the power spectrum. See above in the input arguments for details. A tuple of floating point numbers. \nacf: Autocorrelation function(s). Has the same size of your data with the time dimension replaced by lag dimension with n_lags elements. \npsd: Power spectrum/spectra. Has the same size of your data with the time dimension replaced by frequency dimension with freqlims as lowest and highest frequencies. \nfreqs: Frequencies corresponding to PSD. \nlags: Lags corresponding to ACF. \nx_dim: The dimension corresponding to lags and frequencies. Used internally in plotting. ","category":"page"},{"location":"acw/#Plotting","page":"Model-Free Timescale Estimation","title":"Plotting","text":"","category":"section"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"The function acwplot can plot power spectra and autocorrelation functions. Currently it supports only two dimensions (for example, subjects x time or trials x time). ","category":"page"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"p = acwplot(acwresults; only_acf=false, only_psd=false, show=true)","category":"page"},{"location":"acw/#Mandatory-Arguments","page":"Model-Free Timescale Estimation","title":"Mandatory Arguments","text":"","category":"section"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"acwresults: ACWResults type obtained by running the function acw. ","category":"page"},{"location":"acw/#Optional-Arguments","page":"Model-Free Timescale Estimation","title":"Optional Arguments","text":"","category":"section"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"only_acf / only_psd: Plot only the ACF or only the PSD. Boolean. \nshow: Whether to show the plot or only return the variable that contains the plot. ","category":"page"},{"location":"acw/#Returns-2","page":"Model-Free Timescale Estimation","title":"Returns","text":"","category":"section"},{"location":"acw/","page":"Model-Free Timescale Estimation","title":"Model-Free Timescale Estimation","text":"p: The plot for further modification using the Plots library. ","category":"page"},{"location":"one_timescale/#one_timescale","page":"One Timescale Model","title":"One Timescale Model (one_timescale_model)","text":"","category":"section"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"The generative model:","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"fracdxdt = -fracxtau + xi(t)","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"with timescale tau. xi(t) is white noise with unit variance. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"Can be used with either ACF or PSD as the summary method. Below, in each section, we describe the arguments for each summary method. ","category":"page"},{"location":"one_timescale/#ACF-Summary","page":"One Timescale Model","title":"ACF Summary","text":"","category":"section"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"The full function signature for summary_method=:acf is:","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"function one_timescale_model(data, time, fit_method; summary_method=:acf,\n                             prior=nothing, n_lags=nothing,\n                             distance_method=nothing,\n                             dims=ndims(data), distance_combined=false,\n                             weights=[0.5, 0.5])","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"Example usage:","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"# Simulate some data:\nusing IntrinsicTimescales\ntimescale = 0.3 # true timescales\nvariance = 1.0 # variance of data\nduration = 10.0 # duration of data\nn_trials = 10 # How many trials\nfs = 500.0 # Sampling rate\ndata = generate_ou_process(timescale, variance, 1/fs, duration, n_trials) # Data in the form of (trials x time)\n\n# Prepare the vector of time points:\ntime = (1/fs):(1/fs):duration\n\n# Fit the model:\nmodel = one_timescale_model(data, time, :abc, summary_method=:acf)\nresults = int_fit(model)\nint = results.MAP[1] # maximum a posteriori estimate","category":"page"},{"location":"one_timescale/#Mandatory-arguments:","page":"One Timescale Model","title":"Mandatory arguments:","text":"","category":"section"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"data: Your time-series data as a vector or 2-dimensional array. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"If it is n-dimensional, by default, the dimension of time is assumed to be the last dimension. If this is not the case, you can set it via dims argument, similar to acw function. The other dimension should correspond to trials. IntrinsicTimescales.jl calculates one ACF from each trial and averages them to get a less noisy ACF estimate. If the user wants to calculate one INT per trial, they can run a for-loop over trials. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"time: Time points corresponding to the data. \nfit_method: :abc or :advi. Method to use for parameter estimation. ","category":"page"},{"location":"one_timescale/#Optional-arguments:","page":"One Timescale Model","title":"Optional arguments:","text":"","category":"section"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"summary_method: :acf. Method to use for summary statistics. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"If :acf, calculates the autocorrelation function using comp_ac_fft internally. If :psd, calculates the power spectral density using comp_psd. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"prior: Prior distribution for the parameters. \"informed_prior\" or a Distribution object from Distributions.jl. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"If the user does not specify a prior, or specifies \"informed_prior\", IntrinsicTimescales.jl uses a normal distribution with mean determined by fitting an exponential decay to the autocorrelation function using fit_expdecay and standard deviation of 20. Currently we recommend explicitly specifying a prior distribution to improve the accuracy of the inference. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"An example for custom prior distribution:","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"using Distributions\nprior = Normal(0.5, 0.5)\nmodel = one_timescale_model(data, time, :abc, summary_method=:acf, prior=prior)\nresults = int_fit(model)\nint = results.MAP[1]","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"n_lags: Number of lags to use for the ACF calculation. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"By default, this is set to 1.1*acw0. The reason for cutting the autocorrelation function is due to increased measurement noise for autocorrelation function for later lags. Intuitively, when we perform correlation of a time-series with a shifted version of itself, we have less and less number of time points to calculate the correlation. This is why if you plot the autocorrelation function, the portion of it after ACW-0 looks more noisy. For more details, see [Practice 1]. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"distance_method: :linear or :logarithmic. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"Method to use for distance calculation. :linear is RMSE between the ACF from data and the model ACF whereas :logarithmic is RMSE after log-transforming the ACF. The default is :linear. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"distance_combined: true or false. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"If true, the distance is a weighted sum of RMSE between ACFs and RMSE between exponential decay fits to ACFs. Defaults to false.","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"weights: A vector of two numbers. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"The first number is the weight for RMSE between ACFs and the second number is the weight for RMSE between exponential decay fits to ACFs. The default is [0.5, 0.5]. Used only if distance_combined is true. ","category":"page"},{"location":"one_timescale/#PSD-Summary","page":"One Timescale Model","title":"PSD Summary","text":"","category":"section"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"The full function signature for summary_method=:psd is:","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"function one_timescale_model(data, time, fit_method; summary_method=:psd,\n                             prior=nothing, \n                             distance_method=nothing, freqlims=nothing,\n                             dims=ndims(data), distance_combined=false,\n                             weights=[0.5, 0.5])","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"Example usage:","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"model = one_timescale_model(data, time, :abc, summary_method=:psd)\nresults = int_fit(model)\nint = results.MAP[1]","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"There are two arguments that are different from the ACF summary:","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"summary_method: :psd. Method to use for summary statistics. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"Calculates the power spectral density using comp_psd. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"freqlims: A tuple of two numbers. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"The first number is the lower frequency limit and the second number is the upper frequency limit used to index the power spectral density. The default is the output from fftfreq function of AbstractFFTs.jl library. ","category":"page"},{"location":"one_timescale/#Implementation-differences-from-ACF-Summary","page":"One Timescale Model","title":"Implementation differences from ACF Summary","text":"","category":"section"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"prior: \"informed_prior\" fits a lorentzian to the PSD and calculates the INT from the knee frequency using tau_from_knee and find_knee_frequency. \ndistance_method: :linear is RMSE between the PSD from data and the model PSD whereas :logarithmic is RMSE after log-transforming the PSD. The default is :linear. \ndistance_combined: Weighted sum between RMSE between PSDs and RMSE between INT estimates from knee frequency obtained from lorentzian fits to PSD. \nweights: A vector of two numbers. ","category":"page"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"The first number is the weight for RMSE between PSDs and the second number is the weight for RMSE between INT estimates. ","category":"page"},{"location":"one_timescale/#Returns","page":"One Timescale Model","title":"Returns","text":"","category":"section"},{"location":"one_timescale/","page":"One Timescale Model","title":"One Timescale Model","text":"model: A OneTimescaleModel object. Can be used as an input to fit function to estimate INT and posterior_predictive function to plot posterior predictive check. ","category":"page"},{"location":"one_timescale_with_missing/#one_timescale_with_missing","page":"One Timescale Model with Missing Data","title":"One Timescale with Missing Data (one_timescale_with_missing_model)","text":"","category":"section"},{"location":"one_timescale_with_missing/","page":"One Timescale Model with Missing Data","title":"One Timescale Model with Missing Data","text":"Uses the same syntax as one_timescale_model. We refer the user to the documentation of one_timescale_model for details and point out the differences here. ","category":"page"},{"location":"one_timescale_with_missing/","page":"One Timescale Model with Missing Data","title":"One Timescale Model with Missing Data","text":"The generative model is the same as one_timescale_model: ","category":"page"},{"location":"one_timescale_with_missing/","page":"One Timescale Model with Missing Data","title":"One Timescale Model with Missing Data","text":"fracdxdt = -fracxtau + xi(t)","category":"page"},{"location":"one_timescale_with_missing/","page":"One Timescale Model with Missing Data","title":"One Timescale Model with Missing Data","text":"with timescale tau. xi(t) is white noise with unit variance. The missing data points will be replaced by NaNs as in:","category":"page"},{"location":"one_timescale_with_missing/","page":"One Timescale Model with Missing Data","title":"One Timescale Model with Missing Data","text":"generated_data[isnan.(your_data)] .= NaN","category":"page"},{"location":"one_timescale_with_missing/","page":"One Timescale Model with Missing Data","title":"One Timescale Model with Missing Data","text":"To compute the summary statistic, comp_ac_time_missing for ACF and comp_psd_lombscargle for PSD is used. Note that PSD is not supported for ADVI method since the comp_psd_lombscargle is not autodifferentiable. ","category":"page"},{"location":"one_timescale_with_missing/","page":"One Timescale Model with Missing Data","title":"One Timescale Model with Missing Data","text":"For arguments and examples, see the documentation for one_timescale_model. Just replace one_timescale_model with one_timescale_with_missing_model. ","category":"page"},{"location":"practice/practice_5_bayesian/#Bayesian-Estimation-of-Intrinsic-Timescales","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"","category":"section"},{"location":"practice/practice_5_bayesian/#An-Example-With-Missing-Data","page":"Bayesian Estimation of Intrinsic Timescales","title":"An Example With Missing Data","text":"","category":"section"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"We almost came to the end. This is the final chapter of the tutorials. We'll go back to the starting point of this package: abcTau. This was a Python package written by Dr. Roxana Zeraati, accompanying her paper A flexible Bayesian framework for unbiased estimation of timescales. I really liked the idea and wanted to reimplement it in Julia. The rest of the package branched out of this initial spark. A special thanks to Dr. Zeraati for patiently helping me out with a couple questions I asked her before I started this endevaour.","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"Let's consider the following scenario: you have some EEG data. You nicely preprocessed the data and used various artifact rejection algorithms. Usually artifact rejection algorithms chop up the data from 1 to 3 second epochs and reject those small epochs. Let's say in a second step you want to calculate the INTs from this data. Now you have a problem: both the autocorrelation function and power spectrum assume evenly sampled data. If you naively discard the epoch you reject, you will simply mess up your results due to the discontinuity in your data. You might be inclined to run ACW on short epochs but previous tutorials showed us that data length is crucial to get a good ACW estimate. A more proper way to handle the problem is to indicate your missing data points with NaNs and use techniques specialized to deal with missing data.","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"IntrinsicTimescales.jl offers the functions comp_ac_time_missing and comp_psd_lombscargle to deal with missing data. The first one calculates the ACFs in time domain while jumping over the missing numbers. Essentially it replicates the behavior of statsmodels.tsa.stattools.acf with the option missing=\"conservative\". You can read the source code if you would like to see the actual implementation. The second function calculates the PSD using Lomb-Scargle periodogram. IIRC, this is a technique developed by astronomers who wanted to get the PSD but their data had missing observations simply because you can't record the positions of planets when the sky is cloudy. I don't remember where I've read this, don't quote me on this one. IntrinsicTimescales.jl uses LombScargle.jl from Julia Astro Organization to perform the operation. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"Let's start by simulating some data with EEG characteristics. I'll simulate 30 trials, 10 seconds each with a 500 Hz sampling rate with a timescale of 0.3 seconds. For simplicity, I'll ignore oscillations. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"using IntrinsicTimescales\ntimescale = 0.3\nfs = 500.0\ndt = 1 / fs\nsd = 1.0\nduration = 10.0\nnum_trials = 30\ndata = generate_ou_process(timescale, sd, dt, duration, num_trials)","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"Now from each trial, let's randomly select three seconds of data and replace it with NaNs. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"using Random\nRandom.seed!(666)\n# For each trial, randomly select a 1-second window and replace with NaNs\ndata_missing = copy(data)\nrejection_seconds = 3\nsamples_to_reject = Int(fs) * rejection_seconds\nfor trial in 1:num_trials\n    # Random start index between 1 and (total samples - 1 second worth of samples)\n    start_idx = rand(1:(size(data,2) - samples_to_reject))\n    # Replace 1 second of data with NaNs\n    data_missing[trial, start_idx:(start_idx + samples_to_reject - 1)] .= NaN\nend","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"Now let's calculate ACW-e from both the clean data and data with missing numbers and compare the results. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"acw_clean = acw(data, fs; acwtypes=:acweuler, average_over_trials=true)\nprintln(acw_clean.acw_results)\n# 0.26\nacw_missing = acw(data_missing, fs; acwtypes=:acweuler, average_over_trials=true)\nprintln(acw_missing.acw_results)\n# 0.248","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"We are off by about 0.04 seconds from finite data length and another 0.052 seconds from the missing data. This is quite impressive given that we rejected about 1/3 of the data. Averaging over trials really makes the estimations stable. I leave it to you to explore the case of average_over_trials=false, the results can get much worse. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"Can we do better than this and hit the timescale right on the head? Bayesian timescale estimation can do this. In addition to hitting the timescale right on the head, it also gives you uncertainty of the timescale estimation. Since this is technically way more complicated than what we have done so far, I won't give as much detail as the previous tutorials. I recommend you to read Dr. Zeraati's paper for more details. The implementation details can be found in the Simulation Based Timescale Estimation section of the documentation. Nonetheless, I'll try to demistify some of the jargon. ","category":"page"},{"location":"practice/practice_5_bayesian/#Demistifying-Bayes-Theorem","page":"Bayesian Estimation of Intrinsic Timescales","title":"Demistifying Bayes Theorem","text":"","category":"section"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"At the heart of Bayesian estimation, there is, not so surprisingly the Bayes theorem. There are many good explanations on internet. Here's my attempt. Let's start with the following probability:","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"p(a b) = p(a) p(ba)","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"The probability of both a and b is the probability of a multiplied by the  probability of a when b is already known to be right. Example: let's say there are one Turkish, one German, one Chinese, one Cameroonian and two Canadians are in our lab right now. One of the Canadians is male and the other is female. The other members of the lab are incidentally all male. If I randomly pick a lab member, what is the probability that I get a lab member who is both Canadian and male? The probability of being Canadian is 2/6. The probability being a male if we already know the lab member is Canadian is 1/2. We can multiply these and get the probability of being both Canadian and male as 1/6. In math:","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"p(textrmCanadian textrmmale) = p(textrmCanadian)p(textrmmaletextrmCanadian)","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"But consider the fact that p(ab)=p(ba), the ordering doesn't matter. The probability of being Canadian and male is the same as being male and Canadian. Then I can write the first equation alternatively as:","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"p(a b) = p(a) p(ba) = p(b a) = p(b) p(ab)","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"A bit of simple arithmetic:","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"p(a) p(ba) = p(b) p(ab) \n\np(ab) = fracp(ba) p(a)p(b)","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"And this is the Bayes theorem. Why is this useful? Let's replace a and b with actually scientific things. Let's say, we want to estimate INT. And we have some data to do that. To avoid the neutrality of a and b, I'll denote what we want to estimate, the INT as theta and the data as x. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"p(theta  x) = fracp(x  theta) p(theta)p(x)","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"In human language, the probability of our INT estimate is right given the data we know is proportional to the probability of the data we observe assuming our INT estimate is correct and the probability of the INT estimate is right divided by the probability of the data. Let's break it down. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"p(theta  x): The probability of an INT estimate is right given the data we know. Throughout all these tutorials, our theme was \"there is an underlying INT and I want to get it, but my estimations are noisy\". We said that given the data we have on our hands, this is our best bet. Now we are formalizing this. What do we mean by data? In the first tutorial, we started by saying that individual data points are meaningless but their statistical properties are meaningful. And the statistical property of our choice was ACF (or later, PSD). So what I mean by data here is ACF/PSD. p(theta  x) is what we want to calculate and is called the posterior. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"p(x  theta): The probability of the data if we assume that the INT estimate is right. We assume that we have a generative model. In fact, we do have one: OU process! This is where our beloved function generate_ou_process really comes in handy. If we plug in the INT we think, we can get the ACF and various ACW measures. We can get as many of them as we want. This is the ACF (data) we have given the INT estimate we propose. This is called the likelihood. I think of it as the likelihood of the data. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"p(theta): The probability of the INT. This is our prior belief of INT. If we haven't seen any data whatsoever, what is the probability that INT is, say, 5? EEG timescales are usually somewhere between 0.1 and 1 seconds. Rarely they can be less than 0.1 and more than 1. But it would be really weird to see an EEG timescale of 10 seconds. fMRI timescales on the other hand are usually between 1 to 10 seconds. This is called the prior: prior belief of the parameter before we see any data. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"p(x): The probability of the data. This never really made sense to me. If you have a nice explanation of what it means please let me know. How I made peace with it is via the realization that we do not need to calculate it. The nice thing about probabilities is that probabilities have to add up to 1. Let's say we know p(x  theta) and p(theta). We also know that the total probability of both the left hand side and right hand side should add up to 1. That's one degree of freedom we do not have, we can't arbitrarily set these numbers in a way that would violate this property of probabilites. Then we can get p(x) just from this property. This uninteresting part is called marginal likelihood. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"And that's Bayes theorem in a nutshell. The real difficulty is how to get these probabilities. If I have an INT estimate, I can plug it into generate_ou_process and get another ACF but what is the probability that that estimate is right?","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"Turns out this is a very hard problem. There are a number of numerical schemes to get those probabilities, the most famous being Hamiltonian Monte Carlo. But even Hamiltonian Monte Carlo becomes very slow when our model involves a differential equation (i.e. OU process). ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"In IntrinsicTimescales.jl I offer two ways to approximate the posterior. The first one is adaptive approximate Bayesian computation (aABC), developed in  Beaumont et al. 2009 and used in Zeraati et al. 2021 paper I mentioned above. The second method is automatic differentiation variational inference (ADVI), developed in Kucukelbir et al. 2016 and used in the context of INTs for the first time in IntrinsicTimescales.jl. ","category":"page"},{"location":"practice/practice_5_bayesian/#Approximate-Bayesian-Computation-(ABC)","page":"Bayesian Estimation of Intrinsic Timescales","title":"Approximate Bayesian Computation (ABC)","text":"","category":"section"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"In summary, ABC is the following:","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"Specify your data. For example, the ACF from the time-series you have from your EEG dataset. \nSpecify a prior distribution. For example, a uniform distribution of timescales between 0 seconds and 10 seconds. \nSpecify a generative model. For example, the ACF obtained from an Ornstein-Uhlenbeck process. \nInitialize an empty posterior.\nDraw a sample from the prior. Here, sample is the proposed INT value. Let's say, 3. \nPlug your sample into your generative model. For example, run generate_ou_process with its first argument set to 3 and calculate the ACF from the generated time-series using comp_ac_fft. \nCalculate a distance between the ACF generated from your prior sample and the ACF from your data. For example, calculate the root-mean-squared-error between them. \nIf the distance is lower than a threshold (called epsilon), put the sampled prior to your empty posterior distribution. If the distance is higher, reject the prior and discard it. \nRinse and repeat until you have enough samples in your posterior. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"In pseudocode, the whole thing can be summarized as:","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"posterior = []\nWHILE length(posterior) < MINIMUM_SAMPLES\n    sample = sample_from_prior()\n    time_series = generative_model(sample)\n    summary_statistic = calculate_summary_statistic(time_series) # for example, ACF or PSD\n    distance = calculate_distance(summary_statistic, data)\n    IF distance < EPSILON\n        push!(posterior, sample)\n    ENDIF\nENDWHILE","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"This approximates a posterior. However, there are a few issues with this basic ABC approach. First, the acceptance threshold (EPSILON) needs to be carefully chosen - too high and we accept poor samples, too low and we reject too many samples. Second, the choice of the prior can be a problem. How can we make sure we got the right prior? We start with a reasonable guess but it would be great if the prior could be used more effectively, for example, narrowed down to the places where samples would be more probable to be accepted. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"Adaptive ABC (aABC) addresses these issues by iteratively refining both the acceptance threshold (epsilon) and the proposal distribution. It starts with basic ABC using a relatively high epsilon, then:","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"Adaptive ABC improves on this basic approach using a number of techniques:","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"Look at the accepted samples and use them to refine the prior. At each step of aABC, we can look at how much distance each sample gave and when refining the prior we can give more weight to the good samples and decrease the importance of bad samples. \nMaking the threshold (epsilon) stricter over time. From one ABC iteration to the next, we look at the distribution of distances each time and pick a threshold that is at a certain quantile in the distribution of distances, thus reduce the threshold further and further. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"This procedure is called adaptive since we are tuning epsilon and priors adaptively. I'm just scratching the surface here, read the papers and Simulation Based Timescale Estimation part of the documentation to get a better picture. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"Without further ado, here is the code. We look into the documentation I linked above and pick the relevant models for our case: one_timescale_model and one_timescale_with_missing_model for full data and data with missing values respectively. Then we use the default settings to fit these guys. You can see how to change the default settings here. We also need to specify a vector of time points. This was an early decision which was not particularly wise. Note to self, I need to change these functions to accept just the sampling rate and they should work fine. Nonetheless, getting a time vector is quite easy. If the duration between two time points is dt (=1/fs), then the time vector is simply dt:dt:duration where duration is how long your data is. We also need to tell the function that we are going to use ABC method. This is the third argument which can be set as :abc. Finally we should also specify priors. If you don't specify any priors the algorithm will automatically make an informed guess for you but specifying the priors can make your inference much easier. To specify priors, we'll use the Distributions.jl package. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"using Distributions\nRandom.seed!(666)\nfit_method = :abc\ndt = 1/fs\ntime = dt:dt:duration\nprior = Uniform(0.0, 5.0)\nmodel_full = one_timescale_model(data, time, fit_method; prior=prior)\nmodel_missing = one_timescale_with_missing_model(data, time, fit_method; prior=prior)\nresults_full = int_fit(model_full)\nresults_missing = int_fit(model_missing)","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"Note that this way of calculating INTs is relatively slower than our acw function. This is the cost of accuracy. In my computer, it takes 51 seconds for the model with no missing data and 25 minutes for the model with missing data. The main difference is the way we calculate the ACF: comp_ac_fft is much faster than comp_ac_time_missing. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"Let's check the results. The results object carries a bunch of useful information. The final result of interest is the maximum a posteriori (MAP) estimate. For other information coming from the result, see the relevant part of the documentation. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"map_full = results_full.MAP[1]\nmap_missing = results_missing.MAP[1]","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"Our estimates got much better. We can change the parameters of ABC to play around with the estimation. To change default parameters, we use the function get_param_dict_abc. This gives a dictionary with default parameters. For full model, let's increase the :convergence_window to 10. The algorithm checks if the parameter estimates are converging across :convergence_window number of runs and if they are converging, it stops. For the data with missing values, look into your terminal's output and note that epsilon gets quite reduced but doesn't change much after many iterations. The number it gets stuck is roughly 0.0004. We can stop the algorithm early so that it does not tally around any further. This is done via :target_epsilon parameter. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"param_dict_full = get_param_dict_abc()\nparam_dict_full[:convergence_window] = 10\nparam_dict_missing = get_param_dict_abc()\nparam_dict_missing[:target_epsilon] = 0.0004\nresults_full = int_fit(model_full, param_dict_full)\nresults_missing = int_fit(model_missing, param_dict_missing)\nmap_full = results_full.MAP[1]\nmap_missing = results_missing.MAP[1]","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"The full model took about 4 minutes and the model with missing data took about 10 minutes to run. Unfortunately the estimates did not get any better. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"Another advantage of Bayesian methods is that you don't get just a posterior, you also get the uncertainty around it. Your posterior is not a point estimate but a distribution. Let's plot the posteriors:","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"using Plots\np1 = histogram(results_full.final_theta, label=\"Posterior (Full model)\", alpha=0.5)\nvline!(p1, [map_full], color=:black, style=:dash, linewidth=3, label=\"MAP (Full model)\", alpha=0.5)\nhistogram!(p1, results_missing.final_theta, label=\"Posterior (With missing data)\", alpha=0.5)\nvline!([map_full], color=:red, style=:dash, linewidth=3, label=\"MAP (With missing data)\", alpha=0.5)\nvline!([timescale], color=:blue, style=:dash, linewidth=3, label=\"Correct value\", alpha=0.5)\n\nvline!([acw_clean.acw_results], color=:purple, style=:dash, linewidth=3, label=\"ACW-e estimate (full data)\", alpha=0.5)\nvline!([acw_missing.acw_results], color=:brown, style=:dash, linewidth=3, label=\"ACW-e estimate (missing data)\", alpha=0.5)\nplot(p1, size=(1000,1000))","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"(Image: )","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"As you can see, this is a very powerful tool. But (obligatory reference) with great power comes great responsibility. Here, we knew the real timescale and we can compare it with our estimates. We don't have this luxury when we are analyzing data. To make sure we have a reasonable estimate, we need to do a posterior predictive check. We will sample from the posterior, calculate ACFs from that and compare it with ACF from the data. The IntrinsicTimescales.jl function posterior_predictive handles this. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"p1 = posterior_predictive(results_full, model_full)\np2 = posterior_predictive(results_missing, model_missing)\nplot(p1, p2, size=(800, 400))","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"(Image: )","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"Posterior predictive looks fine. Our work with ABC is done. Let's check out ADVI. ","category":"page"},{"location":"practice/practice_5_bayesian/#Automatic-Differentiation-Variational-Inference-(ADVI)","page":"Bayesian Estimation of Intrinsic Timescales","title":"Automatic Differentiation Variational Inference (ADVI)","text":"","category":"section"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"Note: ADVI functionality is currently experimental. Proceed with caution. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"I'll keep the theory brief here. For details, see Fabian Dablander's brilliant blog post. If you want to learn more, go read it. It will be well worth your time. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"Let's remember the Bayes theorem:","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"p(theta  x) = fracp(x  theta) p(theta)p(x)","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"We said that denominator isn't particularly interesting and we can rewrite it based on the knowledge that the total probability should add up to 1. If we integrate over all possible values in the numerator and put what we get at denominator, then we force the total probability to be 1. (As an intuitive demonstration to see why this is true, run the following code. This kind of normalization enforces a sum to be equal to 1.):","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"x = randn(10000)\nx_total = sum(x)\nsum(x ./ x_total)","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"Back to our case:","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"p(x) = intp(x  theta) p(theta) dtheta \n\np(theta  x) = fracp(x  theta) p(theta)intp(x  theta) p(theta) dtheta","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"We have this nasty integral downstairs. There is no easy way to calculate it and get the posterior. Instead, we can be more humble and say that we don't want the posterior per se but a distribution that approximates the posterior. Precisely, we want a distribution that has minimal Kullback-Leibler Divergence (KL divergence or KLD) to the actual posterior. Minimizing or maximizing something as opposed to calculating that thing is called variational inference. KLD is a way to quantify the distance between two probability distributions. You can't calculate something like a root-mean squared distance on probabilities, KLD is your best bet. Let's write down KLD for two arbitrary probability distributions p(x) and q(x)","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"textrmKL(p(x)q(x)) = intp(x)log(fracp(x)q(x)) dx = int p(x) left( log(p(x)) - log(q(x)) right)dx \n\n= langle log(p(x)) - log(q(x)) rangle","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"The first equality is the definition of KLD. The second equality comes from the properties of the logarithm. The crucial insight is hidden in the last equality. Here, angular brackets langle rangle denote averaging. Keep in mind that intp(x) f(x) dx = langle f(x) rangle. So effectively we are calculating the average difference of the logarithms of two probability distributions. Why logarithms? The probabilities are confined between 0 and 1. This makes calculations annoying. Representing them in logarithms maps them to the world of continuous numbers where addition, subtraction etc. are more natural. This is the intuition behind the KLD. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"Our goal is to find a distribution such that it minimizes the KLD between that distribution and the actual posterior. I'll denote a distribution by q(theta). The q(theta) that minimizes KLD is q^*(theta). ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"q^*(theta) = undersetq(theta)textrmargmin textrmKL(q(theta)p(thetax)) = intq(theta) logfracq(theta)p(thetax) d theta = leftlangle logfracq(theta)p(thetax)  rightrangle","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"Let's massage this expression a little. First, I'll explicitly write down what's inside the logarithm, then I'll break it apart to digestible pieces and finally I'll rewrite the posterior using Bayes theorem:","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"leftlangle logfracq(theta)p(thetax)  rightrangle \n\n= langle logq(theta) - logp(thetax) rangle \n\n= langle logq(theta) rangle - langle logp(thetax) rangle ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"Now using the Bayes theorem on p(thetax)","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"langle logq(theta) rangle - left langle logfracp(xtheta)p(theta)p(x)\n    right rangle \n\n= langle log(q(theta)) rangle - bigg ( \n    langle log(p(xtheta)) rangle + langle log(p(theta)) rangle - langle log(p(x)) rangle\n    bigg) \n\n= langle log(q(theta)) rangle - langle log(p(xtheta)) rangle - langle log(p(theta)) rangle + langle log(p(x)) rangle","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"This is quite messy but I haven't done anything other than high school algebra. If it looks scary, take a piece of pen and paper and write down every line in the derivation above. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"We're still not quite there because we encountered the annoying p(x) again which we still don't know what to do about. But let's consider the alternative approach, see if we can find a way around doing the integrals of p(x). There is a quantity called evidence lower bound (ELBO). It is defined as: ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"textrmELBO(q(theta)) = -(textrmKL(q(theta)p(thetax))-logp(x))","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"Plugging in the mess above for KLD:","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"textrmELBO(q(theta)) = \n-bigg(langle log(q(theta)) rangle - langle log(p(xtheta)) rangle - langle log(p(theta)) rangle + langle log(p(x)) rangle-logp(x)bigg)","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"We have some hope now. Note that p(x) does not depend on theta. The averaging we are doing is an average over theta. This means we can write langle p(x) rangle as p(x) (the average value of a constant is the same constant). Then finally p(x)s above cancel and we get","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"textrmELBO(q(theta)) = -bigg(langle log(q(theta)) rangle - langle log(p(xtheta)) rangle - langle log(p(theta)) rangle bigg)","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"What to do with this thing? We want to minimize the KLD. Maximizing ELBO will automatically minimize KLD for us since ELBO depends on -textrmKL. Then we can ignore p(x) and treat maximizing ELBO as an optimization problem where we can use standard techniques inspired by gradient descent in machine learning. I'm cutting the math short here, for more details seriously read Dablander's blog post. I've never seen a clearer explanation of variational inference before. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"In order to use gradient-descent like methods, we need to be able to take derivatives. This is well-known for machine learning or training neural networks. But how do you take the derivative of doing a simulation and calculating its ACF or PSD? This is where Julia's scientific machine learning (SciML) environment comes into play. If we can write our functions in a way that is nice for automatic differentiation, then ForwardDiff.jl takes the derivatives for us. For models with a small number of parameters (like the Ornstein-Uhlenbeck process we have), forward-differentiation is much more effective than backward-differentiation (or backpropogation) used in training neural networks with hundreds of thousands of parameters. I am aware that I already made this tutorial way longer than I intended to so I'll cut the explanations here, refer to the links above to enter the rabbit hole that I've been in for the last couple of months. The name automatic differentiation variational inference (ADVI) comes from the fact that we are using automatic differentiation to perform variational inference. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"In practice, IntrinsicTimescales.jl uses Turing.jl to perform ADVI. Without further ado, here is the actual code to get the INTs. The syntax is the same as ABC, we'll just change fit_method. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"Random.seed!(666)\nfit_method = :advi\nmodel_full_advi = one_timescale_model(data, time, fit_method; prior=prior)\nmodel_missing_advi = one_timescale_with_missing_model(data, time, fit_method; prior=prior)\nresults_full_advi = int_fit(model_full)\nresults_missing_advi = int_fit(model_missing)","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"Similat to the fitting in ABC method, fitting in ADVI is also customizable via the parameters in get_param_dict_advi. If the fitting fails, you can increase the parameters :n_elbo_samples (how many samples to take to estimate ELBO) and :n_iterations (how many ADVI iterations to perform) to get better estimates. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"Finally we can also do posterior predictive check with the result objects from ADVI. ","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"p1 = posterior_predictive(results_full_advi, model_full_advi)\np2 = posterior_predictive(results_missing_advi, model_missing_advi)\nplot(p1, p2, size=(800, 400))","category":"page"},{"location":"practice/practice_5_bayesian/","page":"Bayesian Estimation of Intrinsic Timescales","title":"Bayesian Estimation of Intrinsic Timescales","text":"That's all. Hope you enjoyed this as much as I did. This was the most fun thing I've ever done during my PhD. ","category":"page"},{"location":"practice/practice_3_ou/#Ornstein-Uhlenbeck-Process-as-a-Generative-Model-for-ACF","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"","category":"section"},{"location":"practice/practice_3_ou/#(Or-Mommy,-Where-Do-the-ACFs-Come-From?)","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"(Or Mommy, Where Do the ACFs Come From?)","text":"","category":"section"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"Despite popular belief, ACFs aren't delivered by storks. So far, we just assumed that they exist and calculated ACW metrics from them. This is where we start building a more comprehensive theory. I will keep the math to a minimum and whenever I explain math, I will supplement it with code so that you can play with to get some intuition. ","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"Let's get right into it. You have some data and from that data you can calculate an autocorrelation function. But what is the most minimal, simplest ground truth that can generate the data that you see? This is a very hard question and the discipline of theoretical neuroscience tries to answer it (Neuronal Dynamics by Gerstner et al. is an excellent starting place for the curious). An easier question is what is the simplest ground truth that can generate the autocorrelation function that you observe in the data? Then we can ask how can we think of the ACF beyond a bunch of numbers. One way to think about ACF is to think of it as an exponential decay function. Here is the math + code:","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"textrmACF(l) = e^-fracltau","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"using Plots\ndt = 0.0001 # time resolution\nlags = (0:2500) * dt # x axis\ntau_short = 0.01 # short timescale\ntau_long = 0.03 # long timescale\nacf_short = exp.(-lags ./ tau_short)\nacf_long = exp.(-lags ./ tau_long)\nplot(lags, acf_short, label=\"Short Timescale ACF\")\nplot!(lags, acf_long, label=\"Long Timescale ACF\")","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"(Image: )","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"Compare this with the ACFs we plotted in the previous tutorials. An exponential decay function is an abstraction of the ACF. In the equation above, we represent the lags with l and timescale with tau. As expected, as I increase tau (tau), the ACF decays slower. You can eyeball the ACW-50. We can do better than eyeballing. Let's do high-school math to get the ACW-50. Remember the definition: ACW-50 is the lag where ACF crosses 0.5:","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"e^-fracltau = 05","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"We need to solve this for l. Remember that the inverse of an exponential is a logarithm. Taking the logarithm of both sides:","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"loge^-fracltau = log05 \n\n-fracltau = log05  \n\nl = -tau log05 \n\nl = -tau log2^-1 \n\nl = tau log2 \n\ntextrmACW-50 = tau log2","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"That's it! We effectively showed that ACW-50 is just the timescale or decay rate of the autocorrelation function up to a constant which is log2. This is good, but the data we have is not just an autocorrelation function. It is the whole time series. The next step is to figure out the generative model for the time-series which gives this autocorrelation function. I will explain the birds and bees of this in the Theory section but even pure practicioners need to know a minimum of theory to understand what they practice. The minimum of theory is the mysterious thing that I shied away from explaining properly, the Ornstein-Uhlenbeck (OU) process. The function generate_ou_process that we used again and again without explaining. No more. Here is the OU:","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"fracdx(t)dt = -frac1tau x(t) + xi (t)","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"This equation is called a differential equation. On the left hand side, you have a derivative (the fracddt term). t denotes time here. You have the rate of change of something (x) with respect to time. That is, how does x change when time changes. x is your dynamic variable or your observable: what you observe in data. To see how it changes, look at the right hand side. The first term is -frac1tau x(t). tau is the timescale. This term ensures that your x always moves to 0. To see how, note that tau is always positive. If you give a positive number to x, the - sign will make sure that x decreases with time. If x reduces too much, becomes negative, then the - sign will again ensure that x increases to move it towards zero because if you put a - sign in front of a negative number, it becomes positive (as in -(-3) = +3). The final term, xi (t) ensures that your x doesn't get stuck at zero. This term is called white noise: it is a random number drawn from a Gaussian distribution. How does this all relate to timescales? Let's ask the question: how fast x approaches zero? Well, this is determined by frac1tau in front of it. Higher the tau, slower the approach because frac1textrmbig number is a small number and vice versa. If you feel uncomfortable with the mathematics I present here, I invite you to pull up a pen and paper and plug in different numbers. It'll become clear. There is no better way to build intuition other than grinding your way through intuition and forcing your way through its wooden doors with a battering ram. ","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"Back to generate_ou_process. What this function does is that it solves this equation. What does it mean to solve an equation? There are a number of ways to approach an equation of this type: one can take averages of both sides, calculate moments, use a Fokker-Planck approach, apply perturbation theory via Martin-Siggia-Rose-De Dominicis-Janssen path integral, apply Fourier analysis, use Ito or Stratonovich calculus. generate_ou_process takes a numerical approach: it assigns a random initial number as a starting condition and moves x forward in small steps according to the equation. Under the hood, it uses the amazing DifferentialEquations.jl library which is optimized to the bone, this is why it is fast. Since we solve the equation for x(t), the end result is a time-series. Under ideal conditions (that is, sufficiently enough data), if you calculate the autocorrelation function of this time-series, you will get an exponential decay function of the same type above. Let's test this with the tools that we are hopefully familiar with now:","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"using IntrinsicTimescales, Statistics\nRandom.seed!(123)\nsd = 1.0\nduration = length(lags) * dt # match the number of lags\nnum_trials = 100\nn_lags = length(lags)\ndata_short_ts = generate_ou_process(tau_short, sd, dt, duration, num_trials, rng=Xoshiro(123), deq_seed=123)\ndata_long_ts = generate_ou_process(tau_long, sd, dt, duration, num_trials, rng=Xoshiro(123), deq_seed=123)\n# average over trials to get a less noisy ACF\nacf_numerical_short = mean(comp_ac_fft(data_short_ts), dims=1)[:]\nacf_numerical_long = mean(comp_ac_fft(data_long_ts), dims=1)[:]\np1 = plot(lags, acf_short, label=\"Analytical ACF\", title=\"Short Timescale\")\nplot!(p1, lags, acf_numerical_short, label=\"Numerical ACF\")\np2 = plot(lags, acf_long, label=\"Analytical ACF\", title=\"Long Timescale\")\nplot!(p2, lags, acf_numerical_long, label=\"Numerical ACF\")\nplot(p1, p2, size=(800,400))","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"(Image: )","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"Note that the numerical ACF estimate decays consistenly faster than the analytical ground truth. The difference between the numerical estimate and analytical one increases as timescale increases. This is a limitation of finite data. As long as your data is finite and has a sampling rate that is not infinitesimally small, you will underestimate the INT. We will address this problem in the final tutorial of Practice. ","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"The good thing is even though we are underestimating the INT, the ACF of the long timescale process still decays slower than the short timescale one. ","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"This theoretical knowledge motivates two more ACW types. The first one is the lag where ACF crosses 1e. In IntrinsicTimescales.jl, this is called acweuler (or ACW-e) but I'm not sure if there is a generic name for it in the literature. The math: ","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"e^-fracltau = frac1e \n\nloge^-fracltau = logfrac1e \n\n-fracltau = -1 \n\nl = tau","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"Assuming that your autocorrelation function is a perfect exponential decay, then calculating ACW-e directly gives you the timescale tau. Alternatively, we can fit an exponential decay function to our autocorrelation function and get the decay rate. IntrinsicTimescales.jl uses the Julia package NonlinearSolve.jl for the fitting. In IntrinsicTimescales.jl, this metric is called tau. Note that by default, IntrinsicTimescales.jl cuts the tail of the ACF before fitting. Remember that the ACF estimate gets noisier as we have less and less data for longer lags. If we keep all the ACF, we might fit to the noise as well. By default, the lag where the ACF is cut is 1.1*acw0. You can change this by the parameter n_lags. In the code example below, I show the two methods. ","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"fs = 1 / dt\nacwresults_short = acw(data_short_ts, fs, acwtypes=[:acweuler, :tau], average_over_trials=true)\nacwresults_long = acw(data_long_ts, fs, acwtypes=[:acweuler, :tau], average_over_trials=true)\nacw_e_short, acw_tau_short = acwresults_short.acw_results\nacw_e_long, acw_tau_long = acwresults_long.acw_results\nprintln(\"Short timescale: $(tau_short)\")\n# 0.01\nprintln(\"ACW-e estimate of short timescale: $(acw_e_short)\")\n# 0.0091\nprintln(\"Curve-fitting estimate of short timescale: $(acw_tau_short)\")\n# 0.0087\nprintln(\"Long timescale: $(tau_long)\")\n# 0.03\nprintln(\"ACW-e estimate of long timescale: $(acw_e_long)\")\n# 0.021\nprintln(\"Curve-fitting estimate of long timescale: $(acw_tau_long)\")\n# 0.02","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"There is one note I should mention before closing: it is possible to skip the zero-lag in the ACF when fitting the exponential decay function. Since the zero-lag is always 1, it doesn't carry any information about the timescale. Especially for cases where sampling rate is very low (I'm talking about fMRI, a TR=1 fMRI data has a sampling rate of 1 Hz), the ACF usually decays from 1 (lag 0) to somewhere around 0.3 to 0.6 (lag 1) which makes the fitting noisier. By reparametrizing the ACF as ","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"textrmACF(l) = A (e^-fracltau + B)","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"where A is the amplitude and B is the offset, it is possible to get better estimates. The option skip_zero_lag is for this purpose. An example usage is ","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"acw_results = acw(data_short_ts, fs, acwtypes=:tau, skip_zero_lag=true)","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"Even though we introduced two more parameters to fit, this estimation tends to be more reliable in fMRI data. This method is used in Ito et al., 2020 and Murray et al., 2014. ","category":"page"},{"location":"practice/practice_3_ou/","page":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","title":"Ornstein-Uhlenbeck Process as a Generative Model for ACF","text":"So far, we always assumed that the ACF is a nice exponential decay. This is rarely the case for EEG/MEG data where oscillatory brain activity (alpha oscillations for example) makes a considerable impact on ACF. We will learn how to deal with it in the next section. ","category":"page"},{"location":"practice/practice_4_psd/#Dealing-with-Oscillatory-Artifacts-using-Fourier-Transformation","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"","category":"section"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"So far, we only dealt with autocorrelation functions (ACFs) that are of exponential decay type. This is fine for fMRI data but can be problematic for EEG/MEG data. To demonstrate, I'll use the function generate_ou_with_oscillation. This function adds an oscillation on top of an Ornstein-Uhlenbeck process. It takes three parameters for its generative model: timescale, oscillation frequency and coefficient for OU process (meaning higher the coefficient, lower the oscillatory artifact). The coefficient is bounded between 0 and 1. If you try to give it a coefficient that is greater than 1 or smaller than 0, it will change it to 1 and 0 respectively. It also takes the desired mean and sd for data. This is required for Bayesian estimation of timescales which will be the topic of next tutorial. Consider the following code. I'll simulate two time-series, with and without oscillatory artifacts and calculate ACW-e which I introduced in the previous section. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"using IntrinsicTimescales, Plots, Random, Statistics\nRandom.seed!(666) # for reproducibility\nfs = 1000.0 # 1000 Hz sampling rate\ndt = 1.0 / fs\nduration = 10 # 10 seconds of data\nnum_trials = 10\ndata_mean = 0.0 # desired mean\ndata_sd = 1.0 # desired sd\n\ntimescale = 0.05 # 50 ms\noscillation_freq = 10.0 # 10 Hz alpha oscillation\ncoefficient = 0.95\ntheta = [timescale, oscillation_freq, coefficient] # vector of parameters\n\ndata_osc = generate_ou_with_oscillation(theta, dt, duration, num_trials, data_mean, data_sd)\ndata = generate_ou_process(timescale, data_sd, dt, duration, num_trials)\nacwresults_osc = acw(data_osc, fs; acwtypes=:acweuler)\nacwresults = acw(data, fs; acwtypes=:acweuler)\nprintln(mean(acwresults_osc.acw_results))\n# 0.022\nprintln(mean(acwresults.acw_results))\n# 0.049\np1 = acwplot(acwresults_osc)\ntitle!(p1, \"ACF with oscillatory component\")\np2 = acwplot(acwresults)\ntitle!(p2, \"ACF\")\nplot(p1, p2)","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"(Image: )","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"The ACW calculated from the simulation with the oscillatory component is terribly off. A good question is why do we see oscillations in the ACF (note the wiggles)? Remember that ACF is calculating the correlation of your data with your data shifted by a certain lag. If you consider a perfect oscillation, its correlation with itself will fluctuate. Whenever the peaks and troughs of oscillation correspond to peaks and troughs of the shifted oscillation (which will happen periodically, when you shift the oscillation just right enough so that it matches with itself), it will nicely correlate with itself. If you shift it half the period of oscillation, so that the peaks of the oscillation will match the troughs of the shifted oscillation, it will have a negative autocorrelation. (Try to draw this on your notebook to get a clearer picture). As a result, the ACF of a perfect oscillation is another oscillation. But oscillation here is also coupled with the OU process. Hence the exponential decay + oscillation type of ACF. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"We need to find a way to decouple the oscillation from the OU process. There is a mathematical technology for this, called the Fourier transform. Essentially it is the correlation of a signal with an oscillation. Let's write down the math. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"tildef(omega) = int_-infty^inftyf(t)e^-iomega t dt=int_-infty^inftyf(t)(cosomega t - i sinomega t) dt","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"The first equality is the definition of the Fourier transform. The second equality comes from Euler's formula. The term cosomega t +i sinomega t is a cosine wave with frequency omega and its complex counterpart i sinomega t. The complex part is there to carry the phase information elegantly. Compare this with the formula for covariance of two zero-mean signals x(t) and y(t). Remember that covariance is a correlation that is not normalized between -1 and 1. I will also ignore dividing it by the number of elements in your vector (as in averaging). ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"textrmcov(x y) = sum_tx_ty_t","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"For every time point t, we multiply x and y at that time point, and add all the values we obtained from multiplication. Time is discrete here, as in our data. x_t corresponds to t-th data point of vector x. Let's make time continuous. We will go from x_t to x(t) to note that we can put any t there, not just the elements of a vector. The discrete summation sum will then be replaced by a continuous sum denoted by the integral sign int dt. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"sum_tx_ty_t rightarrow int x(t)y(t) dt","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"The boundaries of the integral are defined by your data. Theoretically, we set them as -infty and infty. To get the final piece, replace y(t) by our complex sinusoid:","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"int x(t)y(t) dt rightarrow int_-infty^inftyx(t)(cosomega t - i sinomega t) dt \n\ntildex(omega) = int_-infty^inftyx(t)(cosomega t - i sinomega t) dt = int_-infty^inftyx(t)e^-i omega t dt","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"Compare this with the definition of Fourier transform above. We denote the Fourier transform of x(t) as tildex(omega). When we sum over the variable t denoting time, that variable disappears. For every time value, we did a multiplication and added the results of those multiplications. There is no time anymore, we picked all of them. We do this for every frequency omega. For frequency omega, we have tildex(omega). This is why when people talk about Fourier transform, they talk about going from time domain to the frequency domain. You remove the indices of time and replace it with indices of frequency. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"Remember our initial starting point: covariance = non-normalized correlation. We are seeing how much our data correlates with a certain frequency. But there is still an annoying part: the complex term with the funny i number in it. Correlation or covariance values need to be real numbers, not imaginary. To resolve this dilemma, we can calculate the magnitude of complex numbers. This would take care of the complex part and give us a real number. In your computer, this is defined as the abs function, corresponding to absolute value. In general:","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"z = a + ib \n\nabs(z) = z=sqrta^2+b^2","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"Finally, we can remove the annoying square root symbol by taking the square of the absolute value. If the square root of something is big, then something got to be big as well. Why bother with square rooting it. These series of handwaving arguments finally lead us to the definition of the power spectrum: the squared magnitude of the fourier transform.","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"textrmPSD(tildex(omega)) = tildex(omega)^2","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"Clearly, the power spectrum is a function of frequency omega. For every value of omega, we have one covariance number. The natural way to represent this is to put the frequencies on the x axis and the covariance values on the y axis. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"Why did we bother with all this. Remember our initial problem. We want to take out the frequency from the rest of the signal so that our timescale estimates are clean. The Fourier transform does exactly that. By moving from time domain to frequency domain, we can pinpoint the oscillations. The power spectrum shows exactly that. IntrinsicTimescales.jl has a bunch of functions to do this: comp_psd for periodogram, comp_psd_adfriendly is an automatic-differentiation friendly version of this (which we'll use in the next section) and comp_psd_lombscargle takes care of the missing data points using Lomb-Scargle periodogram if your data has missing values in it represented by NaNs. In practice, acw function wraps all these and picks the appropriate one for you. Let's plot the two power spectra for our data with and without oscillations. The third plot shows the average across trials. I'll plot on log scale, it is usually easier on the eyes for power spectra. Feel free to remove the scale=:log10 to see them raw. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"psd, freqs = comp_psd(data, fs)\npsd_osc, freqs = comp_psd(data_osc, fs)\np1 = plot(freqs, psd', scale=:log10, label=\"\", title=\"Without oscillation\")\nxlims!(p1, (1.0, 50.0)) # limit to frequencies between 1 and 50 Hz\np2 = plot(freqs, psd_osc', scale=:log10, label=\"\", title=\"With oscillation\")\nxlims!(p2, (1.0, 50.0))\np3 = plot(freqs, mean(psd', dims=2), scale=:log10, \n          linewidth=3, label=\"Without oscillations\", legend_position=:bottomleft)\nplot!(freqs, mean(psd_osc', dims=2), scale=:log10, \n      linewidth=3, label=\"With oscillations\", legend_position=:bottomleft)\nxlims!(p3, (1.0, 50.0))\nvline!(p3, [10.0], color=:black, linewidth=1, label=\"\", linestyle=:dash)\nplot(p1, p2, p3, size=(1200, 400), layout=(1, 3))","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"(Image: )","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"See that huge peak? The correlation with a 10 Hz oscillation and our data is high because our data has a 10 Hz oscillation. That's nice. But what does it have to do with timescales? ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"It is not a coincidence that both autocorrelation and power spectrum are defined in terms of correlations. Correlation with your data with itself and correlation of your data with sinusoids. It would be really nice if there was a way to connect them. Thankfully, there is. Wiener-Khinchin theorem states that the inverse Fourier transform of a power spectrum is the autocorrelation function. Which also means that the Fourier transform of an autocorrelation function is the power spectrum. Showing this is not too difficult but we already did too much math for a documentation page. For the curious reader, I'll link this nice and short proof. Nonetheless, take a look at the source code of the function comp_ac_fft, you'll see this theorem in action. Because Fourier transform on a computer is super fast (hence the name: Fast Fourier Transform, FFT), this function is way faster than comp_ac_time which calculates the ACF by shifting the time-series again and again and calculating correlation. And the really nice thing is the two functions are not approximations of each other. They are exactly the same, up to a floating point error on the order of 10^-16. You can see this in one of the test files for the package. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"How can we utilize this information to get the timescale? Let's assume that our ACF is an exponential decay and calculate its Fourier transform. This is not an easy integral, I won't explain how to do it here (a.k.a. I will use the technique of integration by trust me bro). A nice explanation is given here. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"textrmACF(l) = e^-fracltau \n\ntildetextrmACF(omega) = textrmPSD(omega) = fracAk^2 + omega^2 \n\nk = frac12 pi tau \n\ntau = frac12 pi k","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"fracAa^2 + omega^2 is called a Lorentzian. The A term upstairs is just a normalization factor, not particularly interesting. omega is our usual frequency. We also see that the timescale tau is in there as well. This means that we need only to fit this Lorentzian to our power spectrum and we have access to timescale tau. To my knowledge, this technique was first introduced in Chaudhuri et al., 2017 and further developed in Gao et al., 2020 to cover the cases where the exponent of omega downstairs is not exactly 2. Since version 0.3.0, IntrinsicTimescales.jl supports both methods via the keyword  argument allow_variable_exponent. The default behavior for this is false, which sets the exponent to 2. This should cover most cases. If you are getting funny results, I recommend setting allow_variable_exponent=true (e.g. acw(data, fs; acwtypes=:knee, allow_variable_exponent=true)). Yet another thing to note is the issue of constraints. In general, unconstrained problems, i.e. the problems where the parameters (for example the knee frequency) can take any possible value are solved more effectively. However, for hard problems, this can lead to the algorithm cheating by finding parameter combinations that are physically unrealistic (for example, negative frequencies for the knee frequency). The way to avoid this is by using the keyword argument constrained. When this is set to true, IntrinsicTimescales.jl switches the algorithm and the parameters are bounded in a realistic domain. See more details at acw. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"Some interesting remarks. Consider very low frequencies: omega ll k. When omega is small, then omega^2 will be much smaller compared to k^2 (think of if 2 is smaller than 3, then 2^2=4 is much smaller than 3^2=9). Then we can ignore the omega term and our PSD reduces to fracAk^2 which is just a constant. Now consider big frequencies: omega gg k. Since omega is big, omega^2 is now much bigger than k and we can ignore k. Then the power spectrum is fracAomega^2. This is the so-called scale-free power spectrum with a power-law exponent (PLE) of 2. If we take the logarithm, logfracAomega^2 sim -2omega meaning on the log-scale, we should see a straight line with a slope of 2. Right in between, there is a transition from a flat PSD to PLE=2 PSD. This is where k is approximately equal to omega. Alternatively, the frequency between the flat part and the PLE=2 part is the knee frequency and also corresponds to your timescale up to a multiplicative constant. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"In IntrinsicTimescales.jl, you can use :knee in the acwtypes argument of acw to calculate the INT from the knee frequency. In the code below, I'll show this and plot the PSDs on the log scale to visually show the knee frequency. The function knee_from_tau converts the timescale to the knee frequency. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"acwresults_osc = acw(data_osc, fs; acwtypes=:knee, average_over_trials=true)\nacwresults = acw(data, fs; acwtypes=:knee, average_over_trials=true)\nprintln(mean(acwresults_osc.acw_results))\n# 0.05\nprintln(mean(acwresults.acw_results))\n# 0.046\np1 = acwplot(acwresults_osc)\ntitle!(p1, \"ACF with oscillatory component\")\np2 = acwplot(acwresults)\ntitle!(p2, \"ACF\")\nvline!(p1, [knee_from_tau(acwresults_osc.acw_results)], color=:black, linewidth=1, label=\"Knee frequency\", linestyle=:dash)\nvline!(p2, [knee_from_tau(acwresults.acw_results)], color=:black, linewidth=1, label=\"Knee frequency\", linestyle=:dash)\nplot(p1, p2, size=(800, 500))","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"(Image: )","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"We're almost there. In the final chapter, we will cover Bayesian estimation of INTs and finish the Practice part of the documentation. ","category":"page"},{"location":"practice/practice_4_psd/#Bonus:-Why-does-complex-numbers-in-Fourier-transform-relate-to-phase?","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Bonus: Why does complex numbers in Fourier transform relate to phase?","text":"","category":"section"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"I felt guilty after waving my hands too much when trying to convince you that we really need complex numbers to get the phase information. For the curious, here is a proper explanation. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"Consider the cosine wave g(t)=A cos(ft+phi) with amplitude A, frequency f and phase shift phi. We'll take the Fourier transform of this guy. Let's start with writing it as in exponential form because dealing with trigonometric functions is annoying. Using Euler's formula:","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"textrmEulers Formula e^ix = cosx + i sinx \n\nA cos(ft+phi) = fracA2(e^i(ft+phi)+e^-i(ft+phi)) ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"Now let's take the Fourier transform on the exponential notation. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"tildeg(omega) = int_-infty^inftyg(t)e^-iomega tdt \n\n= fracA2 ( int_-infty^inftye^i(ft+phi)e^-iomega tdt + \nint_-infty^inftye^-i(ft+phi)e^-iomega tdt ) \n\n= fracA2 (e^i phi int_-infty^inftye^t(if-iomega)dt + \ne^-i phi int_-infty^inftye^t(-if-iomega)dt) ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"Integrating complex functions is a messy business but it suffices to know that the integral of a complex exponential is a delta function. See this stackexchange answer for a really nice explanation. If my intuition is right (no guarantees), this is related to the orthogonality of Fourier components. Writing down the solution to the integrals above:","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"tildeg(omega) = fracA2(2 pi delta(omega-f)e^i phi+2 pi delta(omega+f)e^-i phi) \n\n= A pi (delta(omega-f)e^i phi+delta(omega+f)e^-i phi)","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"What this tells us is that in the Fourier domain, there is one peak at the frequency f (from delta(omega-f)) and another peak at the negative frequency -f (from delta(omega+f)). Negative frequencies are not particularly interesting because if your signal is real valued, then the negative and positive parts of the Fourier domain will be symmetrical. The phase information is also encoded in e^i phi and e^-i phi. Now let's try to do the same without using complex exponentials. I used the notation tildeg for Fourier transform. I'll use hatg here to denote that this isn't exactly the Fourier transform but something we cooked up. As a matter of fact, we didn't cook this up, Joseph Fourier used to do his transforms with real functions. We are going back in time and using the old technique. ","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"hatg(omega) = int_-infty^inftycos(ft+phi) cos(omega t) dt","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"This is a very difficult integral. For me, the definition of very difficult is Mathematica can't solve it. To simplify, let's use Euler's formula:","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"cos(ft+phi) cos(omega t) = left( frac12(e^i(ft+phi)+e^-i(ft+phi)) right)\nleft( frac12 (e^i omega t + e^-i omega t) right) \n\n= frac14 left(e^-i (f t+phi )-i t omega+e^i t omega-i (f t+phi )+e^i (f t+phi )-i t omega+e^i (f t+phi )+i t omega right)","category":"page"},{"location":"practice/practice_4_psd/","page":"Dealing with Oscillatory Artifacts using Fourier Transformation","title":"Dealing with Oscillatory Artifacts using Fourier Transformation","text":"And we went back to the complex exponentials. It seems there is no escape from them. Might as well start directly with a complex exponential and save us the trouble. And now I can sleep with peacefully. ","category":"page"},{"location":"fit_parameters/#Model-Fitting-and-Parameters","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"","category":"section"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"All models are fit with int_fit function and return ADVIResults or ABCResults type. The int_fit function has the following signature:","category":"page"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"results = int_fit(model, param_dict=nothing)","category":"page"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"The function determines the inference method based on model attributes which the user provides when initiating the model. When param_dict is not provided, the function uses the default parameters for the inference method, which can be seen with get_param_dict_advi and get_param_dict_abc functions. ","category":"page"},{"location":"fit_parameters/#Parameters-for-Approximate-Bayesian-Computation-(ABC)","page":"Model Fitting and Parameters","title":"Parameters for Approximate Bayesian Computation (ABC)","text":"","category":"section"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"The parameters of ABC can be accessed and modified through the get_param_dict_abc() function. ","category":"page"},{"location":"fit_parameters/#General-ABC-Parameters","page":"Model Fitting and Parameters","title":"General ABC Parameters","text":"","category":"section"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"epsilon_0::Float64 = 1.0: Initial acceptance threshold. If the distance between the observed data and the simulated data is less than epsilon_0, the sample is accepted in the initial step of ABC. Subsequent steps change the epsilon value to adapt better. \nmax_iter::Int = 10000: Maximum number of iterations per basic ABC step\nmin_accepted::Int = 100: The number of accepted samples for basic ABC\nsteps::Int = 30: Number of PMC steps to perform\nsample_only::Bool = false: If true, only perform sampling without adaptation between basic ABC runs","category":"page"},{"location":"fit_parameters/#Epsilon-Selection-Parameters","page":"Model Fitting and Parameters","title":"Epsilon Selection Parameters","text":"","category":"section"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"Different from the Zeraati et al. (2022) method, we adaptively change the epsilon value between basic ABC steps. The epsilon selection procedure adaptively adjusts the acceptance threshold based on the current acceptance rate and distance distribution. The procedure works as follows:","category":"page"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"First, invalid distances (NaN values and distances above distance_max) are filtered out.\nThree quantiles are computed from the valid distances:\nLower quantile (quantile_lower) for conservative threshold\nInitial quantile (quantile_init) for first iteration\nUpper quantile (quantile_upper) for relaxed threshold\nAn adaptive alpha value is computed based on:\nProgress through iterations (iteration/totaliterations) to decay from alphamax to alpha_min\nDifference between current and target acceptance rates:\nIf difference > accratefar: Alpha increases to min(alphamax, basealpha * alphafarmult)\nIf difference < accrateclose: Alpha decreases to max(alphamin, basealpha * alphaclosemult) \nOtherwise: Uses base alpha from iteration progress\nThe new epsilon is then selected:\nFor first iteration: Uses the initial quantile\nFor subsequent iterations:\nIf acceptance rate is too high: Epsilon is set to the maximum of the lower quantile and epsilon * (1-alpha)\nIf acceptance rate is too low: Epsilon is set to the minimum of the upper quantile and epsilon * (1+alpha) \nIf acceptance rate is within buffer of target: Epsilon stays same","category":"page"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"This adaptive procedure helps balance exploration and exploitation during the ABC sampling process by sampling wider for initial steps and narrowing down as the algorithm converges. ","category":"page"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"Parameters controlling epsilon selection:","category":"page"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"target_acc_rate::Float64 = 0.01: Targeted acceptance rate for epsilon adaptation\ndistance_max::Float64 = 10.0: Maximum distance to consider valid\nquantile_lower::Float64 = 25.0: Lower quantile for epsilon adjustment\nquantile_upper::Float64 = 75.0: Upper quantile for epsilon adjustment\nquantile_init::Float64 = 50.0: Initial quantile when no acceptance rate\nacc_rate_buffer::Float64 = 0.1: Buffer around target acceptance rate","category":"page"},{"location":"fit_parameters/#Adaptive-Alpha-Parameters","page":"Model Fitting and Parameters","title":"Adaptive Alpha Parameters","text":"","category":"section"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"alpha_max::Float64 = 0.9: Maximum adaptation rate\nalpha_min::Float64 = 0.1: Minimum adaptation rate\nacc_rate_far::Float64 = 2.0: Threshold for \"far from target\" adjustment\nacc_rate_close::Float64 = 0.2: Threshold for \"close to target\" adjustment\nalpha_far_mult::Float64 = 1.5: Multiplier for alpha when far from target\nalpha_close_mult::Float64 = 0.5: Multiplier for alpha when close to target","category":"page"},{"location":"fit_parameters/#Early-Stopping-Parameters","page":"Model Fitting and Parameters","title":"Early Stopping Parameters","text":"","category":"section"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"convergence_window::Int = 3: Number of iterations to check for convergence\ntheta_rtol::Float64 = 1e-2: Relative tolerance for parameter convergence\ntheta_atol::Float64 = 1e-3: Absolute tolerance for parameter convergence\ntarget_epsilon::Float64 = 5e-3: Stop the PMC if the distance between the observed data and the simulated data is less than target_epsilon.\nminAccRate::Float64 = 0.01: If acceptance rate of basic ABC steps is below minAccRate, the algorithm stops.","category":"page"},{"location":"fit_parameters/#Numerical-Stability-Parameters","page":"Model Fitting and Parameters","title":"Numerical Stability Parameters","text":"","category":"section"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"jitter::Float64 = 1e-6: Small value added to covariance diagonal for numerical stability","category":"page"},{"location":"fit_parameters/#Display-Parameters","page":"Model Fitting and Parameters","title":"Display Parameters","text":"","category":"section"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"show_progress::Bool = true: Whether to show progress bar\nverbose::Bool = true: Whether to print detailed information","category":"page"},{"location":"fit_parameters/#MAP-Estimation-Parameters","page":"Model Fitting and Parameters","title":"MAP Estimation Parameters","text":"","category":"section"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"The find_MAP function estimates the maximum a posteriori (MAP) parameters by performing a grid search over the parameter space. It takes the accepted parameters from the final ABC step and creates a grid of N random positions within the parameter bounds. For each parameter dimension, it estimates the probability density using kernel density estimation (KDE) and evaluates the density at the grid positions. The MAP estimate is then determined by finding the position with maximum probability density for each parameter. This provides a point estimate of the most probable parameter values given the posterior samples.","category":"page"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"N::Int = 10000: Number of samples for maximum a posteriori (MAP) estimation grid search","category":"page"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"To modify these parameters, create a dictionary with your desired values and pass it to the fit function:","category":"page"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"model = one_timescale_model(data, time, :abc)\nparam_dict = get_param_dict_abc()\nparam_dict[:convergence_window] = 10\nparam_dict[:max_iter] = 20000\nresults = int_fit(model, param_dict)","category":"page"},{"location":"fit_parameters/#Parameters-for-Automatic-Differentiation-Variational-Inference-(ADVI)","page":"Model Fitting and Parameters","title":"Parameters for Automatic Differentiation Variational Inference (ADVI)","text":"","category":"section"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"ADVI is performed via Turing.jl package. See the variational inference tutorial to learn more about Turing's ADVI implementation. The parameters can be accessed and modified through the get_param_dict_advi() function. ","category":"page"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"n_samples::Int = 4000: Number of posterior samples to draw after fitting\nn_iterations::Int = 50: Number of ADVI optimization iterations. Increase this number if your model is not fitting well.\nn_elbo_samples::Int = 20: Number of samples used to estimate the ELBO (Evidence Lower BOund) during optimization. Increase this number if your model is not fitting well.\nautodiff = AutoForwardDiff(): The automatic differentiation backend to use for computing gradients. Currently, only AutoForwardDiff() is supported.","category":"page"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"To modify these parameters, create a dictionary with your desired values and pass it to the fit function:","category":"page"},{"location":"fit_parameters/","page":"Model Fitting and Parameters","title":"Model Fitting and Parameters","text":"model = one_timescale_model(data, time, :advi)\nparam_dict = get_param_dict_advi()\nparam_dict[:n_samples] = 8000\nparam_dict[:n_elbo_samples] = 60\nresults = int_fit(model, param_dict)","category":"page"},{"location":"","page":"API","title":"API","text":"Pages = [\"API.md\"]","category":"page"},{"location":"#Functions","page":"API","title":"Functions","text":"","category":"section"},{"location":"","page":"API","title":"API","text":"Modules = [IntrinsicTimescales, IntrinsicTimescales.Models, IntrinsicTimescales.ABC, IntrinsicTimescales.ACW, IntrinsicTimescales.TuringBackend, IntrinsicTimescales.SummaryStats, \n    IntrinsicTimescales.Distances, IntrinsicTimescales.Utils, IntrinsicTimescales.OrnsteinUhlenbeck, IntrinsicTimescales.OneTimescale, \n    IntrinsicTimescales.OneTimescaleAndOsc, IntrinsicTimescales.OneTimescaleWithMissing, \n    IntrinsicTimescales.OneTimescaleAndOscWithMissing, IntrinsicTimescales.Plotting]\npages = [\"IntrinsicTimescales.jl\", [\"core/model.jl\", \"core/one_timescale.jl\", \"core/one_timescale_and_osc.jl\", \"core/one_timescale_with_missing.jl\", \"core/one_timescale_and_osc_with_missing.jl\"], \"core/abc.jl\", \"core/turing_backend.jl\", \"stats/summary.jl\", \"stats/distances.jl\", \"utils/utils.jl\", \"utils/ou_process.jl\", \"core/plotting.jl\"]\nprivate = false","category":"page"},{"location":"#IntrinsicTimescales.IntrinsicTimescales","page":"API","title":"IntrinsicTimescales.IntrinsicTimescales","text":"IntrinsicTimescales\n\nA Julia package for estimation of timescales from time series data.\n\nFeatures\n\nStandard techniques for INT calculation \nApproximate Bayesian Computation (ABC) for parameter inference\nADVI for variational inference\nMultiple model types:\nSingle timescale\nSingle timescale with oscillations\nModels supporting missing data\nSummary statistics using periodogram, Welch (from DSP.jl) and Lomb-Scargle (from LombScargle.jl):\nAutocorrelation function (ACF)\nPower spectral density (PSD)\n\nSubmodules\n\nModels: Abstract model types and interfaces\nABC: Approximate Bayesian Computation algorithms\nTuringBackend: Turing.jl integration for ADVI\nSummaryStats: ACF and PSD implementations\nDistances: Distance metrics for ABC\nUtils: Utility functions for analysis\nOrnsteinUhlenbeck: OU process generation using DifferentialEquations.jl\nOneTimescale: Single timescale model\nOneTimescaleAndOsc: Single timescale with oscillations\nOneTimescaleWithMissing: Single timescale with missing data\nOneTimescaleAndOscWithMissing: Single timescale and oscillations with missing data\nPlotting: Plotting functions for results\n\n\n\n\n\n","category":"module"},{"location":"#IntrinsicTimescales.Models.AbstractTimescaleModel","page":"API","title":"IntrinsicTimescales.Models.AbstractTimescaleModel","text":"AbstractTimescaleModel\n\nAbstract type representing models for timescale inference. All concrete model implementations should subtype this.\n\n\n\n\n\n","category":"type"},{"location":"#IntrinsicTimescales.Models.BaseModel","page":"API","title":"IntrinsicTimescales.Models.BaseModel","text":"BaseModel <: AbstractTimescaleModel\n\nBase model structure for timescale inference using various methods.\n\nFields\n\ndata: Input time series data\ntime: Time points corresponding to the data\ndata_sum_stats: Pre-computed summary statistics of the data\nfitmethod::Symbol: Fitting method to use. Options: :abc, :advi, :acw\nsummary_method::Symbol: Summary statistic type. Options: :psd (power spectral density) or :acf (autocorrelation)\nlags_freqs::AbstractVector{<:Real}: Lags (for ACF) or frequencies (for PSD) at which to compute summary statistics\nprior: Prior distributions for parameters. Can be Vector{Distribution}, single Distribution, or \"informed_prior\"\nacwtypes::Union{Vector{Symbol}, Symbol}: ACW analysis types (e.g., :ACW50, :ACW0, :ACWe, :tau, :knee)\ndistance_method::Symbol: Distance metric type. Options: :linear or :logarithmic\ndt::Real: Time step between observations\nT::Real: Total time span of the data\nnumTrials::Real: Number of trials/iterations\ndata_mean::Real: Mean of the input data\ndata_sd::Real: Standard deviation of the input data\n\n\n\n\n\n","category":"type"},{"location":"#IntrinsicTimescales.Models.check_acwtypes-Tuple{Any, Any}","page":"API","title":"IntrinsicTimescales.Models.check_acwtypes","text":"check_acwtypes(acwtypes, possible_acwtypes)\n\nValidate the ACW analysis types against allowed options.\n\nArguments\n\nacwtypes: Symbol or Vector of Symbols specifying ACW analysis types\npossible_acwtypes: Vector of allowed ACW analysis types\n\nReturns\n\nValidated vector of ACW types\n\nThrows\n\nErrorException: If invalid ACW types are provided\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Models.check_inputs-Tuple{Any, Any}","page":"API","title":"IntrinsicTimescales.Models.check_inputs","text":"check_inputs(fitmethod, summary_method)\n\nValidate the fitting method and summary statistic choices.\n\nArguments\n\nfitmethod: Symbol specifying the fitting method\nsummary_method: Symbol specifying the summary statistic type\n\nThrows\n\nArgumentError: If invalid options are provided\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Models.check_model_inputs","page":"API","title":"IntrinsicTimescales.Models.check_model_inputs","text":"check_model_inputs(data, time, fit_method, summary_method, prior, acwtypes, distance_method)\n\nValidate inputs for timescale model construction.\n\nArguments\n\ndata: Input time series data\ntime: Time points corresponding to the data\nfit_method: Fitting method (:abc, :advi)\nsummary_method: Summary statistic type (:psd or :acf)\nprior: Prior distribution(s) for parameters\nacwtypes: Types of ACW analysis\ndistance_method: Distance metric type (:linear or :logarithmic)\n\nThrows\n\nArgumentError: If any inputs are invalid or incompatible\n\n\n\n\n\n","category":"function"},{"location":"#IntrinsicTimescales.Models.distance_function","page":"API","title":"IntrinsicTimescales.Models.distance_function","text":"distance_function(model::AbstractTimescaleModel, summary_stats, summary_stats_synth)\n\nCompute distance between two sets of summary statistics.\n\nArguments\n\nmodel: Model instance\nsummary_stats: First set of summary statistics\nsummary_stats_synth: Second set of summary statistics (typically from synthetic data)\n\nReturns\n\nDistance value according to model.distance_method\n\n\n\n\n\n","category":"function"},{"location":"#IntrinsicTimescales.Models.draw_theta","page":"API","title":"IntrinsicTimescales.Models.draw_theta","text":"draw_theta(model::AbstractTimescaleModel)\n\nDraw parameter values from the model's prior distributions.\n\nReturns\n\nArray of proposed model parameters sampled from their respective priors\n\n\n\n\n\n","category":"function"},{"location":"#IntrinsicTimescales.Models.generate_data","page":"API","title":"IntrinsicTimescales.Models.generate_data","text":"generate_data(model::AbstractTimescaleModel, theta)\n\nGenerate synthetic data using the forward model with given parameters.\n\nArguments\n\nmodel: Model instance\ntheta: Array of model parameters\n\nReturns\n\nSynthetic dataset with same structure as the original data\n\n\n\n\n\n","category":"function"},{"location":"#IntrinsicTimescales.Models.generate_data_and_reduce-Tuple{AbstractTimescaleModel, Any}","page":"API","title":"IntrinsicTimescales.Models.generate_data_and_reduce","text":"generate_data_and_reduce(model::AbstractTimescaleModel, theta)\n\nCombined function to generate synthetic data and compute distance from observed data. This is a convenience function commonly used in ABC algorithms.\n\nArguments\n\nmodel: Model instance\ntheta: Array of model parameters\n\nReturns\n\nDistance value between synthetic and observed summary statistics\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Models.int_fit","page":"API","title":"IntrinsicTimescales.Models.int_fit","text":"fit(model::AbstractTimescaleModel, param_dict=nothing)\n\nFit the timescale model using the specified fitting method.\n\nArguments\n\nmodel: The timescale model instance to fit\nparam_dict: Optional dictionary of fitting parameters. If not provided, default parameters will be used.\n\nReturns\n\nFor ADVI fitting method:\n\nsamples: Array of posterior samples\nmap_estimate: Maximum a posteriori estimate of parameters\nvi_result: Full variational inference result object\n\nFor ABC fitting method:\n\nsamples: Array of accepted parameter samples\nweights: Importance weights for the samples\ndistances: Distances between simulated and observed summary statistics\n\n\n\n\n\n","category":"function"},{"location":"#IntrinsicTimescales.Models.summary_stats","page":"API","title":"IntrinsicTimescales.Models.summary_stats","text":"summary_stats(model::AbstractTimescaleModel, data)\n\nCompute summary statistics (PSD or ACF) from the data.\n\nArguments\n\nmodel: Model instance\ndata: Input data (original or synthetic)\n\nReturns\n\nArray of summary statistics computed according to model.summary_method\n\n\n\n\n\n","category":"function"},{"location":"#IntrinsicTimescales.ABC.ABCResults","page":"API","title":"IntrinsicTimescales.ABC.ABCResults","text":"ABCResults\n\nContainer for ABC results to standardize plotting interface.\n\nFields\n\ntheta_history::Vector{Matrix{Float64}}: History of parameter values across iterations\nepsilon_history::Vector{Float64}: History of epsilon values\nacc_rate_history::Vector{Float64}: History of acceptance rates\nweights_history::Vector{Vector{Float64}}: History of weights\nfinal_theta::Matrix{Float64}: Final accepted parameter values\nfinal_weights::Vector{Float64}: Final weights\nMAP::Vector{Float64}: Maximum A Posteriori (MAP) estimate of parameters\n\n\n\n\n\n","category":"type"},{"location":"#IntrinsicTimescales.ABC.abc_results-Tuple{Vector{NamedTuple}}","page":"API","title":"IntrinsicTimescales.ABC.abc_results","text":"abc_results(output_record::Vector{NamedTuple})\n\nConstruct an ABCResults struct from PMC-ABC output records.\n\nArguments\n\noutput_record::Vector{NamedTuple}: Vector of named tuples containing PMC-ABC iteration results.  Each tuple must contain:\ntheta_accepted: Accepted parameter values\nepsilon: Epsilon threshold value\nn_accepted: Number of accepted samples\nn_total: Total number of samples\nweights: Importance weights\n\nReturns\n\nABCResults: Struct to contain ABC results. See the documentation for ABCResults for more details.\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.ABC.basic_abc-Tuple{AbstractTimescaleModel}","page":"API","title":"IntrinsicTimescales.ABC.basic_abc","text":"basic_abc(model::Models.AbstractTimescaleModel; kwargs...)\n\nPerform basic ABC rejection sampling. The algorithm stops either when max_iter is reached or  when min_accepted samples have been accepted.\n\nArguments\n\nmodel::Models.AbstractTimescaleModel: Model to perform inference on\nepsilon::Float64: Acceptance threshold for distance between simulated and observed data\nmax_iter::Integer: Maximum number of iterations to perform\nmin_accepted::Integer: Minimum number of accepted samples required before stopping\npmc_mode::Bool=false: Whether to use PMC proposal distribution instead of prior\nweights::Array{Float64}: Importance weights for PMC sampling (only used if pmc_mode=true)\ntheta_prev::Array{Float64}: Previous parameters for PMC sampling (only used if pmc_mode=true)\ntau_squared::Array{Float64}: Covariance matrix for PMC sampling (only used if pmc_mode=true)\nshow_progress::Bool=true: Whether to show progress bar with acceptance count and speed\n\nReturns\n\nNamedTuple containing:\n\nsamples::Matrix{Float64}: Matrix (maxiter × nparams) of all proposed parameters\nisaccepted::Vector{Bool}: Boolean mask of accepted samples for first n_total iterations\ntheta_accepted::Matrix{Float64}: Matrix (naccepted × nparams) of accepted parameters\ndistances::Vector{Float64}: Vector of distances for first n_total iterations\nn_accepted::Int: Number of accepted samples\nn_total::Int: Total number of iterations performed\nepsilon::Float64: Acceptance threshold used\nweights::Vector{Float64}: Uniform weights (ones) for accepted samples\ntau_squared::Matrix{Float64}: Zero matrix (nparams × nparams) for basic ABC\neff_sample::Int: Effective sample size (equals n_accepted in basic ABC)\n\nImplementation Details\n\nDraws parameters either from prior (basic mode) or PMC proposal (pmc_mode)\nGenerates synthetic data and computes distance to observed data\nAccepts parameters if distance ≤ epsilon\nStops when either maxiter reached or minaccepted samples accepted\nReturns uniform weights and zero covariance matrix in basic mode\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.ABC.calc_weights-Tuple{VecOrMat{Float64}, VecOrMat{Float64}, Matrix{Float64}, Vector{Float64}, Union{Distributions.Distribution, Vector}}","page":"API","title":"IntrinsicTimescales.ABC.calc_weights","text":"calc_weights(theta_prev, theta, tau_squared, weights, prior)\n\nCalculate importance weights for PMC-ABC algorithm.\n\nArguments\n\ntheta_prev::Union{Vector{Float64}, Matrix{Float64}}: Previously accepted parameters. For multiple parameters,  each row is a sample and each column is a parameter\ntheta::Union{Vector{Float64}, Matrix{Float64}}: Current parameters in same format as theta_prev\ntau_squared::Matrix{Float64}: Covariance matrix for the proposal distribution\nweights::Vector{Float64}: Previous iteration's importance weights\nprior::Union{Vector, dist.Distribution}: Prior distribution(s). Can be single distribution or vector of distributions\n\nReturns\n\nVector{Float64}: Normalized importance weights (sum to 1)\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.ABC.compute_adaptive_alpha-Tuple{Integer, Float64, Float64}","page":"API","title":"IntrinsicTimescales.ABC.compute_adaptive_alpha","text":"compute_adaptive_alpha(iteration::Integer, current_acc_rate::Float64, target_acc_rate::Float64; kwargs...)\n\nCompute an adaptive step size (alpha) for epsilon adjustment in ABC, based on iteration progress and distance from target acceptance rate.\n\nArguments\n\nRequired Arguments\n\niteration::Integer: Current iteration number\ncurrent_acc_rate::Float64: Current acceptance rate\ntarget_acc_rate::Float64: Target acceptance rate to achieve\n\nOptional Keyword Arguments\n\nBounds Parameters\n\nalpha_max::Float64=0.9: Maximum allowed alpha value\nalpha_min::Float64=0.1: Minimum allowed alpha value\ntotal_iterations::Integer=100: Total number of iterations planned\n\nAdaptation Parameters\n\nacc_rate_far::Float64=2.0: Relative difference threshold for \"far from target\"\nacc_rate_close::Float64=0.2: Relative difference threshold for \"close to target\"\nalpha_far_mult::Float64=1.5: Multiplier for alpha when far from target\nalpha_close_mult::Float64=0.5: Multiplier for alpha when close to target\n\nReturns\n\nFloat64: Adaptive alpha value between alpha_min and alpha_max\n\nImplementation Details\n\nComputes base alpha using linear decay between max and min:\nbase_alpha = alpha_max * (1 - progress) + alpha_min * progress\nwhere progress = iteration/total_iterations\nAdjusts base alpha based on relative difference from target:\nacc_rate_diff = |current_acc_rate - target_acc_rate|/target_acc_rate\nFinal alpha selection:\nIf acc_rate_diff > acc_rate_far: More aggressive adaptation alpha = min(alpha_max, base_alpha * alpha_far_mult)\nIf acc_rate_diff < acc_rate_close: More conservative adaptation alpha = max(alpha_min, base_alpha * alpha_close_mult)\nOtherwise: Use base alpha alpha = base_alpha\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.ABC.draw_theta_pmc-NTuple{4, Any}","page":"API","title":"IntrinsicTimescales.ABC.draw_theta_pmc","text":"draw_theta_pmc(model, theta_prev, weights, tau_squared; jitter::Float64=1e-5)\n\nDraw new parameter values using the PMC proposal distribution.\n\nArguments\n\nmodel: Model instance\ntheta_prev: Previously accepted parameters\nweights: Importance weights from previous iteration\ntau_squared: Covariance matrix for proposal distribution\njitter::Float64=1e-5: Small value added to covariance diagonal for numerical stability\n\nReturns\n\nVector of proposed parameters\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.ABC.effective_sample_size-Tuple{Vector{Float64}}","page":"API","title":"IntrinsicTimescales.ABC.effective_sample_size","text":"effective_sample_size(w::Vector{Float64})\n\nCalculate effective sample size (ESS) from importance weights. \n\nArguments\n\nw::Vector{Float64}: Vector of importance sampling weights (need not be normalized)\n\nReturns\n\nFloat64: Effective sample size\n\nDetails\n\nThe effective sample size is always less than or equal to the actual number of samples. It reaches its maximum (equal to sample size) when all weights are equal, and approaches its minimum (1) when one weight dominates all others.\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.ABC.find_MAP","page":"API","title":"IntrinsicTimescales.ABC.find_MAP","text":"find_MAP(theta_accepted::AbstractArray{Float64}, N::Integer=10000)\n\nFind Maximum A Posteriori (MAP) estimates with grid search.\n\nArguments\n\ntheta_accepted::AbstractArray{Float64}: Matrix of accepted samples from the final step of ABC\nN::Integer=10000: Number of random grid points to evaluate for each parameter\n\nReturns\n\ntheta_map::Vector{Float64}: MAP estimate of the parameters\n\n\n\n\n\n","category":"function"},{"location":"#IntrinsicTimescales.ABC.get_param_dict_abc-Tuple{}","page":"API","title":"IntrinsicTimescales.ABC.get_param_dict_abc","text":"get_param_dict_abc()\n\nGet default parameter dictionary for ABC algorithm.\n\nReturns\n\nDictionary containing default values for all ABC parameters including:\n\nBasic ABC Parameters\n\n:epsilon_0 => 1.0: Initial epsilon threshold\n:max_iter => 10000: Maximum iterations per step\n:min_accepted => 100: Minimum number of accepted samples\n:steps => 30: Maximum PMC steps\n:sample_only => false: If true, only perform sampling without adaptation\n\nAcceptance Rate Parameters\n\n:minAccRate => 0.01: Minimum acceptance rate before early stopping\n:target_acc_rate => 0.01: Target acceptance rate\n:target_epsilon => 1e-4: Target epsilon for early stopping\n\nDisplay Parameters\n\n:show_progress => true: Show progress bar\n:verbose => true: Print detailed progress information\n\nNumerical Stability Parameters\n\n:jitter => 1e-6: Small value added to covariance matrix\n\nEpsilon Selection Parameters\n\n:distance_max => 10.0: Maximum valid distance\n:quantile_lower => 25.0: Lower quantile for epsilon bounds\n:quantile_upper => 75.0: Upper quantile for epsilon bounds\n:quantile_init => 50.0: Initial quantile\n:acc_rate_buffer => 0.1: Allowed deviation from target rate\n\nAdaptive Alpha Parameters\n\n:alpha_max => 0.9: Maximum adaptation rate\n:alpha_min => 0.1: Minimum adaptation rate\n:acc_rate_far => 2.0: Threshold for \"far from target\"\n:acc_rate_close => 0.2: Threshold for \"close to target\"\n:alpha_far_mult => 1.5: Multiplier when far from target\n:alpha_close_mult => 0.5: Multiplier when close to target\n\nEarly Stopping Parameters\n\n:convergence_window => 5: Steps to check for convergence\n:theta_rtol => 1e-2: Relative tolerance for convergence\n:theta_atol => 1e-3: Absolute tolerance for convergence\n\nMAP Estimation Parameters\n\n:N => 10000: Number of grid points for MAP estimation\n\nExample\n\nparams = get_param_dict_abc()\nparams[:epsilon_0] = 0.5  # Modify initial epsilon\nparams[:max_iter] = 5000  # Reduce maximum iterations\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.ABC.pmc_abc-Tuple{AbstractTimescaleModel}","page":"API","title":"IntrinsicTimescales.ABC.pmc_abc","text":"pmc_abc(model::Models.AbstractTimescaleModel; kwargs...)\n\nPerform Population Monte Carlo Approximate Bayesian Computation (PMC-ABC) inference. \n\nArguments\n\nBasic ABC parameters\n\nmodel::Models.AbstractTimescaleModel: Model to perform inference on\nepsilon_0::Real=1.0: Initial epsilon threshold for acceptance\nmax_iter::Integer=10000: Maximum number of iterations per step\nmin_accepted::Integer=100: Minimum number of accepted samples required\nsteps::Integer=10: Maximum number of PMC steps to perform\nsample_only::Bool=false: If true, only perform sampling without adaptation\n\nAcceptance rate parameters\n\nminAccRate::Float64=0.01: Minimum acceptance rate before early stopping\ntarget_acc_rate::Float64=0.01: Target acceptance rate for epsilon adaptation\ntarget_epsilon::Float64=5e-3: Target epsilon value for early stopping\n\nDisplay parameters\n\nshow_progress::Bool=true: Whether to show progress bar\nverbose::Bool=true: Whether to print detailed progress information\n\nNumerical stability parameters\n\njitter::Float64=1e-6: Small value added to covariance matrix for stability\n\nEpsilon selection parameters\n\ndistance_max::Float64=10.0: Maximum distance to consider valid\nquantile_lower::Float64=25.0: Lower quantile for epsilon adjustment\nquantile_upper::Float64=75.0: Upper quantile for epsilon adjustment\nquantile_init::Float64=50.0: Initial quantile when no acceptance rate\nacc_rate_buffer::Float64=0.1: Buffer around target acceptance rate\n\nAdaptive alpha parameters\n\nalpha_max::Float64=0.9: Maximum adaptation rate\nalpha_min::Float64=0.1: Minimum adaptation rate\nacc_rate_far::Float64=2.0: Threshold for \"far from target\" adjustment\nacc_rate_close::Float64=0.2: Threshold for \"close to target\" adjustment\nalpha_far_mult::Float64=1.5: Multiplier for alpha when far from target\nalpha_close_mult::Float64=0.5: Multiplier for alpha when close to target\n\nEarly stopping parameters\n\nconvergence_window::Integer=3: Number of steps to check for convergence\ntheta_rtol::Float64=1e-2: Relative tolerance for parameter convergence\ntheta_atol::Float64=1e-3: Absolute tolerance for parameter convergence\n\nReturns\n\nABCResults: A struct containing:\n\nABCResults.theta_history: Parameter value history across iterations\nABCResults.epsilon_history: Epsilon value history \nABCResults.acc_rate_history: Acceptance rate history\nABCResults.weight_history: Weight history\nABCResults.theta_final: Final parameter values\nABCResults.weights_final: Final weights\nABCResults.theta_map: MAP estimate\n\nEarly Stopping Conditions\n\nThe algorithm stops and returns results if any of these conditions are met:\n\nAcceptance rate falls below minAccRate\nParameters converge within tolerances over convergence_window steps\nEpsilon falls below target_epsilon\nMaximum number of steps reached\n\nImplementation Details\n\nFirst step uses basic ABC with prior sampling\nSubsequent steps use PMC proposal with adaptive epsilon\nEpsilon is adjusted based on acceptance rates and distance quantiles\nCovariance and weights are updated each step unless sample_only=true\nParameter convergence is checked using both relative and absolute tolerances\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.ABC.select_epsilon-Tuple{Vector{Float64}, Float64}","page":"API","title":"IntrinsicTimescales.ABC.select_epsilon","text":"select_epsilon(distances::Vector{Float64}, current_epsilon::Float64; kwargs...)\n\nAdaptively select the epsilon threshold for ABC based on acceptance rates and distance distribution. Uses a combination of quantile-based bounds and adaptive step sizes to adjust epsilon towards achieving the target acceptance rate.\n\nArguments\n\nRequired Arguments\n\ndistances::Vector{Float64}: Vector of distances from ABC simulations\ncurrent_epsilon::Float64: Current epsilon threshold value\n\nOptional Keyword Arguments\n\nAcceptance Rate Parameters\n\ntarget_acc_rate::Float64=0.01: Target acceptance rate to achieve\ncurrent_acc_rate::Float64=0.0: Current acceptance rate\nacc_rate_buffer::Float64=0.1: Allowed deviation from target acceptance rate\n\nIteration Parameters\n\niteration::Integer=1: Current iteration number\ntotal_iterations::Integer=100: Total number of iterations planned\n\nDistance Processing Parameters\n\ndistance_max::Float64=10.0: Maximum valid distance (larger values filtered out)\nquantile_lower::Float64=25.0: Lower quantile for epsilon bounds\nquantile_upper::Float64=75.0: Upper quantile for epsilon bounds\nquantile_init::Float64=50.0: Initial quantile for first iteration\n\nAdaptive Step Size Parameters\n\nalpha_max::Float64=0.9: Maximum adaptation rate\nalpha_min::Float64=0.1: Minimum adaptation rate\nacc_rate_far::Float64=2.0: Threshold for \"far from target\" adjustment\nacc_rate_close::Float64=0.2: Threshold for \"close to target\" adjustment\nalpha_far_mult::Float64=1.5: Multiplier for alpha when far from target\nalpha_close_mult::Float64=0.5: Multiplier for alpha when close to target\n\nReturns\n\nFloat64: New epsilon value\n\nImplementation Details\n\nFilters out NaN and distances larger than distance_max\nComputes quantile-based bounds for epsilon adjustment\nUses adaptive alpha value based on iteration and acceptance rate (see compute_adaptive_alpha)\nFor first iteration (iteration=1):\nReturns initial quantile of valid distances\nFor subsequent iterations:\nIf acceptance rate too high: decreases epsilon by (1-alpha)\nIf acceptance rate too low: increases epsilon by (1+alpha)\nKeeps epsilon unchanged if within buffer of target rate\nAlways constrains new epsilon between quantile bounds\n\nNotes\n\nReturns current epsilon if no valid distances are found\nUses computeadaptivealpha for step size calculation\nAdjustments are proportional to distance from target acceptance rate\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.ABC.weighted_covar-Tuple{Matrix{Float64}, Vector{Float64}}","page":"API","title":"IntrinsicTimescales.ABC.weighted_covar","text":"weighted_covar(x::Matrix{Float64}, w::Vector{Float64})\n\nCalculate weighted covariance matrix.\n\nArguments\n\nx::Matrix{Float64}: Matrix of observations where each row is an observation and each column is a variable\nw::Vector{Float64}: Vector of weights corresponding to each observation (row of x)\n\nReturns\n\nMatrix{Float64}: Weighted covariance matrix of size (nvariables × nvariables)\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.ACW","page":"API","title":"IntrinsicTimescales.ACW","text":"ACW\n\nModule providing autocorrelation width (ACW) calculations for time series analysis, including:\n\nACW-0 (zero-crossing)\nACW-50 (50% decay)\nACW-euler (1/e decay)\nExponential decay timescale (tau)\nKnee frequency estimation\n\n\n\n\n\n","category":"module"},{"location":"#IntrinsicTimescales.ACW.ACWResults","page":"API","title":"IntrinsicTimescales.ACW.ACWResults","text":"ACWResults\n\nStructure holding ACW analysis inputs and results.\n\nFields\n\nfs::Real: Sampling frequency\nacw_results: Computed ACW values (type depends on number of ACW types requested)\nacwtypes::Union{Vector{<:Symbol}, Symbol}: Types of ACW computed\nn_lags::Union{Int, Nothing}: Number of lags used for ACF calculation\nfreqlims::Union{Tuple{Real, Real}, Nothing}: Frequency limits used for spectral analysis\nacf::Union{AbstractVector{<:Real}, AbstractArray{<:Real}, Nothing}: Autocorrelation function\npsd::Union{AbstractVector{<:Real}, AbstractArray{<:Real}, Nothing}: Power spectral density\nfreqs::Union{AbstractVector{<:Real}, AbstractArray{<:Real}, Nothing}: Frequency vector for PSD\nlags::Union{AbstractVector{<:Real}, AbstractArray{<:Real}, Nothing}: Lag vector for ACF\nx_dim::Union{Int, Nothing}: Dimension index corresponding to x-axis (lags/freqs)\n\nNotes\n\nSupported ACW types: :acw0, :acw50, :acweuler, :auc, :tau, :knee\nResults order matches input acwtypes order\nIf only one ACW type is requested, acw_results is a scalar; otherwise it's a vector\n\n\n\n\n\n","category":"type"},{"location":"#IntrinsicTimescales.ACW.acw-Tuple{Any, Any}","page":"API","title":"IntrinsicTimescales.ACW.acw","text":"acw(data, fs; acwtypes=possible_acwtypes, n_lags=nothing, freqlims=nothing, time=nothing, \n    dims=ndims(data), return_acf=true, return_psd=true, average_over_trials=false,\n    trial_dims::Int=setdiff([1, 2], dims)[1], skip_zero_lag::Bool=false, max_peaks::Int=1, oscillation_peak::Bool=true,\n    allow_variable_exponent::Bool=false, parallel::Bool=false)\n\nCompute various timescale measures for time series data. For detailed documentaion, see https://duodenum96.github.io/IntrinsicTimescales.jl/stable/acw/. \n\nArguments\n\ndata::AbstractArray{<:Real}: Input time series data\nfs::Real: Sampling frequency\nacwtypes::Union{Vector{Symbol}, Symbol}=[:acw0, :acw50, :acweuler, :auc, :tau, :knee]: Types of ACW to compute.\n\nSupported ACW types:\n\n:acw0 - Time to first zero crossing\n:acw50 - Time to 50% decay\n:acweuler - Time to 1/e decay\n:auc - Area under curve of ACF before ACW0\n:tau - Exponential decay timescale\n:knee - Knee frequency from spectral analysis\nn_lags::Union{Int, Nothing}=nothing: Number of lags for ACF calculation. If not specified, uses 1.1 * ACW0.\nfreqlims::Union{Tuple{Real, Real}, Nothing}=nothing: Frequency limits for spectral analysis. If not specified, uses full frequency range.\ntime::Union{Vector{Real}, Nothing}=nothing: Time vector. This is required for Lomb-Scargle method in the case of missing data.\ndims::Int=ndims(data): Dimension along which to compute ACW (Dimension of time)\nreturn_acf::Bool=true: Whether to return the ACF\nreturn_psd::Bool=true: Whether to return the PSD\naverage_over_trials::Bool=false: Whether to average the ACF or PSD over trials\ntrial_dims::Int=setdiff([1, 2], dims)[1]: Dimension along which to average the ACF or PSD over trials (Dimension of trials)\nskip_zero_lag::Bool=false: Whether to skip the zero lag for fitting an exponential decay function. Used only for :tau.\nmax_peaks::Int=1: Maximum number of oscillatory peaks to fit in spectral analysis\noscillation_peak::Bool=true: Whether to fit an oscillation peak in the spectral analysis\nallow_variable_exponent::Bool=false: Whether to allow variable exponent in spectral fitting\nparallel::Bool=false: Whether to use parallel computation\n\nReturns\n\nACWResults: Structure containing computed ACW measures and intermediate results\n\nFields of the ACWResults structure:\n\nfs::Real: Sampling frequency\nacw_results: Computed ACW values (type depends on number of ACW types requested)\nacwtypes::Union{Vector{<:Symbol}, Symbol}: Types of ACW computed\nn_lags::Union{Int, Nothing}: Number of lags used for ACF calculation\nfreqlims::Union{Tuple{Real, Real}, Nothing}: Frequency limits used for spectral analysis\nacf::Union{AbstractVector{<:Real}, AbstractArray{<:Real}, Nothing}: Autocorrelation function\npsd::Union{AbstractVector{<:Real}, AbstractArray{<:Real}, Nothing}: Power spectral density\nfreqs::Union{AbstractVector{<:Real}, AbstractArray{<:Real}, Nothing}: Frequency vector for PSD\nlags::Union{AbstractVector{<:Real}, AbstractArray{<:Real}, Nothing}: Lag vector for ACF\nx_dim::Union{Int, Nothing}: Dimension index corresponding to x-axis (lags/freqs)\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.TuringBackend.ADVIResults","page":"API","title":"IntrinsicTimescales.TuringBackend.ADVIResults","text":"ADVIResults{T<:Real}\n\nContainer for ADVI (Automatic Differentiation Variational Inference) results.\n\nFields\n\nsamples::AbstractArray{T}: Matrix of posterior samples\nMAP::AbstractVector{T}: Maximum a posteriori estimates\nvariational_posterior::Any: Turing variational posterior object containing full inference results\n\n\n\n\n\n","category":"type"},{"location":"#IntrinsicTimescales.TuringBackend.create_turing_model-Tuple{Any, Any}","page":"API","title":"IntrinsicTimescales.TuringBackend.create_turing_model","text":"create_turing_model(model, data_sum_stats; σ_prior=Exponential(1))\n\nCreate a Turing probabilistic model for variational inference.\n\nArguments\n\nmodel: Model instance containing prior distributions and data generation methods\ndata_sum_stats: Summary statistics of the observed data\nσ_prior=Exponential(1): Prior distribution for the uncertainty parameter σ\n\nReturns\n\nTuring model object ready for inference\n\nNotes\n\nThe created model includes:\n\nParameter sampling from truncated priors (positive values only)\nData generation using the model's forward simulation\nLikelihood computation using Normal distribution\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.TuringBackend.fit_vi-Tuple{Any}","page":"API","title":"IntrinsicTimescales.TuringBackend.fit_vi","text":"fit_vi(model; n_samples=4000, n_iterations=10, n_elbo_samples=20, \n       optimizer=AutoForwardDiff())\n\nPerform variational inference using ADVI (Automatic Differentiation Variational Inference).\n\nArguments\n\nmodel: Model instance to perform inference on\nn_samples::Int=4000: Number of posterior samples to draw\nn_iterations::Int=10: Number of ADVI iterations\nn_elbo_samples::Int=20: Number of samples for ELBO estimation\noptimizer=AutoForwardDiff(): Optimization algorithm for ADVI\n\nReturns\n\nADVIResults: Container with inference results including:\nsamples_matrix: Matrix of posterior samples\nMAP: Maximum a posteriori parameter estimates\nvariational_posterior: Turing variational posterior object containing full inference results\n\nNotes\n\nUses Turing.jl's ADVI implementation for fast approximate Bayesian inference. The model is automatically constructed with appropriate priors and likelihood.\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.TuringBackend.get_param_dict_advi-Tuple{}","page":"API","title":"IntrinsicTimescales.TuringBackend.get_param_dict_advi","text":"get_param_dict_advi()\n\nGet default parameter dictionary for ADVI (Automatic Differentiation Variational Inference) algorithm.\n\nReturns\n\nDictionary containing default values for ADVI parameters including:\n\nn_samples: Number of posterior samples to draw (default: 4000)\nn_iterations: Number of ADVI iterations (default: 50) \nn_elbo_samples: Number of samples for ELBO estimation (default: 20)\nautodiff: Automatic differentiation backend (default: AutoForwardDiff())\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats","page":"API","title":"IntrinsicTimescales.SummaryStats","text":"SummaryStats\n\nModule for computing various summary statistics from time series data. Includes functions for:\n\nAutocorrelation (FFT and time-domain methods)\nPower spectral density (periodogram and Welch methods)\nCross-correlation\nSpecial handling for missing data (NaN values)\n\n\n\n\n\n","category":"module"},{"location":"#IntrinsicTimescales.SummaryStats._comp_psd_lombscargle-Tuple{AbstractVector{<:Number}, AbstractVector{<:Number}, AbstractVector{<:Number}}","page":"API","title":"IntrinsicTimescales.SummaryStats._comp_psd_lombscargle","text":"_comp_psd_lombscargle(times, data, frequency_grid)\n\nInternal function to compute Lomb-Scargle periodogram for a single time series.\n\nArguments\n\ntimes: Time points vector (without NaN)\ndata: Time series data (without NaN)\nfrequency_grid: Pre-computed frequency grid\n\nReturns\n\npower: Lomb-Scargle periodogram values\n\nNotes\n\nUses LombScargle.jl for core computation\nAssumes data has been pre-processed and doesn't contain NaN values\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats.acf_statsmodels-Union{Tuple{Vector{T}}, Tuple{T}} where T<:Real","page":"API","title":"IntrinsicTimescales.SummaryStats.acf_statsmodels","text":"acf_statsmodels(x::Vector{T}; kwargs...) where {T <: Real}\n\nJulia implementation of statsmodels.tsa.stattools.acf function. Only for testing.\n\nArguments\n\nx: Time series data vector\nadjusted=false: Use n-k denominators if true\nnlags=nothing: Number of lags (default: min(10*log10(n), n-1))\nqstat=false: Return Ljung-Box Q-statistics\nisfft=false: Use FFT method\nalpha=nothing: Confidence level for intervals\nbartlett_confint=false: Use Bartlett's formula\nmissing_handling=\"conservative\": NaN handling method\n\nReturns\n\nVector of autocorrelation values\n\nNotes\n\nSupports multiple missing data handling methods:\n\"none\": No checks\n\"raise\": Error on NaN\n\"conservative\": NaN-aware computations\n\"drop\": Remove NaN values\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats.acovf-Union{Tuple{Vector{T}}, Tuple{T}} where T<:Real","page":"API","title":"IntrinsicTimescales.SummaryStats.acovf","text":"Estimate autocovariances. Translated to Julia from statsmodels.tsa.stattools\n\nParameters\n\nx : array_like     Time series data. Must be 1d. adjusted : bool, default False     If True, then denominators is n-k, otherwise n. demean : bool, default True     If True, then subtract the mean x from each element of x. fft : bool, default True     If True, use FFT convolution.  This method should be preferred     for long time series. missing : str, default \"none\"     A string in [\"none\", \"raise\", \"conservative\", \"drop\"] specifying how     the NaNs are to be treated. \"none\" performs no checks. \"raise\" raises     an exception if NaN values are found. \"drop\" removes the missing     observations and then estimates the autocovariances treating the     non-missing as contiguous. \"conservative\" computes the autocovariance     using nan-ops so that nans are removed when computing the mean     and cross-products that are used to estimate the autocovariance.     When using \"conservative\", n is set to the number of non-missing     observations. nlag : {int, None}, default None     Limit the number of autocovariances returned.  Size of returned     array is nlag + 1.  Setting nlag when fft is False uses a simple,     direct estimator of the autocovariances that only computes the first     nlag + 1 values. This can be much faster when the time series is long     and only a small number of autocovariances are needed.\n\nReturns\n\nndarray     The estimated autocovariances.\n\nReferences\n\n.. [1] Parzen, E., 1963. On spectral analysis with missing observations        and amplitude modulation. Sankhya: The Indian Journal of        Statistics, Series A, pp.383-392.\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats.bat_autocorr-Tuple{AbstractVector{<:Real}}","page":"API","title":"IntrinsicTimescales.SummaryStats.bat_autocorr","text":"bat_autocorr(x::AbstractVector{<:Real})\n\nCompute the normalized autocorrelation function of a 1D time series using FFT. Returns a vector containing the autocorrelation values.\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats.bat_autocorr-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T<:Real","page":"API","title":"IntrinsicTimescales.SummaryStats.bat_autocorr","text":"bat_autocorr(x::AbstractMatrix{<:Real})\n\nCompute the normalized autocorrelation function of a 2D time series using FFT. data is a matrix with dimensions (nseries × ntimepoints)\n\nreturns a matrix with dimensions (nseries × nlags)\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats.bat_integrated_autocorr_len","page":"API","title":"IntrinsicTimescales.SummaryStats.bat_integrated_autocorr_len","text":"bat_integrated_autocorr_len(\n    v::AbstractVectorOfSimilarVectors{<:Real};\n    c::Integer = 5, tol::Integer = 50, strict = true\n)\n\nEstimate the integrated autocorrelation length of variate series v.\n\nc: Step size for window search.\ntol: Minimum number of autocorrelation times needed to trust the estimate.\nstrict: Throw exception if result is not trustworthy\n\nThis estimate uses the iterative procedure described on page 16 of Sokal's notes to determine a reasonable window size.\n\nPorted to Julia from the emcee Python package, under MIT License. Original authors Dan Foreman-Mackey et al.\n\n\n\n\n\n","category":"function"},{"location":"#IntrinsicTimescales.SummaryStats.bat_integrated_autocorr_weight","page":"API","title":"IntrinsicTimescales.SummaryStats.bat_integrated_autocorr_weight","text":"bat_integrated_autocorr_weight(\n    samples::DensitySampleVector;\n    c::Integer = 5, tol::Integer = 50, strict = true\n)\n\nEstimate the integrated autocorrelation weight of samples.\n\nUses bat_integrated_autocorr_len.     \n\n\n\n\n\n","category":"function"},{"location":"#IntrinsicTimescales.SummaryStats.comp_ac_fft-Union{Tuple{AbstractArray{T}}, Tuple{T}} where T<:Real","page":"API","title":"IntrinsicTimescales.SummaryStats.comp_ac_fft","text":"comp_ac_fft(data::AbstractArray{T}; dims::Int=ndims(data), n_lags::Integer=size(data, dims), parallel::Bool=false) where {T <: Real}\n\nCompute autocorrelation using FFT along specified dimension.\n\nArguments\n\ndata: Array of time series data\ndims: Dimension along which to compute autocorrelation (defaults to last dimension)\nn_lags: Number of lags to compute (defaults to size of data along specified dimension)\nparallel: Whether to use parallel computation\n\nReturns\n\nArray with autocorrelation values, the specified dimension becomes the dimension of lags while the other dimensions denote ACF values\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats.comp_ac_fft-Union{Tuple{Vector{T}}, Tuple{T}} where T<:Real","page":"API","title":"IntrinsicTimescales.SummaryStats.comp_ac_fft","text":"comp_ac_fft(data::Vector{T}; n_lags::Real=length(data)) where {T <: Real}\n\nCompute autocorrelation using FFT method.\n\nArguments\n\ndata: Input time series vector\nn_lags: Number of lags to compute (defaults to length of data)\n\nReturns\n\nVector of autocorrelation values from lag 0 to n_lags-1\n\nNotes\n\nUses FFT for efficient computation\nPads data to next power of 2 for FFT efficiency\nNormalizes by variance (first lag)\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats.comp_ac_time-Union{Tuple{AbstractArray{T}}, Tuple{T}} where T<:Real","page":"API","title":"IntrinsicTimescales.SummaryStats.comp_ac_time","text":"comp_ac_time(data::AbstractArray{T}, max_lag::Integer; dims::Int=ndims(data), parallel::Bool=false) where {T <: Real}\n\nCompute autocorrelation in time domain along specified dimension.\n\nArguments\n\ndata: Array of time series data\nmax_lag: Maximum lag to compute\ndims: Dimension along which to compute autocorrelation (defaults to last dimension)\nparallel=false: Whether to use parallel computation\n\nReturns\n\nArray with autocorrelation values, the specified dimension becomes the dimension of lags while the other dimensions denote ACF values\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats.comp_ac_time-Union{Tuple{Vector{T}}, Tuple{T}} where T<:Real","page":"API","title":"IntrinsicTimescales.SummaryStats.comp_ac_time","text":"comp_ac_time(data::AbstractArray{T}, max_lag::Integer; dims::Int=ndims(data)) where {T <: Real}\n\nCompute autocorrelation in time domain along specified dimension.\n\nArguments\n\ndata: Array of time series data\nmax_lag: Maximum lag to compute\ndims: Dimension along which to compute autocorrelation (defaults to last dimension)\n\nReturns\n\nArray with autocorrelation values, the specified dimension becomes the dimension of lags while the other dimensions denote ACF values\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats.comp_ac_time_missing-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T<:Real","page":"API","title":"IntrinsicTimescales.SummaryStats.comp_ac_time_missing","text":"comp_ac_time_missing(data::AbstractArray{T}; kwargs...) where {T <: Real}\n\nCompute autocorrelation for data with missing values.\n\nArguments\n\ndata: Time series data (may contain NaN)\ndims=ndims(data): Dimension along which to compute\nn_lags=size(data,dims): Number of lags to compute\nparallel=false: Whether to use parallel computation\n\nReturns\n\nArray of autocorrelation values\n\nNotes\n\nHandles missing data using missing=\"conservative\" approach of \n\nstatsmodels.tsa.stattools.acf. See https://www.statsmodels.org/dev/generated/statsmodels.tsa.stattools.acf.html  for details. \n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats.comp_cc-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}, Integer}} where T<:Real","page":"API","title":"IntrinsicTimescales.SummaryStats.comp_cc","text":"comp_cc(data1::AbstractArray{T}, data2::AbstractArray{T}, max_lag::Integer;\n       dims::Int=ndims(data1)) where {T <: Real}\n\nCompute cross-correlation between two arrays along specified dimension.\n\nArguments\n\ndata1: First array of time series data\ndata2: Second array of time series data\nmax_lag: Maximum lag to compute\ndims: Dimension along which to compute cross-correlation (defaults to last dimension)\n\nReturns\n\nArray with cross-correlation values, reduced along specified dimension\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats.comp_psd-Union{Tuple{T}, Tuple{AbstractArray{T}, Real}} where T<:Real","page":"API","title":"IntrinsicTimescales.SummaryStats.comp_psd","text":"comp_psd(x::AbstractArray{T}, fs::Real; kwargs...) where {T <: Real}\n\nCompute power spectral density using periodogram or welch method.\n\nArguments\n\nx: Time series data (time × channels)\nfs: Sampling frequency\ndims=ndims(x): Dimension along which to compute PSD\nmethod=\"periodogram\": Method to use (\"periodogram\" or \"welch\")\nwindow=dsp.hamming: Window function\nn=div(size(x,dims),8): Window size for Welch method\nnoverlap=div(n,2): Overlap for Welch method\nparallel=false: Whether to use parallel computation\n\nReturns\n\npower: Power spectral density values (excludes DC component)\nfreqs: Corresponding frequencies (excludes DC component)\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats.comp_psd_adfriendly-Tuple{AbstractArray{<:Real}, Real}","page":"API","title":"IntrinsicTimescales.SummaryStats.comp_psd_adfriendly","text":"comp_psd_adfriendly(x::AbstractArray{<:Real}, fs::Real; dims::Int=ndims(x), parallel::Bool=false)\n\nCompute power spectral density using an automatic differentiation (AD) friendly implementation.\n\nArguments\n\nx: Time series data\nfs: Sampling frequency\ndims=ndims(x): Dimension along which to compute PSD\nparallel=false: Whether to use parallel computation\n\nReturns\n\npower: Power spectral density values\nfreqs: Corresponding frequencies\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats.comp_psd_lombscargle-Tuple{AbstractVector{<:Number}, AbstractVector{<:Number}, AbstractVector{Bool}, Real}","page":"API","title":"IntrinsicTimescales.SummaryStats.comp_psd_lombscargle","text":"comp_psd_lombscargle(times, data, nanmask, dt; dims=ndims(data))\n\nCompute Lomb-Scargle periodogram for data with missing values.\n\nArguments\n\ntimes: Time points vector\ndata: Time series data (may contain NaN)\nnanmask: Boolean mask indicating NaN positions\ndt: Time step\ndims=ndims(data): Dimension along which to compute\n\nReturns\n\npower: Lomb-Scargle periodogram values\nfrequency_grid: Corresponding frequencies\n\nNotes\n\nHandles irregular sampling due to missing data\nUses frequency grid based on shortest valid time series\nAutomatically determines appropriate frequency range\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.SummaryStats.prepare_lombscargle-Union{Tuple{S}, Tuple{T}, Tuple{AbstractVector{T}, AbstractMatrix{S}, AbstractMatrix{Bool}, Real}} where {T<:Number, S<:Number}","page":"API","title":"IntrinsicTimescales.SummaryStats.prepare_lombscargle","text":"prepare_lombscargle(times, data, nanmask)\n\nPrepare data for Lomb-Scargle periodogram computation by handling missing values.\n\nArguments\n\ntimes: Time points vector\ndata: Time series data (may contain NaN)\nnanmask: Boolean mask indicating NaN positions\n\nReturns\n\nvalid_times: Time points with NaN values removed\nvalid_data: Data points with NaN values removed\nfrequency_grid: Suggested frequency grid for analysis\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Distances","page":"API","title":"IntrinsicTimescales.Distances","text":"Distances\n\nModule providing distance metrics for comparing summary statistics in ABC inference. Currently implements linear (L2) and logarithmic distances.\n\n\n\n\n\n","category":"module"},{"location":"#IntrinsicTimescales.Distances.linear_distance-Tuple{Union{Real, AbstractArray}, Union{Real, AbstractArray}}","page":"API","title":"IntrinsicTimescales.Distances.linear_distance","text":"linear_distance(data, synth_data)\n\nCompute mean squared error (MSE) between summary statistics.\n\nArguments\n\ndata::Union{AbstractArray, Real}: Observed data summary statistics\nsynth_data::Union{AbstractArray, Real}: Simulated data summary statistics\n\nReturns\n\nFloat64: Mean squared difference between data and synth_data\n\nNotes\n\nHandles both scalar and array inputs\nFor arrays, computes element-wise differences before averaging\nUseful for comparing summary statistics on linear scales\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Distances.logarithmic_distance-Tuple{Union{Real, AbstractArray}, Union{Real, AbstractArray}}","page":"API","title":"IntrinsicTimescales.Distances.logarithmic_distance","text":"logarithmic_distance(data, synth_data)\n\nCompute mean squared distance between logarithms of summary statistics.\n\nArguments\n\ndata::Union{AbstractArray, Real}: Observed data summary statistics\nsynth_data::Union{AbstractArray, Real}: Simulated data summary statistics\n\nReturns\n\nFloat64: Mean squared difference between log(data) and log(synth_data)\n\nNotes\n\nHandles both scalar and array inputs\nFor arrays, computes element-wise log differences before averaging\nUseful for comparing summary statistics spanning multiple orders of magnitude\nAssumes all values are positive\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils","page":"API","title":"IntrinsicTimescales.Utils","text":"Utils\n\nModule providing utility functions for time series analysis, including:\n\nExponential decay fitting\nOscillation peak detection\nKnee frequency estimation\nLorentzian fitting\nACF width calculations\n\n\n\n\n\n","category":"module"},{"location":"#IntrinsicTimescales.Utils.acw0-Union{Tuple{S}, Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{S}}} where {T<:Real, S<:Real}","page":"API","title":"IntrinsicTimescales.Utils.acw0","text":"acw0(lags, acf; dims=ndims(acf), parallel=false)\n\nCompute the ACW0 (autocorrelation width at zero crossing) along specified dimension.\n\nArguments\n\nlags::AbstractVector{T}: Vector of lag values\nacf::AbstractArray{T}: Array of autocorrelation values\ndims::Int=ndims(acf): Dimension along which to compute ACW0\nparallel=false: Whether to use parallel computation\n\nReturns\n\nFirst lag where autocorrelation crosses zero\n\nNotes\n\nAlternative measure of characteristic timescale\nMore sensitive to noise than ACW50\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.acw50-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T<:Real","page":"API","title":"IntrinsicTimescales.Utils.acw50","text":"acw50(lags, acf; dims=ndims(acf), parallel=false)\n\nCompute the ACW50 (autocorrelation width at 50%) along specified dimension.\n\nArguments\n\nlags::AbstractVector{T}: Vector of lag values\nacf::AbstractArray{T}: Array of autocorrelation values\ndims::Int=ndims(acf): Dimension along which to compute ACW50\nparallel=false: Whether to use parallel computation\n\nReturns\n\nFirst lag where autocorrelation falls below 0.5\n\nNotes\n\nUsed for estimating characteristic timescales\nRelated to tau by: tau = -acw50/log(0.5)\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.acw_romberg-Union{Tuple{S}, Tuple{Real, AbstractVector{S}}} where S<:Real","page":"API","title":"IntrinsicTimescales.Utils.acw_romberg","text":"acw_romberg(dt, acf; dims=ndims(acf), parallel=false)\n\nCalculate the area under the curve of ACF using Romberg integration.\n\nArguments\n\ndt::Real: Time step\nacf::AbstractVector: Array of autocorrelation values\ndims::Int=ndims(acf): Dimension along which to compute\nparallel=false: Whether to use parallel computation\n\nReturns\n\nAUC of ACF\n\nNotes\n\nReturns only the integral value, discarding the error estimate\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.acweuler-Union{Tuple{S}, Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{S}}} where {T<:Real, S<:Real}","page":"API","title":"IntrinsicTimescales.Utils.acweuler","text":"acweuler(lags, acf; dims=ndims(acf), parallel=false)\n\nCompute the ACW at 1/e (≈ 0.368) along specified dimension.\n\nArguments\n\nlags::AbstractVector{T}: Vector of lag values\nacf::AbstractVector{S}: Array of autocorrelation values\ndims::Int=ndims(acf): Dimension along which to compute\nparallel=false: Whether to use parallel computation\n\nReturns\n\nFirst lag where autocorrelation falls below 1/e\n\nNotes\n\nFor exponential decay, equals the timescale parameter tau\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.expdecay-Tuple{Any, Any}","page":"API","title":"IntrinsicTimescales.Utils.expdecay","text":"expdecay(tau, lags)\n\nCompute exponential decay function.\n\nArguments\n\ntau::Real: Timescale parameter\nlags::AbstractVector: Time lags\n\nReturns\n\nVector of exp(-t/tau) values\n\nNotes\n\nUsed for fitting autocorrelation functions\nAssumes exponential decay model: acf = exp(-t/tau)\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.expdecay_3_parameters-Tuple{Any, Any}","page":"API","title":"IntrinsicTimescales.Utils.expdecay_3_parameters","text":"expdecay_3_parameters(p, lags)\n\nCompute exponential decay function with amplitude and offset parameters.  This is used in acw with the setting skip_zero_lag=true. \n\nArguments\n\np::AbstractVector: Parameters [A, tau, B] where:\nA: Amplitude parameter\ntau: Timescale parameter \nB: Offset parameter\nlags::AbstractVector: Time lags\n\nReturns\n\nVector of A*(exp(-t/tau) + B) values\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.find_knee_frequency-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T<:Real","page":"API","title":"IntrinsicTimescales.Utils.find_knee_frequency","text":"find_knee_frequency(psd, freqs; dims=ndims(psd), min_freq=freqs[1], max_freq=freqs[end], constrained=false, allow_variable_exponent=false, parallel=false)\n\nFind knee frequency by fitting Lorentzian to power spectral density.\n\nArguments\n\npsd::AbstractArray{T}: Power spectral density values\nfreqs::Vector{T}: Frequency values\ndims::Int=ndims(psd): Dimension along which to compute\nmin_freq::T=freqs[1]: Minimum frequency to consider\nmax_freq::T=freqs[end]: Maximum frequency to consider\nconstrained::Bool=false: Whether to use constrained optimization\nallow_variable_exponent::Bool=false: Whether to allow variable exponent (PLE)\nparallel=false: Whether to use parallel computation\n\nReturns\n\nVector of the fit  for the equation amp/(1 + (f/knee)^{exponent}). \n\nIf allowvariableexponent=false, assumes exponent=2 and returns [amplitude, kneefrequency]. If true,  returns [amplitude, kneefrequency, exponent].\n\nNotes\n\nUses Lorentzian fitting with NonlinearSolve.jl or Optimization.jl\nInitial guess for amplitude based on low frequency power\nInitial guess for knee at half-power point\nInitial guess for exponent is 2.0 when variable exponent allowed\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.find_oscillation_peak-Tuple{AbstractVector{<:Real}, AbstractVector{<:Real}}","page":"API","title":"IntrinsicTimescales.Utils.find_oscillation_peak","text":"find_oscillation_peak(psd, freqs; min_freq=5.0/1000.0, max_freq=50.0/1000.0, min_prominence_ratio=0.1)\n\nFind dominant oscillatory peak in power spectral density.\n\nArguments\n\npsd::AbstractVector: Power spectral density values\nfreqs::AbstractVector: Frequency values\nmin_freq::Real=5.0/1000.0: Minimum frequency to consider\nmax_freq::Real=50.0/1000.0: Maximum frequency to consider\nmin_prominence_ratio::Real=0.1: Minimum peak prominence as fraction of max PSD\n\nReturns\n\nFrequency of most prominent peak, or NaN if no significant peak found\n\nNotes\n\nUses peak prominence for robustness\nFilters peaks by minimum prominence threshold\nReturns NaN if no peaks meet criteria\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.fit_expdecay-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T<:Real","page":"API","title":"IntrinsicTimescales.Utils.fit_expdecay","text":"fit_expdecay(lags, acf; dims=ndims(acf), parallel=false)\n\nFit exponential decay to autocorrelation function.\n\nArguments\n\nlags::AbstractVector{T}: Time lags\nacf::AbstractArray{T}: Autocorrelation values\ndims::Int=ndims(acf): Dimension along which to fit\nparallel=false: Whether to use parallel computation\n\nReturns\n\nFitted timescale parameter(s)\n\nNotes\n\nUses NonlinearSolve.jl with FastShortcutNLLSPolyalg\nInitial guess based on ACW50\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.fit_expdecay_3_parameters-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T<:Real","page":"API","title":"IntrinsicTimescales.Utils.fit_expdecay_3_parameters","text":"fit_expdecay_3_parameters(lags, acf; parallel=false)\n\nFit a 3-parameter exponential decay function to autocorrelation data ( A*(exp(-t/tau) + B) ).  Excludes lag 0 from fitting. \n\nArguments\n\nlags::AbstractVector{T}: Time lags\nacf::AbstractVector{T}: Autocorrelation values\nparallel=false: Whether to use parallel computation\n\nReturns\n\nFitted timescale parameter (tau)\n\nNotes\n\nInitial guess: A=0.5, tau from ACW50, B=0.0\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.fit_gaussian-Tuple{AbstractVector{<:Real}, AbstractVector{<:Real}, Real}","page":"API","title":"IntrinsicTimescales.Utils.fit_gaussian","text":"fit_gaussian(psd, freqs, initial_peak; min_freq=freqs[1], max_freq=freqs[end])\n\nFit Gaussian to power spectral density around a peak.\n\nArguments\n\npsd::AbstractVector{<:Real}: Power spectral density values\nfreqs::AbstractVector{<:Real}: Frequency values\ninitial_peak::Real: Initial guess for center frequency\nmin_freq::Real: Minimum frequency to consider\nmax_freq::Real: Maximum frequency to consider\n\nReturns\n\nVector{Float64}: Fitted parameters [amplitude, centerfreq, stddev]\n\nNotes\n\nUses initial peak location from findoscillationpeak\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.fooof_fit-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}} where T<:Real","page":"API","title":"IntrinsicTimescales.Utils.fooof_fit","text":"fooof_fit(psd, freqs; dims=ndims(psd), min_freq=freqs[1], max_freq=freqs[end], \n          oscillation_peak=true, max_peaks=3, allow_variable_exponent=false, constrained=false, parallel=false)\n\nPerform FOOOF-style fitting of power spectral density. The default behavior is to fit a Lorentzian with PLE = 2.  If allowvariableexponent=true, the function will fit a Lorentzian with variable PLE. \n\nArguments\n\npsd::AbstractArray{T}: Power spectral density values\nfreqs::Vector{T}: Frequency values\ndims::Int=ndims(psd): Dimension along which to compute\nmin_freq::T=freqs[1]: Minimum frequency to consider\nmax_freq::T=freqs[end]: Maximum frequency to consider\noscillation_peak::Bool=true: Whether to compute oscillation peaks\nmax_peaks::Int=3: Maximum number of oscillatory peaks to fit\nreturn_only_knee::Bool=false: Whether to return only knee frequency\nallow_variable_exponent::Bool=false: Whether to allow variable exponent (PLE)\nconstrained::Bool=false: Whether to use constrained optimization\nparallel=false: Whether to use parallel computation\n\nReturns\n\nIf returnonlyknee=false:\n\nTuple of (kneefrequency, oscillationparameters) where oscillationparameters is Vector of (centerfreq, amplitude, std_dev) for each peak\n\nIf returnonlyknee=true:\n\nknee_frequency only\n\nNotes\n\nImplements iterative FOOOF-style fitting:\nFit initial Lorentzian to PSD\nFind and fit Gaussian peaks iteratively\nSubtract all Gaussians from original PSD\nRefit Lorentzian to cleaned PSD\nDefault behavior fits standard Lorentzian (PLE = 2)\nIf allowvariableexponent=true, fits Lorentzian with variable PLE\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.gaussian-Tuple{Any, Any}","page":"API","title":"IntrinsicTimescales.Utils.gaussian","text":"gaussian(f, u)\n\nGaussian function for fitting oscillations. \n\nArguments\n\nf::AbstractVector: Frequency values\nu::Vector: Parameters [amplitude, centerfreq, stddev]\n\nReturns\n\nVector of Gaussian values: amp * exp(-(f-center)²/(2*std²))\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.get_slices-Union{Tuple{AbstractArray{T}}, Tuple{T}} where T","page":"API","title":"IntrinsicTimescales.Utils.get_slices","text":"get_slices(x::AbstractArray{T}; dims::Int=ndims(x)) where {T}\n\nGet array slices along all dimensions except the specified one.\n\nArguments\n\nx::AbstractArray{T}: Input array\ndims::Int=ndims(x): Dimension to exclude when taking slices\n\nReturns\n\nIterator of array slices\n\nNotes\n\nReturns slices along all dimensions except dims\nEach slice represents a view into the array with dims fixed\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.lorentzian-Tuple{Any, Any}","page":"API","title":"IntrinsicTimescales.Utils.lorentzian","text":"lorentzian(f, u)\n\nCompute Lorentzian function values.\n\nArguments\n\nf::AbstractVector: Frequency values\nu::Vector: Parameters [amplitude, knee_frequency]\n\nReturns\n\nVector of Lorentzian values: amp/(1 + (f/knee)²)\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.lorentzian_initial_guess-Tuple{AbstractVector{<:Real}, AbstractVector{<:Real}}","page":"API","title":"IntrinsicTimescales.Utils.lorentzian_initial_guess","text":"lorentzian_initial_guess(psd, freqs; min_freq=freqs[1], max_freq=freqs[end])\n\nEstimate initial parameters for Lorentzian fitting.\n\nArguments\n\npsd::AbstractVector{<:Real}: Power spectral density values\nfreqs::AbstractVector{<:Real}: Frequency values\nmin_freq::Real: Minimum frequency to consider\nmax_freq::Real: Maximum frequency to consider\n\nReturns\n\nVector{Float64}: Initial guess for [amplitude, knee_frequency]\n\nNotes\n\nEstimates amplitude from average power of low frequencies. \nEstimates knee frequency from half-power point. \nIf allowvariableexponent=true, sets initial guess for exponent to 2.0. \nUsed as starting point for nonlinear fitting\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.lorentzian_with_exponent-Tuple{Any, Any}","page":"API","title":"IntrinsicTimescales.Utils.lorentzian_with_exponent","text":"lorentzian_with_exponent(f, u)\n\nCompute Lorentzian function values that allow variable exponent (PLE).\n\nArguments\n\nf::AbstractVector: Frequency values\nu::Vector: Parameters [amplitude, knee_frequency, exponent]\n\nReturns\n\nVector of Lorentzian values: amp/(1 + (f/knee)^exponent)\n\nNotes\n\nGeneralizes standard Lorentzian to allow variable power law exponent\nWhen exponent=2, reduces to standard Lorentzian\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.residual_expdecay!-Tuple{Any, Any, Any}","page":"API","title":"IntrinsicTimescales.Utils.residual_expdecay!","text":"Residual function for expdecay du: residual u: parameters p: data\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Utils.stack_and_reshape-Tuple{Any}","page":"API","title":"IntrinsicTimescales.Utils.stack_and_reshape","text":"stack_and_reshape(slices; dims::Int)\n\nReconstruct an array from slices by stacking and reshaping them back to original dimensions.\n\nArguments\n\nslices: Iterator or collection of array slices (from get_slices)\ndims::Int: Dimension along which the original slicing was performed (should be same as dims fed into get_slices)\n\nReturns\n\nReconstructed array with proper dimension ordering\n\nExample\n\n# Split array into slices along dimension 2\nx = rand(3, 4, 5)\nslices = get_slices(x; dims=2)\n\n# Reconstruct original array\nx_reconstructed = stack_and_reshape(slices; dims=2)\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.OrnsteinUhlenbeck","page":"API","title":"IntrinsicTimescales.OrnsteinUhlenbeck","text":"OrnsteinUhlenbeck\n\nModule for generating Ornstein-Uhlenbeck processes with various configurations. Uses DifferentialEquations.jl. \n\n\n\n\n\n","category":"module"},{"location":"#IntrinsicTimescales.OrnsteinUhlenbeck.generate_ou_process-Tuple{Union{Real, Vector{<:Real}}, Vararg{Real, 4}}","page":"API","title":"IntrinsicTimescales.OrnsteinUhlenbeck.generate_ou_process","text":"generate_ou_process(tau, true_D, dt, duration, num_trials; standardize=true, rng=Random.default_rng(), deq_seed=nothing)\n\nGenerate an Ornstein-Uhlenbeck process with a single timescale\n\nArguments\n\ntau::Union{Real, Vector{<:Real}}: Timescale(s) of the OU process\ntrue_D::Real: Target variance for scaling the process\ndt::Real: Time step size\nduration::Real: Total time length\nnum_trials::Real: Number of trials/trajectories\nstandardize::Bool=true: Whether to standardize output to match true_D\nrng::AbstractRNG=Random.default_rng(): Random number generator for initial conditions\ndeq_seed::Integer=nothing: Random seed for DifferentialEquations.jl solver. If nothing, uses StochasticDiffEq.jl defaults. Note that for full replicability, \n\nyou need to set both rng and deq_seed. \n\nReturns\n\nMatrix{Float64}: Generated OU process data with dimensions (numtrials, numtimesteps)\n\nNotes\n\nUses generateouprocess_sciml internally\nReturns NaN matrix if SciML solver fails\nStandardizes output to have specified variance if standardize=true\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.OrnsteinUhlenbeck.generate_ou_process_sciml-Union{Tuple{T}, Tuple{Union{Vector{T}, T}, Real, Real, Real, Integer}, Tuple{Union{Vector{T}, T}, Real, Real, Real, Integer, Bool}} where T<:Real","page":"API","title":"IntrinsicTimescales.OrnsteinUhlenbeck.generate_ou_process_sciml","text":"generate_ou_process_sciml(tau, true_D, dt, duration, num_trials, standardize; rng=Random.default_rng(), deq_seed=nothing)\n\nGenerate an Ornstein-Uhlenbeck process using DifferentialEquations.jl.\n\nArguments\n\ntau::Union{T, Vector{T}}: Timescale(s) of the OU process\ntrue_D::Real: Target variance for scaling\ndt::Real: Time step size\nduration::Real: Total time length\nnum_trials::Integer: Number of trials/trajectories\nstandardize::Bool: Whether to standardize output to match true_D\nrng::AbstractRNG=Random.default_rng(): Random number generator for initial conditions\ndeq_seed::Union{Integer, Nothing}=nothing: Random seed for DifferentialEquations.jl solver. If nothing, uses StochasticDiffEq.jl defaults. Note that for full replicability, \n\nyou need to set both rng and deq_seed. \n\nReturns\n\nTuple{Matrix{Float64}, ODESolution}: \nScaled OU process data\nFull SDE solution object\n\nNotes\n\nSwitches between static and dynamic arrays based on num_trials\n\nExample: \n\ntau = 1.0\ntrue_D = 1.0\ndt = 0.01\nduration = 10.0\nnum_trials = 100\n\nou, _ = generate_ou_process_sciml(tau, true_D, dt, duration, num_trials, true)\n\n# Reproducible example\ndeq_seed = 42\nou, _ = generate_ou_process_sciml(tau, true_D, dt, duration, num_trials, true, rng=Xoshiro(42), deq_seed=deq_seed)\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.OrnsteinUhlenbeck.generate_ou_with_oscillation-Union{Tuple{T}, Tuple{Vector{T}, Real, Real, Integer, Real, Real}} where T<:Real","page":"API","title":"IntrinsicTimescales.OrnsteinUhlenbeck.generate_ou_with_oscillation","text":"generate_ou_with_oscillation(theta, dt, duration, num_trials, data_mean, data_sd; rng=Random.default_rng(), deq_seed=nothing)\n\nGenerate a one-timescale OU process with an additive oscillation.\n\nArguments\n\ntheta::Vector{T}: Parameters [timescale, frequency, coefficient]\ndt::Real: Time step size\nduration::Real: Total time length\nnum_trials::Integer: Number of trials\ndata_mean::Real: Target mean value\ndata_sd::Real: Target standard deviation\nrng::AbstractRNG=Random.default_rng(): Random number generator for initial conditions\ndeq_seed::Union{Integer, Nothing}=nothing: Random seed for DifferentialEquations.jl solver. If nothing, uses StochasticDiffEq.jl defaults. Note that for full replicability, \n\nyou need to set both rng and deq_seed. \n\nReturns\n\nMatrix{Float64}: Generated data with dimensions (numtrials, numtimesteps)\n\nNotes\n\nCoefficient is bounded between 0 and 1\nCombines OU process with sinusoidal oscillation\nStandardizes and scales output to match target mean and standard deviation\nReturns NaN matrix if SciML solver fails\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.OneTimescale","page":"API","title":"IntrinsicTimescales.OneTimescale","text":"OneTimescale\n\nModule for inferring a single timescale from time series data using the Ornstein-Uhlenbeck process.\n\n\n\n\n\n","category":"module"},{"location":"#IntrinsicTimescales.OneTimescale.OneTimescaleModel","page":"API","title":"IntrinsicTimescales.OneTimescale.OneTimescaleModel","text":"OneTimescaleModel <: AbstractTimescaleModel\n\nModel for inferring a single timescale from time series data using the Ornstein-Uhlenbeck process. We recommend using the one_timescale_model constructor function rather than creating directly.\n\nFields\n\ndata::AbstractArray{<:Real}: Input time series data\ntime::AbstractVector{<:Real}: Time points corresponding to the data\nfit_method::Symbol: Fitting method (:abc or :advi)\nsummary_method::Symbol: Summary statistic type (:psd or :acf)\nlags_freqs: Lags (for ACF) or frequencies (for PSD)\nprior: Prior distribution(s) for parameters\ndistance_method::Symbol: Distance metric type (:linear or :logarithmic)\ndata_sum_stats: Pre-computed summary statistics\ndt::Real: Time step between observations\nT::Real: Total time span\nnumTrials::Real: Number of trials/iterations\ndata_mean::Real: Mean of input data\ndata_sd::Real: Standard deviation of input data\nfreqlims: Frequency limits for PSD analysis\nn_lags: Number of lags for ACF\nfreq_idx: Boolean mask for frequency selection\ndims::Int: Dimension along which to compute statistics\ndistance_combined::Bool: Whether to use combined distance metric\nweights::Vector{Real}: Weights for combined distance\ndata_tau::Union{Real, Nothing}: Pre-computed timescale\nu0::Union{Vector{Real}, Nothing}: Initial parameter guess\n\n\n\n\n\n","category":"type"},{"location":"#IntrinsicTimescales.Models.distance_function-Tuple{OneTimescaleModel, Any, Any}","page":"API","title":"IntrinsicTimescales.Models.distance_function","text":"Models.distance_function(model::OneTimescaleModel, sum_stats, data_sum_stats)\n\nCalculate the distance between summary statistics of simulated and observed data.\n\nArguments\n\nmodel::OneTimescaleModel: Model instance\nsum_stats: Summary statistics from simulated data\ndata_sum_stats: Summary statistics from observed data\n\nReturns\n\nDistance value based on model.distancemethod (:linear or :logarithmic) or combined distance if model.distancecombined is true\n\nNotes\n\nIf distance_combined is true:\n\nFor ACF: Combines ACF distance with fitted exponential decay timescale distance\nFor PSD: Combines PSD distance with knee frequency timescale distance\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Models.generate_data-Tuple{OneTimescaleModel, Any}","page":"API","title":"IntrinsicTimescales.Models.generate_data","text":"Models.generate_data(model::OneTimescaleModel, theta)\n\nGenerate synthetic data from the Ornstein-Uhlenbeck process with given timescale.\n\nArguments\n\nmodel::OneTimescaleModel: Model instance containing simulation parameters\ntheta: Vector containing single timescale parameter (τ)\n\nReturns\n\nSynthetic time series data with same dimensions as model.data\n\nNotes\n\nUses the model's stored parameters:\n\ndata_sd: Standard deviation for the OU process\ndt: Time step\nT: Total time span\nnumTrials: Number of trials/trajectories\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Models.int_fit-2","page":"API","title":"IntrinsicTimescales.Models.int_fit","text":"int_fit(model::OneTimescaleModel, param_dict::Dict=Dict())\n\nPerform inference using the specified fitting method.\n\nArguments\n\nmodel::OneTimescaleModel: Model instance\nparam_dict::Dict=Dict(): Dictionary of algorithm parameters. If empty, uses defaults.\n\nReturns\n\nFor ABC method:\n\nposterior_samples: Matrix of accepted parameter samples\nposterior_MAP: Maximum a posteriori estimate\nabc_record: Full record of ABC iterations\n\nFor ADVI method:\n\nADVIResults: Container with samples, MAP estimates, variances, and full chain\n\nNotes\n\nFor ABC: Uses Population Monte Carlo ABC with adaptive epsilon selection\nFor ADVI: Uses Automatic Differentiation Variational Inference via Turing.jl\nParameter dictionary can be customized for each method (see getparamdict_abc())\n\n\n\n\n\n","category":"function"},{"location":"#IntrinsicTimescales.Models.summary_stats-Tuple{OneTimescaleModel, Any}","page":"API","title":"IntrinsicTimescales.Models.summary_stats","text":"Models.summary_stats(model::OneTimescaleModel, data)\n\nCompute summary statistics (ACF or PSD) from time series data.\n\nArguments\n\nmodel::OneTimescaleModel: Model instance specifying summary statistic type\ndata: Time series data to analyze\n\nReturns\n\nFor ACF (summary_method = :acf):\n\nMean autocorrelation function up to n_lags\n\nFor PSD (summary_method = :psd):\n\nMean power spectral density within specified frequency range\n\nNotes\n\nACF is computed using FFT-based method\nPSD is computed and filtered according to model.freq_idx\nThrows ArgumentError if summary_method is invalid\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.OneTimescale.combined_distance-Tuple{OneTimescaleModel, Vararg{Any, 5}}","page":"API","title":"IntrinsicTimescales.OneTimescale.combined_distance","text":"combined_distance(model::OneTimescaleModel, simulation_summary, data_summary,\n                 weights, data_tau, simulation_tau)\n\nCompute combined distance metric between simulated and observed data.\n\nArguments\n\nmodel: OneTimescaleModel instance\nsimulation_summary: Summary statistics from simulation\ndata_summary: Summary statistics from observed data\nweights: Weights for combining distances\ndata_tau: Timescale from observed data\nsimulation_tau: Timescale from simulation\n\nReturns\n\nWeighted combination of summary statistic distance and timescale distance\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.OneTimescale.one_timescale_model-Tuple{Any, Any, Any}","page":"API","title":"IntrinsicTimescales.OneTimescale.one_timescale_model","text":"one_timescale_model(data, time, fit_method; kwargs...)\n\nConstruct a OneTimescaleModel for time series analysis.\n\nArguments\n\ndata: Input time series data\ntime: Time points corresponding to the data\nfit_method: Fitting method to use (:abc or :advi)\n\nKeyword Arguments\n\nsummary_method=:acf: Summary statistic type (:psd or :acf)\ndata_sum_stats=nothing: Pre-computed summary statistics\nlags_freqs=nothing: Custom lags or frequencies\nprior=nothing: Prior distribution(s) for parameters\nn_lags=nothing: Number of lags for ACF\ndistance_method=nothing: Distance metric type\ndt=time[2]-time[1]: Time step\nT=time[end]: Total time span\nnumTrials=size(data,1): Number of trials\ndata_mean=mean(data): Data mean\ndata_sd=std(data): Data standard deviation\nfreqlims=nothing: Frequency limits for PSD\nfreq_idx=nothing: Frequency selection mask\ndims=ndims(data): Analysis dimension\ndistance_combined=false: Use combined distance\nweights=[0.5, 0.5]: Distance weights\ndata_tau=nothing: Pre-computed timescale\nu0=nothing: Initial parameter guess\n\nReturns\n\nOneTimescaleModel: Model instance configured for specified analysis method\n\nNotes\n\nTwo main usage patterns:\n\nACF-based inference: summary_method=:acf, fit_method=:abc/:advi\nPSD-based inference: summary_method=:psd, fit_method=:abc/:advi\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.OneTimescaleAndOsc.OneTimescaleAndOscModel","page":"API","title":"IntrinsicTimescales.OneTimescaleAndOsc.OneTimescaleAndOscModel","text":"OneTimescaleAndOscModel <: AbstractTimescaleModel\n\nModel for inferring a single timescale and oscillation from time series data using the Ornstein-Uhlenbeck process. Parameters: [tau, freq, coeff] representing timescale, oscillation frequency, and oscillation coefficient.\n\nFields\n\ndata::AbstractArray{<:Real}: Input time series data\ntime::AbstractVector{<:Real}: Time points corresponding to the data\nfit_method::Symbol: Fitting method (:abc or :advi)\nsummary_method::Symbol: Summary statistic type (:psd or :acf)\nlags_freqs: Lags (for ACF) or frequencies (for PSD)\nprior: Prior distribution(s) for parameters\ndistance_method::Symbol: Distance metric type (:linear or :logarithmic)\ndata_sum_stats: Pre-computed summary statistics\ndt::Real: Time step between observations\nT::Real: Total time span\nnumTrials::Real: Number of trials/iterations\ndata_mean::Real: Mean of input data\ndata_sd::Real: Standard deviation of input data\nfreqlims: Frequency limits for PSD analysis\nn_lags: Number of lags for ACF\nfreq_idx: Boolean mask for frequency selection\ndims::Int: Dimension along which to compute statistics\ndistance_combined::Bool: Whether to use combined distance metric\nweights::Vector{Real}: Weights for combined distance\ndata_tau::Union{Real, Nothing}: Pre-computed timescale\ndata_osc::Union{Real, Nothing}: Pre-computed oscillation frequency\n\n\n\n\n\n","category":"type"},{"location":"#IntrinsicTimescales.Models.distance_function-Tuple{OneTimescaleAndOscModel, Any, Any}","page":"API","title":"IntrinsicTimescales.Models.distance_function","text":"Models.distance_function(model::OneTimescaleAndOscModel, sum_stats, data_sum_stats)\n\nCalculate the distance between summary statistics of simulated and observed data.\n\nArguments\n\nmodel::OneTimescaleAndOscModel: Model instance\nsum_stats: Summary statistics from simulated data\ndata_sum_stats: Summary statistics from observed data\n\nReturns\n\nDistance value based on model.distancemethod (:linear or :logarithmic) or combined distance if model.distancecombined is true\n\nNotes\n\nIf distance_combined is true:\n\nFor ACF: Combines ACF distance with fitted exponential decay timescale distance\nFor PSD: Combines PSD distance with knee frequency timescale distance and oscillation frequency distance\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Models.generate_data-Tuple{OneTimescaleAndOscModel, AbstractVector{<:Real}}","page":"API","title":"IntrinsicTimescales.Models.generate_data","text":"Models.generate_data(model::OneTimescaleAndOscModel, theta::AbstractVector{<:Real})\n\nGenerate synthetic data from the Ornstein-Uhlenbeck process with oscillation.\n\nArguments\n\nmodel::OneTimescaleAndOscModel: Model instance containing simulation parameters\ntheta::AbstractVector{<:Real}: Vector containing parameters [tau, freq, coeff]\n\nReturns\n\nSynthetic time series data with same dimensions as model.data\n\nNotes\n\nUses the model's stored parameters:\n\ndt: Time step\nT: Total time span\nnumTrials: Number of trials/trajectories\ndata_mean: Mean of the process\ndata_sd: Standard deviation of the process\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Models.int_fit-3","page":"API","title":"IntrinsicTimescales.Models.int_fit","text":"int_fit(model::OneTimescaleAndOscModel, param_dict::Dict=Dict())\n\nPerform inference using the specified fitting method.\n\nArguments\n\nmodel::OneTimescaleAndOscModel: Model instance\nparam_dict::Dict=Dict(): Dictionary of algorithm parameters. If empty, uses defaults.\n\nReturns\n\nFor ABC method:\n\nabc_record: Complete record of ABC iterations including posterior samples and MAP estimate\n\nFor ADVI method:\n\nresult: Results from variational inference containing samples, MAP estimates, and variational posterior\n\nNotes\n\nFor ABC: Uses Population Monte Carlo ABC with adaptive epsilon selection\nFor ADVI: Uses Automatic Differentiation Variational Inference via Turing.jl\nParameter dictionary can be customized for each method (see getparamdictabc() and getparamdictadvi())\n\n\n\n\n\n","category":"function"},{"location":"#IntrinsicTimescales.Models.summary_stats-Tuple{OneTimescaleAndOscModel, Any}","page":"API","title":"IntrinsicTimescales.Models.summary_stats","text":"Models.summary_stats(model::OneTimescaleAndOscModel, data)\n\nCompute summary statistics (ACF or PSD) from time series data.\n\nArguments\n\nmodel::OneTimescaleAndOscModel: Model instance specifying summary statistic type\ndata: Time series data to analyze\n\nReturns\n\nFor ACF (summary_method = :acf):\n\nMean autocorrelation function up to n_lags\n\nFor PSD (summary_method = :psd):\n\nMean power spectral density within specified frequency range\n\nNotes\n\nACF is computed using FFT-based method\nPSD is computed using AD-friendly implementation\nThrows ArgumentError if summary_method is invalid\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.OneTimescaleAndOsc.combined_distance-Tuple{OneTimescaleAndOscModel, Vararg{Any, 8}}","page":"API","title":"IntrinsicTimescales.OneTimescaleAndOsc.combined_distance","text":"combined_distance(model::OneTimescaleAndOscModel, simulation_summary, data_summary,\n                 weights, distance_method, data_tau, simulation_tau, \n                 data_osc, simulation_osc)\n\nCompute combined distance metric between simulated and observed data.\n\nArguments\n\nmodel: OneTimescaleAndOscModel instance\nsimulation_summary: Summary statistics from simulation\ndata_summary: Summary statistics from observed data\nweights: Weight vector for combining distances. For ACF: [w1, w2]. For PSD: [w1, w2, w3]\ndistance_method: Distance metric type (:linear or :logarithmic)\ndata_tau: Timescale from observed data\nsimulation_tau: Timescale from simulation\ndata_osc: Oscillation frequency from observed data (used for PSD only)\nsimulation_osc: Oscillation frequency from simulation (used for PSD only)\n\nReturns\n\nFor ACF:\n\nWeighted combination: weights[1] * summarydistance + weights[2] * timescaledistance\n\nFor PSD:\n\nWeighted combination: weights[1] * summarydistance + weights[2] * timescaledistance + weights[3] * oscillation_distance\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.OneTimescaleAndOsc.one_timescale_and_osc_model-Tuple{Any, Any, Any}","page":"API","title":"IntrinsicTimescales.OneTimescaleAndOsc.one_timescale_and_osc_model","text":"one_timescale_and_osc_model(data, time, fit_method; kwargs...)\n\nConstruct a OneTimescaleAndOscModel for time series with oscillations. See https://duodenum96.github.io/IntrinsicTimescales.jl/stable/onetimescaleand_osc/ for details and complete examples. \n\nArguments\n\ndata: Input time series data\ntime: Time points corresponding to the data\nfit_method: Fitting method to use (:abc or :advi)\n\nKeyword Arguments\n\nsummary_method=:psd: Summary statistic type (:psd or :acf)\ndata_sum_stats=nothing: Pre-computed summary statistics\nlags_freqs=nothing: Custom lags or frequencies\nprior=nothing: Prior distribution(s) for parameters\nn_lags=nothing: Number of lags for ACF\ndistance_method=nothing: Distance metric type\ndt=time[2]-time[1]: Time step\nT=time[end]: Total time span\nnumTrials=size(data,1): Number of trials\ndata_mean=mean(data): Data mean\ndata_sd=std(data): Data standard deviation\nfreqlims=nothing: Frequency limits for PSD\nfreq_idx=nothing: Frequency selection mask\ndims=ndims(data): Analysis dimension\ndistance_combined=false: Use combined distance\nweights=[0.5, 0.5]: Distance weights for combined distance\ndata_tau=nothing: Pre-computed timescale\ndata_osc=nothing: Pre-computed oscillation frequency\n\nReturns\n\nOneTimescaleAndOscModel: Model instance configured for specified analysis method\n\nNotes\n\nFour main usage patterns:\n\nACF-based ABC/ADVI: summary_method=:acf, fit_method=:abc/:advi\nPSD-based ABC/ADVI: summary_method=:psd, fit_method=:abc/:advi\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.OneTimescaleWithMissing","page":"API","title":"IntrinsicTimescales.OneTimescaleWithMissing","text":"OneTimescaleWithMissing\n\nModule for handling time series analysis with missing data. Uses specialized methods for handling NaN values:\n\nFor ACF: Uses compactime_missing (equivalent to statsmodels.tsa.statstools.acf with missing=\"conservative\")\nFor PSD: Uses Lomb-Scargle periodogram to handle irregular sampling\n\n\n\n\n\n","category":"module"},{"location":"#IntrinsicTimescales.OneTimescaleWithMissing.OneTimescaleWithMissingModel","page":"API","title":"IntrinsicTimescales.OneTimescaleWithMissing.OneTimescaleWithMissingModel","text":"OneTimescaleWithMissingModel <: AbstractTimescaleModel\n\nModel for inferring a single timescale from time series data with missing values.\n\nFields\n\ndata::AbstractArray{<:Real}: Input time series data (may contain NaN)\ntime::AbstractVector{<:Real}: Time points corresponding to the data\nfit_method::Symbol: Fitting method (:abc or :advi)\nsummary_method::Symbol: Summary statistic type (:psd or :acf)\nlags_freqs: Lags (for ACF) or frequencies (for PSD)\nprior: Prior distribution(s) for parameters\ndistance_method::Symbol: Distance metric type (:linear or :logarithmic)\ndata_sum_stats: Pre-computed summary statistics\ndt::Real: Time step between observations\nT::Real: Total time span\nnumTrials::Real: Number of trials/iterations\ndata_mean::Real: Mean of input data (excluding NaN)\ndata_sd::Real: Standard deviation of input data (excluding NaN)\nfreqlims: Frequency limits for PSD analysis\nn_lags: Number of lags for ACF\nfreq_idx: Boolean mask for frequency selection\ndims::Int: Dimension along which to compute statistics\ndistance_combined::Bool: Whether to use combined distance metric\nweights::Vector{Real}: Weights for combined distance\ndata_tau::Union{Real, Nothing}: Pre-computed timescale\nmissing_mask::AbstractArray{Bool}: Boolean mask indicating NaN positions\n\n\n\n\n\n","category":"type"},{"location":"#IntrinsicTimescales.Models.distance_function-Tuple{OneTimescaleWithMissingModel, Any, Any}","page":"API","title":"IntrinsicTimescales.Models.distance_function","text":"Models.distance_function(model::OneTimescaleWithMissingModel, sum_stats, data_sum_stats)\n\nCalculate the distance between summary statistics of simulated and observed data.\n\nArguments\n\nmodel::OneTimescaleWithMissingModel: Model instance\nsum_stats: Summary statistics from simulated data\ndata_sum_stats: Summary statistics from observed data\n\nReturns\n\nDistance value based on model.distancemethod (:linear or :logarithmic) or combined distance if model.distancecombined is true\n\nNotes\n\nIf distance_combined is true:\n\nFor ACF: Combines ACF distance with fitted exponential decay timescale distance\nFor PSD: Combines PSD distance with knee frequency timescale distance\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Models.generate_data-Tuple{OneTimescaleWithMissingModel, Any}","page":"API","title":"IntrinsicTimescales.Models.generate_data","text":"Models.generate_data(model::OneTimescaleWithMissingModel, theta)\n\nGenerate synthetic data from the Ornstein-Uhlenbeck process and apply missing data mask.\n\nArguments\n\nmodel::OneTimescaleWithMissingModel: Model instance containing simulation parameters\ntheta: Vector containing single timescale parameter (τ)\n\nReturns\n\nSynthetic time series data with NaN values at positions specified by model.missing_mask\n\nNotes\n\nGenerates complete OU process data\nApplies missing data mask from original data\nReturns data with same missing value pattern as input\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Models.int_fit-4","page":"API","title":"IntrinsicTimescales.Models.int_fit","text":"int_fit(model::OneTimescaleWithMissingModel, param_dict=Dict())\n\nPerform inference using the specified fitting method.\n\nArguments\n\nmodel::OneTimescaleWithMissingModel: Model instance\nparam_dict=nothing: Optional dictionary of algorithm parameters. If nothing, uses defaults.\n\nReturns\n\nFor ABC method:\n\nposterior_samples: Matrix of accepted parameter samples\nposterior_MAP: Maximum a posteriori estimate\nabc_record: Full record of ABC iterations\n\nFor ADVI method:\n\nADVIResults: Container with samples, MAP estimates, variances, and full chain\n\nNotes\n\nFor ABC: Uses Population Monte Carlo ABC with adaptive epsilon selection\nFor ADVI: Uses Automatic Differentiation Variational Inference via Turing.jl\nParameter dictionary can be customized for each method (see getparamdict_abc())\n\n\n\n\n\n","category":"function"},{"location":"#IntrinsicTimescales.Models.summary_stats-Tuple{OneTimescaleWithMissingModel, Any}","page":"API","title":"IntrinsicTimescales.Models.summary_stats","text":"Models.summary_stats(model::OneTimescaleWithMissingModel, data)\n\nCompute summary statistics (ACF or PSD) from time series data with missing values.\n\nArguments\n\nmodel::OneTimescaleWithMissingModel: Model instance specifying summary statistic type\ndata: Time series data to analyze (may contain NaN)\n\nReturns\n\nFor ACF (summary_method = :acf):\n\nMean autocorrelation function up to n_lags, computed with missing data handling\n\nFor PSD (summary_method = :psd):\n\nMean Lomb-Scargle periodogram within specified frequency range\n\nNotes\n\nACF uses compactime_missing for proper handling of NaN values\nPSD uses Lomb-Scargle periodogram for irregular sampling\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.OneTimescaleWithMissing.combined_distance-Tuple{OneTimescaleWithMissingModel, Vararg{Any, 5}}","page":"API","title":"IntrinsicTimescales.OneTimescaleWithMissing.combined_distance","text":"combined_distance(model, simulation_summary, data_summary, weights, data_tau, simulation_tau)\n\nCalculate a weighted combination of summary statistic distance and timescale distance.\n\nArguments\n\nmodel::OneTimescaleWithMissingModel: Model instance\nsimulation_summary: Summary statistics from simulated data\ndata_summary: Summary statistics from observed data  \nweights: Weight vector for combining distances\ndata_tau: Timescale extracted from observed data\nsimulation_tau: Timescale extracted from simulated data\n\nReturns\n\nCombined weighted distance value\n\nNotes\n\nUses the model's distance_method (:linear or :logarithmic) for summary statistic comparison and linear distance for timescale comparison.\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.OneTimescaleWithMissing.one_timescale_with_missing_model-Tuple{Any, Any, Any}","page":"API","title":"IntrinsicTimescales.OneTimescaleWithMissing.one_timescale_with_missing_model","text":"one_timescale_with_missing_model(data, time, fit_method; kwargs...)\n\nConstruct a OneTimescaleWithMissingModel for time series analysis with missing data. See https://duodenum96.github.io/IntrinsicTimescales.jl/stable/onetimescalewith_missing/ for details and complete examples. \n\nArguments\n\ndata: Input time series data (may contain NaN)\ntime: Time points corresponding to the data\nfit_method: Fitting method to use (:abc or :advi)\n\nKeyword Arguments\n\nsummary_method=:acf: Summary statistic type (:psd or :acf)\ndata_sum_stats=nothing: Pre-computed summary statistics\nlags_freqs=nothing: Custom lags or frequencies\nprior=nothing: Prior distribution(s) for parameters\nn_lags=nothing: Number of lags for ACF\ndistance_method=nothing: Distance metric type\ndt=time[2]-time[1]: Time step\nT=time[end]: Total time span\nnumTrials=size(data,1): Number of trials\ndata_mean=nanmean(data): Data mean (excluding NaN)\ndata_sd=nanstd(data): Data standard deviation (excluding NaN)\nfreqlims=nothing: Frequency limits for PSD\nfreq_idx=nothing: Frequency selection mask\ndims=ndims(data): Analysis dimension\ndistance_combined=false: Use combined distance\nweights=[0.5, 0.5]: Distance weights\ndata_tau=nothing: Pre-computed timescale\n\nReturns\n\nOneTimescaleWithMissingModel: Model instance configured for specified analysis method\n\nNotes\n\nFour main usage patterns:\n\nACF-based ABC/ADVI: summary_method=:acf, fit_method=:abc/:advi\nPSD-based ABC/ADVI: summary_method=:psd, fit_method=:abc/:advi\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.OneTimescaleAndOscWithMissing","page":"API","title":"IntrinsicTimescales.OneTimescaleAndOscWithMissing","text":"OneTimescaleAndOscWithMissing\n\nModule for handling time series analysis with both oscillations and missing data. Uses specialized methods for handling NaN values:\n\nFor ACF: Uses compactime_missing for proper handling of gaps\nFor PSD: Uses Lomb-Scargle periodogram for irregular sampling\n\n\n\n\n\n","category":"module"},{"location":"#IntrinsicTimescales.OneTimescaleAndOscWithMissing.OneTimescaleAndOscWithMissingModel","page":"API","title":"IntrinsicTimescales.OneTimescaleAndOscWithMissing.OneTimescaleAndOscWithMissingModel","text":"OneTimescaleAndOscWithMissingModel <: AbstractTimescaleModel\n\nModel for inferring a single timescale from time series data with missing values. Parameters: [tau, freq, coeff] representing timescale, oscillation frequency, and oscillation coefficient.\n\nFields\n\ndata::AbstractArray{<:Real}: Input time series data (may contain NaN)\ntime::AbstractVector{<:Real}: Time points corresponding to the data\nfit_method::Symbol: Fitting method (:abc, :advi)\nsummary_method::Symbol: Summary statistic type (:psd or :acf)\nlags_freqs: Lags (for ACF) or frequencies (for PSD)\nprior: Prior distribution(s) for parameters\ndistance_method::Symbol: Distance metric type (:linear or :logarithmic)\ndata_sum_stats: Pre-computed summary statistics\ndt::Real: Time step between observations\nT::Real: Total time span\nnumTrials::Real: Number of trials/iterations\ndata_mean::Real: Mean of input data (excluding NaN)\ndata_sd::Real: Standard deviation of input data (excluding NaN)\nfreqlims: Frequency limits for PSD analysis\nn_lags: Number of lags for ACF\nfreq_idx: Boolean mask for frequency selection\ndims::Int: Dimension along which to compute statistics\ndistance_combined::Bool: Whether to use combined distance metric\nweights::Vector{Real}: Weights for combined distance\ndata_tau::Union{Real, Nothing}: Pre-computed timescale\ndata_osc::Union{Real, Nothing}: Pre-computed oscillation frequency\nmissing_mask::AbstractArray{Bool}: Boolean mask indicating NaN positions\n\n\n\n\n\n","category":"type"},{"location":"#IntrinsicTimescales.Models.distance_function-Tuple{OneTimescaleAndOscWithMissingModel, Any, Any}","page":"API","title":"IntrinsicTimescales.Models.distance_function","text":"Models.distance_function(model::OneTimescaleAndOscWithMissingModel, sum_stats, data_sum_stats)\n\nCompute distance between simulated and observed summary statistics.\n\nArguments\n\nmodel::OneTimescaleAndOscWithMissingModel: Model instance specifying distance configuration\nsum_stats: Summary statistics from simulation\ndata_sum_stats: Summary statistics from observed data\n\nReturns\n\nDistance value based on model configuration\n\nNotes\n\nIf distancecombined=true: Uses combineddistance with both the estimated parameters and the full summary statistics. \nIf distance_combined=false: Uses simple distance based on full summary statistics. \nDistance methods: :linear (Euclidean) or :logarithmic (log-space Euclidean)\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Models.generate_data-Tuple{OneTimescaleAndOscWithMissingModel, Any}","page":"API","title":"IntrinsicTimescales.Models.generate_data","text":"Models.generate_data(model::OneTimescaleAndOscWithMissingModel, theta)\n\nGenerate synthetic data from the Ornstein-Uhlenbeck process with oscillation and apply missing data mask.\n\nArguments\n\nmodel::OneTimescaleAndOscWithMissingModel: Model instance containing simulation parameters\ntheta: Vector containing parameters [tau, freq, coeff]\n\nReturns\n\nSynthetic time series data with oscillations and NaN values at positions specified by model.missing_mask\n\nNotes\n\nGenerates complete OU process data with oscillation\nApplies missing data mask from original data\nReturns data with same missing value pattern as input\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Models.int_fit-5","page":"API","title":"IntrinsicTimescales.Models.int_fit","text":"int_fit(model::OneTimescaleAndOscWithMissingModel, param_dict=Dict())\n\nPerform inference using the specified fitting method.\n\nArguments\n\nmodel::OneTimescaleAndOscWithMissingModel: Model instance\nparam_dict=Dict(): Dictionary of algorithm parameters. If empty, uses defaults.\n\nReturns\n\nFor ABC method:\n\nabc_record: Complete ABC record containing accepted samples, distances, and convergence info\n\nFor ADVI method:\n\nADVIResults: Container with samples, MAP estimates, variances, and convergence information\n\nNotes\n\nFor ABC: Uses Population Monte Carlo ABC with adaptive epsilon selection\nFor ADVI: Uses Automatic Differentiation Variational Inference via Turing.jl\nDefault parameters available via getparamdictabc() and getparamdictadvi()\n\n\n\n\n\n","category":"function"},{"location":"#IntrinsicTimescales.Models.summary_stats-Tuple{OneTimescaleAndOscWithMissingModel, Any}","page":"API","title":"IntrinsicTimescales.Models.summary_stats","text":"Models.summary_stats(model::OneTimescaleAndOscWithMissingModel, data)\n\nCompute summary statistics (ACF or PSD) from time series data with missing values.\n\nArguments\n\nmodel::OneTimescaleAndOscWithMissingModel: Model instance specifying summary statistic type\ndata: Time series data to analyze (may contain NaN)\n\nReturns\n\nFor ACF (summary_method = :acf):\n\nMean autocorrelation function up to n_lags, computed with missing data handling\n\nFor PSD (summary_method = :psd):\n\nMean Lomb-Scargle periodogram within specified frequency range\n\nNotes\n\nACF uses compactime_missing for proper handling of NaN values\nPSD uses Lomb-Scargle periodogram for irregular sampling\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.OneTimescaleAndOscWithMissing.combined_distance-Tuple{OneTimescaleAndOscWithMissingModel, Vararg{Any, 8}}","page":"API","title":"IntrinsicTimescales.OneTimescaleAndOscWithMissing.combined_distance","text":"combined_distance(model::OneTimescaleAndOscWithMissingModel, simulation_summary, data_summary,\n                 weights, distance_method, data_tau, simulation_tau, data_osc, simulation_osc)\n\nCompute combined distance metric between simulated and observed data.\n\nArguments\n\nmodel: OneTimescaleAndOscWithMissingModel instance\nsimulation_summary: Summary statistics from simulation\ndata_summary: Summary statistics from observed data\nweights: Weights for combining distances (length 2 for ACF, length 3 for PSD)\ndistance_method: Distance metric type (:linear or :logarithmic)\ndata_tau: Timescale from observed data\nsimulation_tau: Timescale from simulation\ndata_osc: Oscillation frequency from observed data (used only for PSD)\nsimulation_osc: Oscillation frequency from simulation (used only for PSD)\n\nReturns\n\nFor ACF:\n\nWeighted combination: weights[1] * summarydistance + weights[2] * taudistance\n\nFor PSD:\n\nWeighted combination: weights[1] * summarydistance + weights[2] * taudistance + weights[3] * osc_distance\n\nNotes\n\nEnsure weights vector has correct length: 2 for ACF, 3 for PSD method\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.OneTimescaleAndOscWithMissing.one_timescale_and_osc_with_missing_model-Tuple{Any, Any, Any}","page":"API","title":"IntrinsicTimescales.OneTimescaleAndOscWithMissing.one_timescale_and_osc_with_missing_model","text":"one_timescale_and_osc_with_missing_model(data, time, fit_method; kwargs...)\n\nConstruct a OneTimescaleAndOscWithMissingModel for inferring timescales and oscillations from data with missing values.\n\nArguments\n\ndata: Input time series data (may contain NaN values)\ntime: Time points corresponding to the data\nfit_method: Fitting method to use (:abc or :advi)\n\nKeyword Arguments\n\nsummary_method=:psd: Summary statistic type (:psd or :acf)\ndata_sum_stats=nothing: Pre-computed summary statistics\nlags_freqs=nothing: Custom lags or frequencies\nprior=nothing: Prior distribution(s) for parameters\nn_lags=nothing: Number of lags for ACF\ndistance_method=nothing: Distance metric type (:linear or :logarithmic)\ndt=time[2]-time[1]: Time step between observations\nT=time[end]: Total time span\ndata_mean=nanmean(data): Data mean (excluding NaN values)\ndata_sd=nanstd(data): Data standard deviation (excluding NaN values)\nfreqlims=nothing: Frequency limits for PSD analysis (defaults to (0.5/1000, 100/1000) kHz)\nfreq_idx=nothing: Frequency selection mask\ndims=ndims(data): Dimension along which to compute statistics\nnumTrials=size(data, setdiff([1,2], dims)[1]): Number of trials/iterations\ndistance_combined=false: Whether to use combined distance metric\nweights=[0.5, 0.5]: Weights for combined distance (length 2 for ACF, length 3 for PSD)\ndata_tau=nothing: Pre-computed timescale for combined distance\ndata_osc=nothing: Pre-computed oscillation frequency for combined distance\n\nReturns\n\nOneTimescaleAndOscWithMissingModel: Model instance configured for specified analysis method\n\nNotes\n\nMain usage patterns:\n\nACF-based analysis: summary_method=:acf - Uses autocorrelation with missing data handling\nPSD-based analysis: summary_method=:psd - Uses Lomb-Scargle periodogram for irregular sampling\n\nKey differences from regular model:\n\nAutomatically detects and handles NaN values in input data\nUses nanmean/nanstd for robust statistics computation\nApplies specialized missing-data algorithms for summary statistics\nPreserves missing data pattern in synthetic data generation\n\nFor combined distance (distance_combined=true):\n\nACF method: Combines summary distance with timescale distance (2 weights)\nPSD method: Combines summary, timescale, and oscillation distances (3 weights)\n\n\n\n\n\n","category":"method"},{"location":"#IntrinsicTimescales.Plotting.acwplot","page":"API","title":"IntrinsicTimescales.Plotting.acwplot","text":"acwplot(acwresults::ACWResults; only_acf::Bool=false, only_psd::Bool=false, show::Bool=true)\n\nPlaceholder function for plotting autocorrelation function (ACF) and power spectral density (PSD) results.\n\nThis function is implemented in the IntPlottingExt extension package, which is automatically loaded when Plots.jl is available. The extension provides comprehensive visualization capabilities for ACW analysis results.\n\nFunctionality (available when Plots.jl is loaded):\n\nPlots autocorrelation function and/or power spectral density from ACWResults\nSupports plotting either ACF alone, PSD alone, or both in a subplot layout\nACF plots show individual traces in color with mean overlaid in black\nPSD plots use logarithmic scales for both axes\nHandles up to 2-dimensional ACF data\n\nArguments (when extension is loaded):\n\nacwresults::ACWResults: Container with ACF and/or PSD analysis results\nonly_acf::Bool=false: If true, plot only the autocorrelation function\nonly_psd::Bool=false: If true, plot only the power spectral density  \nshow::Bool=true: Whether to display the plot immediately\n\nRequirements:\n\nRequires Plots.jl to be loaded for the extension to activate\nInstall with: using Plots or import Plots\n\nSee IntPlottingExt documentation for complete details.\n\n\n\n\n\n","category":"function"},{"location":"#IntrinsicTimescales.Plotting.posterior_predictive","page":"API","title":"IntrinsicTimescales.Plotting.posterior_predictive","text":"posterior_predictive(container::Union{ABCResults, ADVIResults}, model::Models.AbstractTimescaleModel; show::Bool=true, n_samples::Int=100)\n\nPlaceholder function for posterior predictive check plotting.\n\nThis function is implemented in the IntPlottingExt extension package, which is automatically loaded when Plots.jl is available. The extension provides posterior predictive checking visualization for both ABC and ADVI inference results.\n\nFunctionality (available when Plots.jl is loaded):\n\nCreates posterior predictive check plots showing data vs. model predictions\nWorks with both ABCResults and ADVIResults containers\nShows posterior predictive intervals with uncertainty bands\nAutomatically handles ACF and PSD summary statistics with appropriate scaling\nSupports logarithmic scaling when the distance method is logarithmic\n\nArguments:\n\ncontainer::Union{ABCResults, ADVIResults}: Results container from inference\nmodel::Models.AbstractTimescaleModel: Model used for inference\nshow::Bool=true: Whether to display the plot immediately\nn_samples::Int=100: Number of posterior samples to use for prediction\n\nRequirements:\n\nRequires Plots.jl to be loaded for the extension to activate\nInstall with: using Plots or import Plots\n\nSee IntPlottingExt documentation for complete details.\n\n\n\n\n\n","category":"function"},{"location":"#Index","page":"API","title":"Index","text":"","category":"section"},{"location":"","page":"API","title":"API","text":"","category":"page"}]
}
